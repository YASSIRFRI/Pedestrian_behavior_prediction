{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T11:59:26.535797Z",
     "iopub.status.busy": "2025-07-23T11:59:26.535177Z",
     "iopub.status.idle": "2025-07-23T12:00:49.185590Z",
     "shell.execute_reply": "2025-07-23T12:00:49.184868Z",
     "shell.execute_reply.started": "2025-07-23T11:59:26.535771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import gc\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "\n",
    "def install_qwen_packages():\n",
    "    \"\"\"Install required packages for Qwen\"\"\"\n",
    "    packages = [\n",
    "        \"transformers>=4.45.0\",\n",
    "        \"accelerate\",\n",
    "        \"tiktoken\",\n",
    "        \"qwen-vl-utils\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                          check=True, capture_output=True, text=True)\n",
    "            print(f\"‚úÖ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning installing {package}: {e}\")\n",
    "\n",
    "def setup_qwen_model():\n",
    "    \"\"\"Initialize Qwen 2.5 Vision model\"\"\"\n",
    "    print(\"üß† Setting up Qwen 2.5 Vision model...\")\n",
    "    \n",
    "    # Install packages\n",
    "    install_qwen_packages()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    try:\n",
    "        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "        from qwen_vl_utils import process_vision_info\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "        \n",
    "        model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "        \n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=\"auto\", \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        \n",
    "        print(f\"‚úÖ Qwen 2.5 Vision model loaded successfully!\")\n",
    "        return model, processor, device, process_vision_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Qwen model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_dataset_with_annotations(dataset_path):\n",
    "    \"\"\"Load images and corresponding annotations\"\"\"\n",
    "    print(f\"üìÅ Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Find all images and annotations\n",
    "    raw_images = glob.glob(os.path.join(dataset_path, \"*_*_r.*\"))\n",
    "    extended_images = glob.glob(os.path.join(dataset_path, \"*_*_x.*\"))\n",
    "    annotation_files = glob.glob(os.path.join(dataset_path, \"*_annotation.csv\"))\n",
    "    \n",
    "    print(f\"üñºÔ∏è Found {len(raw_images)} raw images\")\n",
    "    print(f\"üñºÔ∏è Found {len(extended_images)} extended images\") \n",
    "    print(f\"üìÑ Found {len(annotation_files)} annotation files\")\n",
    "    \n",
    "    # Process annotations\n",
    "    annotations_dict = {}\n",
    "    for ann_file in annotation_files:\n",
    "        frame_id = os.path.basename(ann_file).replace('_annotation.csv', '')\n",
    "        try:\n",
    "            df = pd.read_csv(ann_file, header=None)\n",
    "            behaviors = df.iloc[0].tolist()  # First row contains behaviors\n",
    "            annotations_dict[frame_id] = behaviors\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {ann_file}: {e}\")\n",
    "    \n",
    "    # Create dataset entries\n",
    "    dataset_entries = []\n",
    "    \n",
    "    # Process raw images\n",
    "    for img_path in raw_images:\n",
    "        filename = os.path.basename(img_path)\n",
    "        # Parse: FRAMEID_PERSONID_r.ext\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            frame_id = parts[0]\n",
    "            person_id = int(parts[1])\n",
    "            \n",
    "            if frame_id in annotations_dict:\n",
    "                behaviors = annotations_dict[frame_id]\n",
    "                if person_id <= len(behaviors):\n",
    "                    true_behavior = behaviors[person_id - 1].strip()  # person_id is 1-indexed\n",
    "                    \n",
    "                    dataset_entries.append({\n",
    "                        'image_path': img_path,\n",
    "                        'frame_id': frame_id,\n",
    "                        'person_id': person_id,\n",
    "                        'true_behavior': true_behavior,\n",
    "                        'crop_type': 'raw',\n",
    "                        'filename': filename\n",
    "                    })\n",
    "    \n",
    "    # Process extended images\n",
    "    for img_path in extended_images:\n",
    "        filename = os.path.basename(img_path)\n",
    "        # Parse: FRAMEID_PERSONID_x.ext\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            frame_id = parts[0]\n",
    "            person_id = int(parts[1])\n",
    "            \n",
    "            if frame_id in annotations_dict:\n",
    "                behaviors = annotations_dict[frame_id]\n",
    "                if person_id <= len(behaviors):\n",
    "                    true_behavior = behaviors[person_id - 1].strip()  # person_id is 1-indexed\n",
    "                    \n",
    "                    dataset_entries.append({\n",
    "                        'image_path': img_path,\n",
    "                        'frame_id': frame_id,\n",
    "                        'person_id': person_id,\n",
    "                        'true_behavior': true_behavior,\n",
    "                        'crop_type': 'extended',\n",
    "                        'filename': filename\n",
    "                    })\n",
    "    \n",
    "    df_dataset = pd.DataFrame(dataset_entries)\n",
    "    print(f\"üìä Dataset loaded: {len(df_dataset)} total entries\")\n",
    "    \n",
    "    return df_dataset, annotations_dict\n",
    "\n",
    "def analyze_dataset_statistics(df_dataset):\n",
    "    \"\"\"Generate comprehensive dataset statistics\"\"\"\n",
    "    print(\"\\nüìä DATASET STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_entries = len(df_dataset)\n",
    "    unique_frames = df_dataset['frame_id'].nunique()\n",
    "    raw_count = len(df_dataset[df_dataset['crop_type'] == 'raw'])\n",
    "    extended_count = len(df_dataset[df_dataset['crop_type'] == 'extended'])\n",
    "    \n",
    "    print(f\"Total entries: {total_entries}\")\n",
    "    print(f\"Unique frames: {unique_frames}\")\n",
    "    print(f\"Raw crops: {raw_count}\")\n",
    "    print(f\"Extended crops: {extended_count}\")\n",
    "    \n",
    "    # Behavior distribution\n",
    "    print(f\"\\nüéØ BEHAVIOR DISTRIBUTION:\")\n",
    "    behavior_counts = df_dataset['true_behavior'].value_counts()\n",
    "    for behavior, count in behavior_counts.items():\n",
    "        percentage = (count / total_entries) * 100\n",
    "        print(f\"  {behavior}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Per-frame statistics\n",
    "    frames_stats = df_dataset.groupby('frame_id').agg({\n",
    "        'person_id': 'count',\n",
    "        'true_behavior': lambda x: list(x.unique())\n",
    "    }).rename(columns={'person_id': 'person_count'})\n",
    "    \n",
    "    print(f\"\\nüë• PERSONS PER FRAME:\")\n",
    "    person_count_dist = frames_stats['person_count'].value_counts().sort_index()\n",
    "    for persons, frames in person_count_dist.items():\n",
    "        print(f\"  {persons} persons: {frames} frames\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Behavior distribution\n",
    "    behavior_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Behavior Distribution')\n",
    "    axes[0,0].set_xlabel('Behavior')\n",
    "    axes[0,0].set_ylabel('Count')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Crop type distribution\n",
    "    crop_type_counts = df_dataset['crop_type'].value_counts()\n",
    "    crop_type_counts.plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%')\n",
    "    axes[0,1].set_title('Crop Type Distribution')\n",
    "    \n",
    "    # Persons per frame\n",
    "    person_count_dist.plot(kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('Persons per Frame Distribution')\n",
    "    axes[1,0].set_xlabel('Number of Persons')\n",
    "    axes[1,0].set_ylabel('Number of Frames')\n",
    "    \n",
    "    # Behavior by crop type\n",
    "    behavior_crop = pd.crosstab(df_dataset['true_behavior'], df_dataset['crop_type'])\n",
    "    behavior_crop.plot(kind='bar', ax=axes[1,1], width=0.8)\n",
    "    axes[1,1].set_title('Behaviors by Crop Type')\n",
    "    axes[1,1].set_xlabel('Behavior')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].legend(title='Crop Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return behavior_counts\n",
    "\n",
    "def classify_with_qwen(image_path, model, processor, device, process_vision_info):\n",
    "    \"\"\"Classify pedestrian behavior using Qwen\"\"\"\n",
    "    \n",
    "    behavior_classes = [\n",
    "        \"walking\",\n",
    "        \"running\",\n",
    "        \"pushing a stroller\",\n",
    "        \"biking\",\n",
    "        \"standing\",\n",
    "        \"skateboarding\"\n",
    "    ]\n",
    "    \n",
    "    prompt = f\"\"\"Look at this image of a person and classify their behavior/activity. Choose from these options:\n",
    "{', '.join(behavior_classes)}\n",
    "\n",
    "Important: Respond with ONLY ONE WORD from the list above. Do not add any other text.\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load and process the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Prepare messages for Qwen\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": image,\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Process vision info\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        # Clean up response\n",
    "        classification = response.strip().lower()\n",
    "        \n",
    "        # Map to predefined classes\n",
    "        for behavior in behavior_classes:\n",
    "            if behavior.lower() in classification:\n",
    "                return behavior\n",
    "        \n",
    "        return classification\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error classifying {image_path}: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "def evaluate_qwen_model(df_dataset, model, processor, device, process_vision_info, crop_type='raw'):\n",
    "    \"\"\"Evaluate Qwen model on dataset\"\"\"\n",
    "    print(f\"\\nüß† EVALUATING QWEN MODEL ON {crop_type.upper()} CROPS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter dataset by crop type\n",
    "    df_eval = df_dataset[df_dataset['crop_type'] == crop_type].copy()\n",
    "    print(f\"üìä Evaluating on {len(df_eval)} {crop_type} images\")\n",
    "    \n",
    "    # Run predictions\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    detailed_results = []\n",
    "    \n",
    "    for idx, row in df_eval.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        true_behavior = row['true_behavior']\n",
    "        \n",
    "        print(f\"üîÑ Processing {idx+1}/{len(df_eval)}: {row['filename']}\")\n",
    "        \n",
    "        # Get prediction\n",
    "        predicted_behavior = classify_with_qwen(\n",
    "            image_path, model, processor, device, process_vision_info\n",
    "        )\n",
    "        \n",
    "        predictions.append(predicted_behavior)\n",
    "        true_labels.append(true_behavior)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'filename': row['filename'],\n",
    "            'frame_id': row['frame_id'],\n",
    "            'person_id': row['person_id'],\n",
    "            'true_behavior': true_behavior,\n",
    "            'predicted_behavior': predicted_behavior,\n",
    "            'correct': predicted_behavior == true_behavior,\n",
    "            'crop_type': crop_type\n",
    "        })\n",
    "        \n",
    "        print(f\"   True: {true_behavior} | Predicted: {predicted_behavior} | ‚úÖ\" if predicted_behavior == true_behavior else f\"   True: {true_behavior} | Predicted: {predicted_behavior} | ‚ùå\")\n",
    "        \n",
    "        # Clean GPU memory periodically\n",
    "        if torch.cuda.is_available() and idx % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return predictions, true_labels, detailed_results\n",
    "\n",
    "def compute_metrics(true_labels, predictions, detailed_results, crop_type, model_name=\"Qwen\"):\n",
    "    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n",
    "    print(f\"\\nüìà COMPUTING METRICS FOR {model_name} ({crop_type.upper()} CROPS)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get unique labels\n",
    "    all_labels = sorted(list(set(true_labels + predictions)))\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        true_labels, predictions, labels=all_labels, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1)\n",
    "    \n",
    "    # Weighted averages\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ OVERALL METRICS:\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   Macro Precision: {macro_precision:.3f}\")\n",
    "    print(f\"   Macro Recall: {macro_recall:.3f}\")\n",
    "    print(f\"   Macro F1: {macro_f1:.3f}\")\n",
    "    print(f\"   Weighted Precision: {weighted_precision:.3f}\")\n",
    "    print(f\"   Weighted Recall: {weighted_recall:.3f}\")\n",
    "    print(f\"   Weighted F1: {weighted_f1:.3f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\nüìä PER-CLASS METRICS:\")\n",
    "    for i, label in enumerate(all_labels):\n",
    "        print(f\"   {label}:\")\n",
    "        print(f\"     Precision: {precision[i]:.3f}\")\n",
    "        print(f\"     Recall: {recall[i]:.3f}\")\n",
    "        print(f\"     F1-Score: {f1[i]:.3f}\")\n",
    "        print(f\"     Support: {support[i]}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=all_labels)\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=all_labels, yticklabels=all_labels, ax=axes[0])\n",
    "    axes[0].set_title(f'{model_name} Confusion Matrix ({crop_type} crops)')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    \n",
    "    # Metrics comparison\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }, index=all_labels)\n",
    "    \n",
    "    metrics_df.plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title(f'{model_name} Per-Class Metrics ({crop_type} crops)')\n",
    "    axes[1].set_xlabel('Behavior')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nüìã DETAILED CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(true_labels, predictions, labels=all_labels, zero_division=0))\n",
    "    \n",
    "    # Error Analysis\n",
    "    df_results = pd.DataFrame(detailed_results)\n",
    "    errors = df_results[~df_results['correct']]\n",
    "    \n",
    "    print(f\"\\n‚ùå ERROR ANALYSIS:\")\n",
    "    print(f\"   Total errors: {len(errors)}/{len(df_results)} ({len(errors)/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(f\"\\n   Most common error patterns:\")\n",
    "        error_patterns = errors.groupby(['true_behavior', 'predicted_behavior']).size().sort_values(ascending=False)\n",
    "        for (true_b, pred_b), count in error_patterns.head(5).items():\n",
    "            print(f\"     {true_b} ‚Üí {pred_b}: {count} times\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_precision': weighted_precision,\n",
    "        'weighted_recall': weighted_recall,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'per_class_precision': precision,\n",
    "        'per_class_recall': recall,\n",
    "        'per_class_f1': f1,\n",
    "        'support': support,\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': all_labels,\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "\n",
    "def main_qwen_evaluation():\n",
    "    \"\"\"Main evaluation function for Qwen\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATASET_PATH = '/kaggle/input/pedestrian-cropped-annot'  \n",
    "    OUTPUT_PATH = '/kaggle/working/qwen_evaluation_results'\n",
    "    \n",
    "    print(\"üß† QWEN 2.5 VISION PEDESTRIAN BEHAVIOR EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n",
    "    print(f\"üìÅ Output path: {OUTPUT_PATH}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df_dataset, annotations_dict = load_dataset_with_annotations(DATASET_PATH)\n",
    "        if len(df_dataset) == 0:\n",
    "            print(\"‚ùå No valid dataset entries found!\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Analyze dataset statistics\n",
    "    behavior_counts = analyze_dataset_statistics(df_dataset)\n",
    "    \n",
    "    # Setup Qwen model\n",
    "    model, processor, device, process_vision_info = setup_qwen_model()\n",
    "    if model is None:\n",
    "        print(\"‚ùå Failed to load Qwen model!\")\n",
    "        return\n",
    "    \n",
    "    # Evaluate on raw crops\n",
    "    print(f\"\\nüöÄ Starting evaluation on RAW crops...\")\n",
    "    predictions_raw, true_labels_raw, detailed_results_raw = evaluate_qwen_model(\n",
    "        df_dataset, model, processor, device, process_vision_info, crop_type='raw'\n",
    "    )\n",
    "    \n",
    "    metrics_raw = compute_metrics(\n",
    "        true_labels_raw, predictions_raw, detailed_results_raw, 'raw', 'Qwen'\n",
    "    )\n",
    "    \n",
    "    # Evaluate on extended crops\n",
    "    print(f\"\\nüöÄ Starting evaluation on EXTENDED crops...\")\n",
    "    predictions_ext, true_labels_ext, detailed_results_ext = evaluate_qwen_model(\n",
    "        df_dataset, model, processor, device, process_vision_info, crop_type='extended'\n",
    "    )\n",
    "    \n",
    "    metrics_ext = compute_metrics(\n",
    "        true_labels_ext, predictions_ext, detailed_results_ext, 'extended', 'Qwen'\n",
    "    )\n",
    "    \n",
    "    # Compare raw vs extended\n",
    "    print(f\"\\nüîÑ RAW vs EXTENDED CROPS COMPARISON:\")\n",
    "    print(\"=\" * 50)\n",
    "    comparison_metrics = ['accuracy', 'macro_f1', 'weighted_f1']\n",
    "    for metric in comparison_metrics:\n",
    "        raw_val = metrics_raw[metric]\n",
    "        ext_val = metrics_ext[metric]\n",
    "        improvement = ext_val - raw_val\n",
    "        print(f\"{metric.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Raw: {raw_val:.3f}\")\n",
    "        print(f\"  Extended: {ext_val:.3f}\")\n",
    "        print(f\"  Improvement: {improvement:+.3f} ({'‚úÖ' if improvement > 0 else '‚ùå'})\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    final_results = {\n",
    "        'evaluation_info': {\n",
    "            'model': 'Qwen/Qwen2.5-VL-3B-Instruct',\n",
    "            'dataset_path': DATASET_PATH,\n",
    "            'total_samples': len(df_dataset),\n",
    "            'raw_samples': len(df_dataset[df_dataset['crop_type'] == 'raw']),\n",
    "            'extended_samples': len(df_dataset[df_dataset['crop_type'] == 'extended']),\n",
    "            'evaluation_date': datetime.now().isoformat()\n",
    "        },\n",
    "        'dataset_statistics': {\n",
    "            'behavior_distribution': behavior_counts.to_dict(),\n",
    "            'total_frames': df_dataset['frame_id'].nunique(),\n",
    "            'avg_persons_per_frame': df_dataset.groupby('frame_id')['person_id'].count().mean()\n",
    "        },\n",
    "        'raw_crops_results': {\n",
    "            'metrics': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                       for k, v in metrics_raw.items() if k != 'detailed_results'},\n",
    "            'detailed_results': detailed_results_raw\n",
    "        },\n",
    "        'extended_crops_results': {\n",
    "            'metrics': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                       for k, v in metrics_ext.items() if k != 'detailed_results'},\n",
    "            'detailed_results': detailed_results_ext\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    results_file = os.path.join(OUTPUT_PATH, 'qwen_evaluation_complete_results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüéâ QWEN EVALUATION COMPLETE!\")\n",
    "    print(f\"üìÅ Results saved to: {results_file}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ GPU memory cleaned up\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_qwen_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T12:07:19.422110Z",
     "iopub.status.busy": "2025-07-23T12:07:19.421348Z",
     "iopub.status.idle": "2025-07-23T12:08:11.651405Z",
     "shell.execute_reply": "2025-07-23T12:08:11.650796Z",
     "shell.execute_reply.started": "2025-07-23T12:07:19.422079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import gc\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "\n",
    "def setup_blip_bart_models():\n",
    "    \"\"\"Initialize BLIP for captioning and BART for classification\"\"\"\n",
    "    print(\"ü§ñ Setting up BLIP + BART models...\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    try:\n",
    "        from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "        \n",
    "        # Load BLIP for image captioning\n",
    "        blip_model_id = \"Salesforce/blip-image-captioning-large\"\n",
    "        print(f\"üì• Loading BLIP: {blip_model_id}\")\n",
    "        \n",
    "        blip_processor = BlipProcessor.from_pretrained(blip_model_id)\n",
    "        blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "            blip_model_id,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        if device == \"cuda\":\n",
    "            blip_model = blip_model.to(device)\n",
    "        \n",
    "        # Load BART for classification\n",
    "        print(f\"üì• Loading BART classifier...\")\n",
    "        classifier = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=\"facebook/bart-large-mnli\",\n",
    "            device=0 if device == \"cuda\" else -1\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ BLIP + BART models loaded successfully!\")\n",
    "        return blip_model, blip_processor, classifier, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading models: {e}\")\n",
    "        print(\"üîÑ Installing required packages...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"transformers>=4.45.0\", \"torch\"], check=True)\n",
    "        \n",
    "        try:\n",
    "            from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
    "            \n",
    "            blip_processor = BlipProcessor.from_pretrained(blip_model_id)\n",
    "            blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "                blip_model_id,\n",
    "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                blip_model = blip_model.to(device)\n",
    "            \n",
    "            classifier = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=\"facebook/bart-large-mnli\",\n",
    "                device=0 if device == \"cuda\" else -1\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Models loaded after installing dependencies!\")\n",
    "            return blip_model, blip_processor, classifier, device\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Still failed: {e2}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "def load_dataset_with_annotations(dataset_path):\n",
    "    \"\"\"Load images and corresponding annotations (same as Qwen cell)\"\"\"\n",
    "    print(f\"üìÅ Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Find all images and annotations\n",
    "    raw_images = glob.glob(os.path.join(dataset_path, \"*_*_r.*\"))\n",
    "    extended_images = glob.glob(os.path.join(dataset_path, \"*_*_x.*\"))\n",
    "    annotation_files = glob.glob(os.path.join(dataset_path, \"*_annotation.csv\"))\n",
    "    \n",
    "    print(f\"üñºÔ∏è Found {len(raw_images)} raw images\")\n",
    "    print(f\"üñºÔ∏è Found {len(extended_images)} extended images\") \n",
    "    print(f\"üìÑ Found {len(annotation_files)} annotation files\")\n",
    "    \n",
    "    # Process annotations\n",
    "    annotations_dict = {}\n",
    "    for ann_file in annotation_files:\n",
    "        frame_id = os.path.basename(ann_file).replace('_annotation.csv', '')\n",
    "        try:\n",
    "            df = pd.read_csv(ann_file, header=None)\n",
    "            behaviors = df.iloc[0].tolist()  # First row contains behaviors\n",
    "            annotations_dict[frame_id] = behaviors\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {ann_file}: {e}\")\n",
    "    \n",
    "    # Create dataset entries\n",
    "    dataset_entries = []\n",
    "    \n",
    "    # Process raw images\n",
    "    for img_path in raw_images:\n",
    "        filename = os.path.basename(img_path)\n",
    "        # Parse: FRAMEID_PERSONID_r.ext\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            frame_id = parts[0]\n",
    "            person_id = int(parts[1])\n",
    "            \n",
    "            if frame_id in annotations_dict:\n",
    "                behaviors = annotations_dict[frame_id]\n",
    "                if person_id <= len(behaviors):\n",
    "                    true_behavior = behaviors[person_id - 1].strip()  # person_id is 1-indexed\n",
    "                    \n",
    "                    dataset_entries.append({\n",
    "                        'image_path': img_path,\n",
    "                        'frame_id': frame_id,\n",
    "                        'person_id': person_id,\n",
    "                        'true_behavior': true_behavior,\n",
    "                        'crop_type': 'raw',\n",
    "                        'filename': filename\n",
    "                    })\n",
    "    \n",
    "    # Process extended images\n",
    "    for img_path in extended_images:\n",
    "        filename = os.path.basename(img_path)\n",
    "        # Parse: FRAMEID_PERSONID_x.ext\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            frame_id = parts[0]\n",
    "            person_id = int(parts[1])\n",
    "            \n",
    "            if frame_id in annotations_dict:\n",
    "                behaviors = annotations_dict[frame_id]\n",
    "                if person_id <= len(behaviors):\n",
    "                    true_behavior = behaviors[person_id - 1].strip()  # person_id is 1-indexed\n",
    "                    \n",
    "                    dataset_entries.append({\n",
    "                        'image_path': img_path,\n",
    "                        'frame_id': frame_id,\n",
    "                        'person_id': person_id,\n",
    "                        'true_behavior': true_behavior,\n",
    "                        'crop_type': 'extended',\n",
    "                        'filename': filename\n",
    "                    })\n",
    "    \n",
    "    df_dataset = pd.DataFrame(dataset_entries)\n",
    "    print(f\"üìä Dataset loaded: {len(df_dataset)} total entries\")\n",
    "    \n",
    "    return df_dataset, annotations_dict\n",
    "\n",
    "def classify_with_blip_bart(image_path, blip_model, blip_processor, classifier, device):\n",
    "    \"\"\"\n",
    "    1. Generate caption using BLIP\n",
    "    2. Classify caption using BART\n",
    "    \"\"\"\n",
    "    \n",
    "    behavior_classes = [\n",
    "        \"walking\",\n",
    "        \"running\",\n",
    "        \"pushing a stroller\",\n",
    "        \"biking\",\n",
    "        \"standing\",\n",
    "        \"skateboarding\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Step 1: Generate caption with BLIP\n",
    "        inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = blip_model.generate(**inputs, max_length=50, do_sample=False, num_beams=5)\n",
    "        \n",
    "        caption = blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Step 2: Classify the caption using BART\n",
    "        classification_result = classifier(caption, behavior_classes)\n",
    "        \n",
    "        # Get the best classification\n",
    "        best_label = classification_result['labels'][0]\n",
    "        best_score = classification_result['scores'][0]\n",
    "        \n",
    "        # Map back to simple behavior names\n",
    "        behavior_mapping = {\n",
    "            \"person walking\": \"walking\",\n",
    "            \"person running\": \"running\",\n",
    "            \"person pushing a stroller\": \"pushing a stroller\",\n",
    "            \"person biking or cycling\": \"biking\",\n",
    "            \"person standing still\": \"standing\",\n",
    "            \"person on skateboard\": \"skateboarding\",\n",
    "        }\n",
    "        \n",
    "        final_classification = behavior_mapping.get(best_label, best_label)\n",
    "        \n",
    "        return final_classification, caption, best_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {image_path}: {e}\")\n",
    "        return \"error\", str(e), 0.0\n",
    "\n",
    "def evaluate_blip_bart_model(df_dataset, blip_model, blip_processor, classifier, device, crop_type='raw'):\n",
    "    \"\"\"Evaluate BLIP+BART model on dataset\"\"\"\n",
    "    print(f\"\\nü§ñ EVALUATING BLIP+BART MODEL ON {crop_type.upper()} CROPS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter dataset by crop type\n",
    "    df_eval = df_dataset[df_dataset['crop_type'] == crop_type].copy()\n",
    "    print(f\"üìä Evaluating on {len(df_eval)} {crop_type} images\")\n",
    "    \n",
    "    # Run predictions\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    detailed_results = []\n",
    "    \n",
    "    for idx, row in df_eval.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        true_behavior = row['true_behavior']\n",
    "        \n",
    "        print(f\"üîÑ Processing {idx+1}/{len(df_eval)}: {row['filename']}\")\n",
    "        \n",
    "        # Get prediction\n",
    "        predicted_behavior, caption, confidence = classify_with_blip_bart(\n",
    "            image_path, blip_model, blip_processor, classifier, device\n",
    "        )\n",
    "        \n",
    "        predictions.append(predicted_behavior)\n",
    "        true_labels.append(true_behavior)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'filename': row['filename'],\n",
    "            'frame_id': row['frame_id'],\n",
    "            'person_id': row['person_id'],\n",
    "            'true_behavior': true_behavior,\n",
    "            'predicted_behavior': predicted_behavior,\n",
    "            'caption': caption,\n",
    "            'confidence': float(confidence),\n",
    "            'correct': predicted_behavior == true_behavior,\n",
    "            'crop_type': crop_type\n",
    "        })\n",
    "        \n",
    "        print(f\"   Caption: '{caption}'\")\n",
    "        print(f\"   True: {true_behavior} | Predicted: {predicted_behavior} | Conf: {confidence:.3f} | ‚úÖ\" if predicted_behavior == true_behavior else f\"   True: {true_behavior} | Predicted: {predicted_behavior} | Conf: {confidence:.3f} | ‚ùå\")\n",
    "        \n",
    "        # Clean GPU memory periodically\n",
    "        if torch.cuda.is_available() and idx % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return predictions, true_labels, detailed_results\n",
    "\n",
    "def compute_metrics_blip(true_labels, predictions, detailed_results, crop_type, model_name=\"BLIP+BART\"):\n",
    "    \"\"\"Compute comprehensive evaluation metrics for BLIP+BART\"\"\"\n",
    "    print(f\"\\nüìà COMPUTING METRICS FOR {model_name} ({crop_type.upper()} CROPS)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get unique labels\n",
    "    all_labels = sorted(list(set(true_labels + predictions)))\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        true_labels, predictions, labels=all_labels, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1)\n",
    "    \n",
    "    # Weighted averages\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ OVERALL METRICS:\")\n",
    "    print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   Macro Precision: {macro_precision:.3f}\")\n",
    "    print(f\"   Macro Recall: {macro_recall:.3f}\")\n",
    "    print(f\"   Macro F1: {macro_f1:.3f}\")\n",
    "    print(f\"   Weighted Precision: {weighted_precision:.3f}\")\n",
    "    print(f\"   Weighted Recall: {weighted_recall:.3f}\")\n",
    "    print(f\"   Weighted F1: {weighted_f1:.3f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\nüìä PER-CLASS METRICS:\")\n",
    "    for i, label in enumerate(all_labels):\n",
    "        print(f\"   {label}:\")\n",
    "        print(f\"     Precision: {precision[i]:.3f}\")\n",
    "        print(f\"     Recall: {recall[i]:.3f}\")\n",
    "        print(f\"     F1-Score: {f1[i]:.3f}\")\n",
    "        print(f\"     Support: {support[i]}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=all_labels)\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=all_labels, yticklabels=all_labels, ax=axes[0])\n",
    "    axes[0].set_title(f'{model_name} Confusion Matrix ({crop_type} crops)')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('True')\n",
    "    \n",
    "    # Metrics comparison\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }, index=all_labels)\n",
    "    \n",
    "    metrics_df.plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title(f'{model_name} Per-Class Metrics ({crop_type} crops)')\n",
    "    axes[1].set_xlabel('Behavior')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nüìã DETAILED CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(true_labels, predictions, labels=all_labels, zero_division=0))\n",
    "    \n",
    "    # Error Analysis\n",
    "    df_results = pd.DataFrame(detailed_results)\n",
    "    errors = df_results[~df_results['correct']]\n",
    "    \n",
    "    print(f\"\\n‚ùå ERROR ANALYSIS:\")\n",
    "    print(f\"   Total errors: {len(errors)}/{len(df_results)} ({len(errors)/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(f\"\\n   Most common error patterns:\")\n",
    "        error_patterns = errors.groupby(['true_behavior', 'predicted_behavior']).size().sort_values(ascending=False)\n",
    "        for (true_b, pred_b), count in error_patterns.head(5).items():\n",
    "            print(f\"     {true_b} ‚Üí {pred_b}: {count} times\")\n",
    "        \n",
    "        # Show some example captions for errors\n",
    "        print(f\"\\n   Sample error captions:\")\n",
    "        for idx, row in errors.head(3).iterrows():\n",
    "            print(f\"     {row['true_behavior']} ‚Üí {row['predicted_behavior']}: '{row['caption']}'\")\n",
    "    \n",
    "    # Confidence analysis for BLIP+BART\n",
    "    df_results = pd.DataFrame(detailed_results)\n",
    "    \n",
    "    print(f\"\\nüéØ CONFIDENCE ANALYSIS:\")\n",
    "    correct_samples = df_results[df_results['correct']]\n",
    "    incorrect_samples = df_results[~df_results['correct']]\n",
    "    \n",
    "    if len(correct_samples) > 0 and len(incorrect_samples) > 0:\n",
    "        avg_conf_correct = correct_samples['confidence'].mean()\n",
    "        avg_conf_incorrect = incorrect_samples['confidence'].mean()\n",
    "        \n",
    "        print(f\"   Average confidence (correct): {avg_conf_correct:.3f}\")\n",
    "        print(f\"   Average confidence (incorrect): {avg_conf_incorrect:.3f}\")\n",
    "        print(f\"   Confidence gap: {avg_conf_correct - avg_conf_incorrect:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_precision': weighted_precision,\n",
    "        'weighted_recall': weighted_recall,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'per_class_precision': precision,\n",
    "        'per_class_recall': recall,\n",
    "        'per_class_f1': f1,\n",
    "        'support': support,\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': all_labels,\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "\n",
    "def analyze_dataset_statistics_simple(df_dataset):\n",
    "    \"\"\"Generate basic dataset statistics (lighter version for BLIP cell)\"\"\"\n",
    "    print(\"\\nüìä DATASET OVERVIEW\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    total_entries = len(df_dataset)\n",
    "    raw_count = len(df_dataset[df_dataset['crop_type'] == 'raw'])\n",
    "    extended_count = len(df_dataset[df_dataset['crop_type'] == 'extended'])\n",
    "    \n",
    "    print(f\"Total entries: {total_entries}\")\n",
    "    print(f\"Raw crops: {raw_count}\")\n",
    "    print(f\"Extended crops: {extended_count}\")\n",
    "    \n",
    "    # Behavior distribution\n",
    "    behavior_counts = df_dataset['true_behavior'].value_counts()\n",
    "    print(f\"\\nüéØ Behavior Distribution:\")\n",
    "    for behavior, count in behavior_counts.items():\n",
    "        print(f\"  {behavior}: {count}\")\n",
    "    \n",
    "    return behavior_counts\n",
    "\n",
    "def main_blip_bart_evaluation():\n",
    "    \"\"\"Main evaluation function for BLIP+BART\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATASET_PATH = '/kaggle/input/pedestrian-cropped-annot'  \n",
    "    OUTPUT_PATH = '/kaggle/working/blip_bart_evaluation_results'\n",
    "    \n",
    "    print(\"ü§ñ BLIP + BART PEDESTRIAN BEHAVIOR EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n",
    "    print(f\"üìÅ Output path: {OUTPUT_PATH}\")\n",
    "    print(f\"üîß Method: BLIP captioning ‚Üí BART zero-shot classification\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        df_dataset, annotations_dict = load_dataset_with_annotations(DATASET_PATH)\n",
    "        if len(df_dataset) == 0:\n",
    "            print(\"‚ùå No valid dataset entries found!\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Analyze dataset statistics (lighter version)\n",
    "    behavior_counts = analyze_dataset_statistics_simple(df_dataset)\n",
    "    \n",
    "    # Setup BLIP+BART models\n",
    "    blip_model, blip_processor, classifier, device = setup_blip_bart_models()\n",
    "    if blip_model is None:\n",
    "        print(\"‚ùå Failed to load BLIP+BART models!\")\n",
    "        return\n",
    "    \n",
    "    # Evaluate on raw crops\n",
    "    print(f\"\\nüöÄ Starting evaluation on RAW crops...\")\n",
    "    predictions_raw, true_labels_raw, detailed_results_raw = evaluate_blip_bart_model(\n",
    "        df_dataset, blip_model, blip_processor, classifier, device, crop_type='raw'\n",
    "    )\n",
    "    \n",
    "    metrics_raw = compute_metrics_blip(\n",
    "        true_labels_raw, predictions_raw, detailed_results_raw, 'raw', 'BLIP+BART'\n",
    "    )\n",
    "    \n",
    "    # Evaluate on extended crops\n",
    "    print(f\"\\nüöÄ Starting evaluation on EXTENDED crops...\")\n",
    "    predictions_ext, true_labels_ext, detailed_results_ext = evaluate_blip_bart_model(\n",
    "        df_dataset, blip_model, blip_processor, classifier, device, crop_type='extended'\n",
    "    )\n",
    "    \n",
    "    metrics_ext = compute_metrics_blip(\n",
    "        true_labels_ext, predictions_ext, detailed_results_ext, 'extended', 'BLIP+BART'\n",
    "    )\n",
    "    \n",
    "    # Compare raw vs extended\n",
    "    print(f\"\\nüîÑ RAW vs EXTENDED CROPS COMPARISON:\")\n",
    "    print(\"=\" * 50)\n",
    "    comparison_metrics = ['accuracy', 'macro_f1', 'weighted_f1']\n",
    "    for metric in comparison_metrics:\n",
    "        raw_val = metrics_raw[metric]\n",
    "        ext_val = metrics_ext[metric]\n",
    "        improvement = ext_val - raw_val\n",
    "        print(f\"{metric.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Raw: {raw_val:.3f}\")\n",
    "        print(f\"  Extended: {ext_val:.3f}\")\n",
    "        print(f\"  Improvement: {improvement:+.3f} ({'‚úÖ' if improvement > 0 else '‚ùå'})\")\n",
    "    \n",
    "    # Caption analysis\n",
    "    print(f\"\\nüìù CAPTION ANALYSIS:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Analyze captions for correct vs incorrect predictions\n",
    "    df_raw = pd.DataFrame(detailed_results_raw)\n",
    "    df_ext = pd.DataFrame(detailed_results_ext)\n",
    "    \n",
    "    print(f\"Sample captions from RAW crops:\")\n",
    "    for i, row in df_raw.head(3).iterrows():\n",
    "        status = \"‚úÖ\" if row['correct'] else \"‚ùå\"\n",
    "        print(f\"  {status} {row['true_behavior']}: '{row['caption']}'\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    final_results = {\n",
    "        'evaluation_info': {\n",
    "            'captioning_model': 'Salesforce/blip-image-captioning-large',\n",
    "            'classification_model': 'facebook/bart-large-mnli',\n",
    "            'method': 'BLIP captioning + BART zero-shot classification',\n",
    "            'dataset_path': DATASET_PATH,\n",
    "            'total_samples': len(df_dataset),\n",
    "            'raw_samples': len(df_dataset[df_dataset['crop_type'] == 'raw']),\n",
    "            'extended_samples': len(df_dataset[df_dataset['crop_type'] == 'extended']),\n",
    "            'evaluation_date': datetime.now().isoformat()\n",
    "        },\n",
    "        'dataset_statistics': {\n",
    "            'behavior_distribution': behavior_counts.to_dict(),\n",
    "            'total_frames': df_dataset['frame_id'].nunique(),\n",
    "            'avg_persons_per_frame': df_dataset.groupby('frame_id')['person_id'].count().mean()\n",
    "        },\n",
    "        'raw_crops_results': {\n",
    "            'metrics': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                       for k, v in metrics_raw.items() if k != 'detailed_results'},\n",
    "            'detailed_results': detailed_results_raw\n",
    "        },\n",
    "        'extended_crops_results': {\n",
    "            'metrics': {k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                       for k, v in metrics_ext.items() if k != 'detailed_results'},\n",
    "            'detailed_results': detailed_results_ext\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    results_file = os.path.join(OUTPUT_PATH, 'blip_bart_evaluation_complete_results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüéâ BLIP+BART EVALUATION COMPLETE!\")\n",
    "    print(f\"üìÅ Results saved to: {results_file}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ GPU memory cleaned up\")\n",
    "\n",
    "# Run the evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    main_blip_bart_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7926317,
     "sourceId": 12553349,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
