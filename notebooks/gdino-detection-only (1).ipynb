{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12420997,"sourceType":"datasetVersion","datasetId":7834206}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\nimport json\nfrom datetime import datetime\n\n# Configuration\nVIDEO_FOLDER = '/kaggle/input/sequences'\nOUTPUT_FOLDER = '/kaggle/working/detection_results'\nMIN_AREA_PERCENTAGE = 0.005 \n\ndef setup_environment():\n    \"\"\"Install required packages and setup environment\"\"\"\n    print(\"ğŸš€ Setting up Person Detection Pipeline...\")\n    \n    # Install dependencies\n    packages = [\n        \"torch torchvision transformers\",\n        \"opencv-python-headless\",\n        \"tqdm\",\n        \"matplotlib\"\n    ]\n    \n    for package in packages:\n        print(f\"ğŸ“¦ Installing {package}...\")\n        result = subprocess.run(f\"pip install {package}\", shell=True, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(f\"âŒ Error installing {package}: {result.stderr}\")\n        else:\n            print(f\"âœ… {package} installed successfully\")\n\ndef verify_gpu():\n    \"\"\"Verify GPU availability and setup\"\"\"\n    print(\"\\nğŸ”§ GPU VERIFICATION\")\n    print(\"=\" * 40)\n    \n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0)\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        print(f\"âœ… GPU Available: {gpu_name}\")\n        print(f\"ğŸ’¾ GPU Memory: {total_memory:.1f} GB\")\n        \n        # Setup for optimal performance\n        torch.cuda.set_device(0)\n        torch.backends.cudnn.benchmark = True\n        torch.cuda.empty_cache()\n        \n        return \"cuda\"\n    else:\n        print(\"âš ï¸ No GPU available, using CPU\")\n        return \"cpu\"\n\nclass PersonDetector:\n    \"\"\"Clean person detection pipeline using Grounding DINO\"\"\"\n    \n    def __init__(self):\n        self.device = verify_gpu()\n        print(f\"\\nğŸ”„ Loading Grounding DINO model on {self.device}...\")\n        \n        # Load Grounding DINO\n        model_id = \"IDEA-Research/grounding-dino-tiny\"\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n        self.model = self.model.to(self.device)\n        self.model.eval()\n        \n        print(f\"âœ… Model loaded successfully!\")\n        print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n        \n        if self.device == \"cuda\":\n            torch.cuda.empty_cache()\n            allocated = torch.cuda.memory_allocated() / 1024**3\n            print(f\"ğŸ”‹ GPU Memory Used: {allocated:.2f} GB\")\n    \n    def detect_persons(self, image, confidence_threshold=0.3):\n        \"\"\"Detect persons in image with area filtering\"\"\"\n        print(f\"\\nğŸ” Detecting persons...\")\n        \n        # Convert to PIL if needed\n        if isinstance(image, np.ndarray):\n            if len(image.shape) == 3 and image.shape[2] == 3:\n                pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n            else:\n                pil_image = Image.fromarray(image)\n        else:\n            pil_image = image\n        \n        width, height = pil_image.size\n        total_area = width * height\n        min_area = total_area * MIN_AREA_PERCENTAGE\n        \n        print(f\"ğŸ“ Image size: {width} x {height}\")\n        print(f\"ğŸ“Š Total area: {total_area:,} pixels\")\n        print(f\"ğŸ¯ Min area (10%): {min_area:,} pixels\")\n        \n        # Prepare inputs\n        text_prompt = [[\"a person\"]]\n        inputs = self.processor(images=pil_image, text=text_prompt, return_tensors=\"pt\")\n        device_inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n                        for k, v in inputs.items()}\n        \n        # Run detection\n        with torch.no_grad():\n            outputs = self.model(**device_inputs)\n        \n        # Process results\n        target_sizes = torch.tensor([[height, width]]).to(self.device)\n        results = self.processor.post_process_grounded_object_detection(\n            outputs,\n            input_ids=device_inputs.get('input_ids'),\n            box_threshold=confidence_threshold,\n            text_threshold=0,\n            target_sizes=target_sizes\n        )[0]\n        \n        # Filter detections\n        detections = []\n        if len(results['boxes']) > 0:\n            boxes = results['boxes'].cpu().numpy()\n            scores = results['scores'].cpu().numpy()\n            \n            print(f\"ğŸ“¦ Raw detections: {len(boxes)}\")\n            \n            for i, (box, score) in enumerate(zip(boxes, scores)):\n                x1, y1, x2, y2 = box.astype(int)\n                box_width = x2 - x1\n                box_height = y2 - y1\n                box_area = box_width * box_height\n                area_percentage = (box_area / total_area) * 100\n                \n                print(f\"   Box {i+1}: [{x1}, {y1}, {x2}, {y2}]\")\n                print(f\"   Size: {box_width}x{box_height}, Area: {box_area:,} ({area_percentage:.1f}%)\")\n                print(f\"   Confidence: {score:.3f}\")\n                \n                # Apply area filter\n                if box_area >= min_area:\n                    detections.append({\n                        'box': [x1, y1, x2, y2],\n                        'confidence': float(score),\n                        'area_pixels': int(box_area),\n                        'area_percentage': float(area_percentage)\n                    })\n                    print(f\"   âœ… KEPT (area â‰¥ 10%)\")\n                else:\n                    print(f\"   âŒ FILTERED (area < 10%)\")\n        \n        print(f\"ğŸ¯ Final detections: {len(detections)}\")\n        return detections, pil_image\n    \n    def visualize_detections(self, image, detections, frame_name=\"\"):\n        \"\"\"Visualize detections with cropped bounding boxes\"\"\"\n        if len(detections) == 0:\n            print(\"âš ï¸ No detections to visualize\")\n            return\n        \n        # Convert image to numpy array\n        if isinstance(image, Image.Image):\n            img_array = np.array(image)\n        else:\n            img_array = image\n        \n        # First show the overview with all detections\n        num_detections = len(detections)\n        fig_width = min(20, 5 * (num_detections + 1))\n        fig, axes = plt.subplots(1, num_detections + 1, figsize=(fig_width, 6))\n        \n        if num_detections == 1:\n            axes = [axes] if not isinstance(axes, list) else axes\n        elif isinstance(axes, np.ndarray):\n            axes = axes.flatten()\n        \n        # Show original image with all bounding boxes\n        axes[0].imshow(img_array)\n        axes[0].set_title(f\"Original Frame\\n{len(detections)} Person(s) Detected\", \n                         fontsize=12, fontweight='bold')\n        axes[0].axis('off')\n        \n        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]\n        \n        # Draw all bounding boxes on original image\n        for i, det in enumerate(detections):\n            x1, y1, x2, y2 = det['box']\n            color = np.array(colors[i % len(colors)]) / 255.0\n            \n            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n                                   linewidth=3, edgecolor=color, facecolor='none')\n            axes[0].add_patch(rect)\n            \n            # Add label\n            axes[0].text(x1, y1-5, f\"Person {i+1}\\n{det['confidence']:.2f}\",\n                        color=color, fontsize=10, fontweight='bold',\n                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n        \n        # Show individual cropped detections\n        for i, det in enumerate(detections):\n            col = i + 1\n            x1, y1, x2, y2 = det['box']\n            \n            # Crop the detection\n            crop = img_array[y1:y2, x1:x2]\n            \n            if crop.size > 0:\n                axes[col].imshow(crop)\n                title = f\"Person {i+1}\\nConf: {det['confidence']:.3f}\\nArea: {det['area_percentage']:.1f}%\"\n                axes[col].set_title(title, fontsize=11, fontweight='bold')\n            else:\n                axes[col].text(0.5, 0.5, \"Empty\\nCrop\", ha='center', va='center',\n                              transform=axes[col].transAxes, fontsize=12)\n                axes[col].set_title(f\"Person {i+1} - Error\", fontsize=11)\n            \n            axes[col].axis('off')\n        \n        # Main title\n        main_title = f\"Person Detection Results - {frame_name}\"\n        if len(detections) > 0:\n            main_title += f\" | {len(detections)} Person(s) â‰¥ 10% Area\"\n        \n        plt.suptitle(main_title, fontsize=14, fontweight='bold', y=0.95)\n        plt.tight_layout()\n        plt.show()\n        \n        # Now show individual cropped images in console\n        print(f\"\\nğŸ–¼ï¸ DISPLAYING CROPPED BOUNDING BOXES - {frame_name}\")\n        print(\"=\" * 70)\n        \n        for i, det in enumerate(detections):\n            x1, y1, x2, y2 = det['box']\n            crop = img_array[y1:y2, x1:x2]\n            \n            if crop.size > 0:\n                print(f\"\\nğŸ‘¤ PERSON {i+1} CROPPED IMAGE:\")\n                print(f\"   ğŸ“ Bounding Box: [{x1}, {y1}, {x2}, {y2}]\")\n                print(f\"   ğŸ“ Crop Size: {crop.shape[1]}x{crop.shape[0]} pixels\")\n                print(f\"   ğŸ“Š Area: {det['area_pixels']:,} pixels ({det['area_percentage']:.1f}%)\")\n                print(f\"   ğŸ¯ Confidence: {det['confidence']:.3f}\")\n                \n                # Display the cropped image\n                plt.figure(figsize=(6, 8))\n                plt.imshow(crop)\n                plt.title(f\"Person {i+1} - Confidence: {det['confidence']:.3f}\\nArea: {det['area_percentage']:.1f}% | Size: {crop.shape[1]}x{crop.shape[0]}\", \n                         fontsize=14, fontweight='bold')\n                plt.axis('off')\n                plt.tight_layout()\n                plt.show()\n                print(f\"   âœ… Cropped image displayed above â†‘\")\n            else:\n                print(f\"\\nâŒ PERSON {i+1}: Empty crop (invalid bounding box)\")\n        \n        print(\"=\" * 70)\n\ndef process_single_frame(detector, frame_path):\n    \"\"\"Process a single frame\"\"\"\n    frame_name = os.path.basename(frame_path)\n    print(f\"\\nğŸ¬ PROCESSING: {frame_name}\")\n    print(\"=\" * 50)\n    \n    try:\n        # Load image\n        image = cv2.imread(frame_path)\n        if image is None:\n            print(f\"âŒ Could not load image: {frame_path}\")\n            return None\n        \n        # Detect persons\n        detections, pil_image = detector.detect_persons(image, confidence_threshold=0.5)\n        \n        if len(detections) == 0:\n            print(\"âŒ No valid person detections (â‰¥10% area)\")\n            return None\n        \n        # Visualize results\n        detector.visualize_detections(pil_image, detections, frame_name)\n        \n        # Clean GPU memory\n        if detector.device == \"cuda\":\n            torch.cuda.empty_cache()\n        \n        return {\n            'frame_name': frame_name,\n            'detections_count': len(detections),\n            'detections': detections\n        }\n        \n    except Exception as e:\n        print(f\"âŒ Error processing frame: {e}\")\n        if detector.device == \"cuda\":\n            torch.cuda.empty_cache()\n        return None\n\ndef process_video_frames(detector, video_path, max_frames=10, skip_frames=30):\n    \"\"\"Process frames from a video with skip frame parameter\n    \n    Args:\n        detector: PersonDetector instance\n        video_path: Path to video file\n        max_frames: Maximum number of frames to process\n        skip_frames: Process every Nth frame (1 = every frame, 30 = every 30th frame)\n    \"\"\"\n    video_name = os.path.basename(video_path).split('.')[0]\n    print(f\"\\nğŸ“¹ PROCESSING VIDEO: {video_name}\")\n    print(\"=\" * 60)\n    \n    try:\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            print(f\"âŒ Could not open video: {video_path}\")\n            return None\n        \n        # Get video properties\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        print(f\"ğŸ“Š Video Properties:\")\n        print(f\"   ğŸ“ Resolution: {width}x{height}\")\n        print(f\"   ğŸï¸ FPS: {fps}\")\n        print(f\"   ğŸ“Š Total frames: {total_frames}\")\n        print(f\"   â±ï¸ Duration: {total_frames/fps:.1f} seconds\")\n        print(f\"âš™ï¸ Processing Settings:\")\n        print(f\"   ğŸ¯ Max frames to process: {max_frames}\")\n        print(f\"   â­ï¸ Skip frames: {skip_frames} (process every {skip_frames} frames)\")\n        print(f\"   ğŸ“ˆ Estimated frames to process: {min(max_frames, total_frames // skip_frames)}\")\n        \n        # Create output directory\n        output_dir = os.path.join(OUTPUT_FOLDER, video_name)\n        os.makedirs(output_dir, exist_ok=True)\n        \n        frame_results = []\n        frames_processed = 0\n        frame_count = 0\n        \n        print(f\"\\nğŸš€ Starting frame processing...\")\n        \n        while frames_processed < max_frames and cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            if frame_count % skip_frames == 0:\n                frame_filename = f\"frame_{frame_count:06d}.jpg\"\n                frame_path = os.path.join(output_dir, frame_filename)\n                cv2.imwrite(frame_path, frame)  \n                # Process frame\n                result = process_single_frame(detector, frame_path)\n                if result:\n                    result['frame_number'] = frame_count\n                    result['timestamp_seconds'] = frame_count / fps if fps > 0 else 0\n                    frame_results.append(result)\n                    print(f\"âœ… Frame processed successfully: {result['detections_count']} detections\")\n                else:\n                    print(f\"âš ï¸ Frame processed: No valid detections\")\n                \n                frames_processed += 1\n                \n                # Show progress\n                remaining = max_frames - frames_processed\n                print(f\"\\nğŸ“ˆ PROGRESS: {frames_processed}/{max_frames} frames processed\")\n                print(f\"ğŸ“Š Frames with detections: {len(frame_results)}\")\n                print(f\"ğŸ‘¥ Total detections so far: {sum(r['detections_count'] for r in frame_results)}\")\n                if remaining > 0:\n                    print(f\"â³ Remaining: {remaining} frames\")\n                print(f\"{'='*80}\")\n            \n            frame_count += 1\n        \n        cap.release()\n        \n        # Save results\n        results_summary = {\n            'video_name': video_name,\n            'video_path': video_path,\n            'processing_settings': {\n                'max_frames': max_frames,\n                'skip_frames': skip_frames,\n                'frames_actually_processed': frames_processed\n            },\n            'video_properties': {\n                'total_frames': total_frames,\n                'fps': fps,\n                'width': width,\n                'height': height,\n                'duration_seconds': total_frames / fps if fps > 0 else 0\n            },\n            'processing_info': {\n                'frames_processed': frames_processed,\n                'frames_with_detections': len(frame_results),\n                'total_detections': sum(r['detections_count'] for r in frame_results),\n                'detection_rate': len(frame_results) / frames_processed if frames_processed > 0 else 0,\n                'avg_detections_per_frame': sum(r['detections_count'] for r in frame_results) / frames_processed if frames_processed > 0 else 0\n            },\n            'frame_results': frame_results,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        results_path = os.path.join(output_dir, f\"{video_name}_detection_results.json\")\n        with open(results_path, 'w') as f:\n            json.dump(results_summary, f, indent=2)\n        \n        print(f\"\\nâœ… VIDEO PROCESSING COMPLETE!\")\n        print(f\"ğŸ“Š Final Statistics:\")\n        print(f\"   ğŸï¸ Frames processed: {frames_processed}\")\n        print(f\"   ğŸ­ Frames with detections: {len(frame_results)} ({len(frame_results)/frames_processed*100:.1f}%)\")\n        print(f\"   ğŸ‘¥ Total persons detected: {sum(r['detections_count'] for r in frame_results)}\")\n        print(f\"   ğŸ“ˆ Average persons per frame: {sum(r['detections_count'] for r in frame_results)/frames_processed:.1f}\")\n        print(f\"   ğŸ’¾ Results saved: {results_path}\")\n        \n        return results_summary\n        \n    except Exception as e:\n        print(f\"âŒ Error processing video: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    print(\"ğŸš€ PERSON DETECTION PIPELINE\")\n    print(\"=\" * 60)\n    print(\"ğŸ¯ Detects persons with bounding box area â‰¥ 10%\")\n    print(\"ğŸ“¦ Shows cropped detections with confidence values\")\n    print(\"ğŸ–¼ï¸ Displays individual cropped images in console\")\n    print(\"â­ï¸ Configurable frame skipping for efficient processing\")\n    print(\"=\" * 60)\n    \n    MAX_FRAMES_PER_VIDEO = 1000    \n    SKIP_FRAMES = 10               \n    CONFIDENCE_THRESHOLD = 0.5      \n    \n    print(f\"âš™ï¸ PROCESSING CONFIGURATION:\")\n    print(f\"   ğŸï¸ Max frames per video: {MAX_FRAMES_PER_VIDEO}\")\n    print(f\"   â­ï¸ Skip frames: {SKIP_FRAMES} (process every {SKIP_FRAMES} frames)\")\n    print(f\"   ğŸ¯ Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n    print(f\"   ğŸ“Š Min area requirement: {MIN_AREA_PERCENTAGE*100}% of image\")\n    print(\"=\" * 60)\n    \n    # Setup environment\n    setup_environment()\n    \n    # Check input folder\n    if not os.path.exists(VIDEO_FOLDER):\n        print(f\"âŒ Video folder not found: {VIDEO_FOLDER}\")\n        print(\"ğŸ“ Available paths:\")\n        if os.path.exists('/kaggle/input'):\n            for item in os.listdir('/kaggle/input'):\n                print(f\"   - /kaggle/input/{item}\")\n        return\n    \n    # Find video files\n    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']\n    video_files = []\n    \n    for file in os.listdir(VIDEO_FOLDER):\n        if any(file.lower().endswith(ext) for ext in video_extensions):\n            video_files.append(file)\n    \n    if not video_files:\n        print(f\"âŒ No video files found in {VIDEO_FOLDER}\")\n        print(\"ğŸ“ Available files:\")\n        for item in os.listdir(VIDEO_FOLDER):\n            print(f\"   - {item}\")\n        return\n    \n    print(f\"\\nğŸ¯ Found {len(video_files)} videos:\")\n    for i, video in enumerate(video_files, 1):\n        print(f\"   {i}. {video}\")\n    \n    # Create output directory\n    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n    \n    # Initialize detector\n    detector = PersonDetector()\n    \n    # Process videos\n    all_results = []\n    total_videos = len(video_files)\n    \n    for i, video_file in enumerate(video_files, 1):\n        video_path = os.path.join(VIDEO_FOLDER, video_file)\n        print(f\"ğŸ¬ PROCESSING VIDEO {i}/{total_videos}: {video_file}\")\n        \n        try:\n            result = process_video_frames(\n                detector, \n                video_path, \n                max_frames=MAX_FRAMES_PER_VIDEO, \n                skip_frames=SKIP_FRAMES\n            )\n            if result:\n                all_results.append(result)\n                print(f\"âœ… Video {i} completed successfully!\")\n            else:\n                print(f\"âŒ Video {i} processing failed!\")\n        \n        except Exception as e:\n            print(f\"âŒ Error processing video {i}: {e}\")\n            import traceback\n            traceback.print_exc()\n        \n        # Clean GPU memory between videos\n        if detector.device == \"cuda\":\n            torch.cuda.empty_cache()\n            print(f\"ğŸ§¹ GPU memory cleared between videos\")\n        if i < total_videos:\n            print(f\"\\n{'â³'*30}\")\n            print(f\"â³ Moving to next video... ({i}/{total_videos} completed)\")\n            print(f\"{'â³'*30}\")\n    \n    print(f\"\\n{'ğŸ‰'*50}\")\n    print(f\"ğŸ‰ ALL VIDEOS PROCESSING COMPLETE!\")\n    print(f\"{'ğŸ‰'*50}\")\n    \n    successful_videos = len(all_results)\n    failed_videos = total_videos - successful_videos\n    \n    print(f\"ğŸ“Š OVERALL STATISTICS:\")\n    print(f\"   ğŸ“¹ Total videos found: {total_videos}\")\n    print(f\"   âœ… Videos processed successfully: {successful_videos}\")\n    print(f\"   âŒ Videos failed: {failed_videos}\")\n    \n    if successful_videos > 0:\n        total_detections = sum(r['processing_info']['total_detections'] for r in all_results)\n        total_frames = sum(r['processing_info']['frames_processed'] for r in all_results)\n        total_frames_with_detections = sum(r['processing_info']['frames_with_detections'] for r in all_results)\n        \n        print(f\"   ğŸï¸ Total frames processed: {total_frames}\")\n        print(f\"   ğŸ­ Frames with detections: {total_frames_with_detections} ({total_frames_with_detections/total_frames*100:.1f}%)\")\n        print(f\"   ğŸ‘¥ Total persons detected: {total_detections}\")\n        print(f\"   ğŸ“Š Average persons per frame: {total_detections/total_frames:.1f}\")\n        \n        print(f\"\\nğŸ“‹ PER-VIDEO BREAKDOWN:\")\n        print(\"-\" * 80)\n        print(f\"{'Video':<25} | {'Frames':<7} | {'Detected':<9} | {'Rate':<6} | {'Persons':<7}\")\n        print(\"-\" * 80)\n        for result in all_results:\n            frames = result['processing_info']['frames_processed']\n            det_frames = result['processing_info']['frames_with_detections']\n            rate = det_frames / frames * 100 if frames > 0 else 0\n            persons = result['processing_info']['total_detections']\n            print(f\"{result['video_name']:<25} | {frames:<7} | {det_frames:<9} | {rate:<6.1f}% | {persons:<7}\")\n        print(\"-\" * 80)\n    \n    print(f\"ğŸ“ All results saved in: {OUTPUT_FOLDER}\")\n    if detector.device == \"cuda\":\n        torch.cuda.empty_cache()\n        print(\"ğŸ§¹ Final GPU memory cleanup completed\")\n    \n    print(f\"ğŸ Pipeline execution finished!\")\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T12:27:50.377296Z","iopub.execute_input":"2025-07-14T12:27:50.377680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}