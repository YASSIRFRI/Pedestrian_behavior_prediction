{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12680189,"sourceType":"datasetVersion","datasetId":8004235}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/deepseek-ai/DeepSeek-VL.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T11:17:58.394255Z","iopub.execute_input":"2025-08-05T11:17:58.394538Z","iopub.status.idle":"2025-08-05T11:19:20.788667Z","shell.execute_reply.started":"2025-08-05T11:17:58.394518Z","shell.execute_reply":"2025-08-05T11:19:20.787749Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/deepseek-ai/DeepSeek-VL.git\n  Cloning https://github.com/deepseek-ai/DeepSeek-VL.git to /tmp/pip-req-build-_gtry3d5\n  Running command git clone --filter=blob:none --quiet https://github.com/deepseek-ai/DeepSeek-VL.git /tmp/pip-req-build-_gtry3d5\n  Resolved https://github.com/deepseek-ai/DeepSeek-VL.git to commit 681bffb4519856ad27cc17531aacde31ddf6f1a7\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (4.52.4)\nRequirement already satisfied: timm>=0.9.16 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (1.0.15)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (1.8.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (0.2.0)\nCollecting attrdict (from deepseek_vl==1.0.0)\n  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (0.8.1)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.1->deepseek_vl==1.0.0) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (25.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (0.21.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->deepseek_vl==1.0.0) (7.0.0)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from attrdict->deepseek_vl==1.0.0) (1.17.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.9.16->deepseek_vl==1.0.0) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->deepseek_vl==1.0.0) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2025.6.15)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=0.9.16->deepseek_vl==1.0.0) (11.2.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: deepseek_vl\n  Building wheel for deepseek_vl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for deepseek_vl: filename=deepseek_vl-1.0.0-py3-none-any.whl size=58999 sha256=666436d7248e4df9ddccfb8772e1d3f86d9f5fddb45843a4a1c2d46ac5a839c0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mtofe3ae/wheels/f6/29/b1/e1b943f55abbed361464f2d1635fa8ee8ada935755f5e315e6\nSuccessfully built deepseek_vl\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, attrdict, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepseek_vl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed attrdict-2.0.1 deepseek_vl-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image, ImageEnhance\nfrom transformers import (\n    AutoProcessor, \n    AutoModelForZeroShotObjectDetection,\n    AutoModelForCausalLM, \n    pipeline\n)\nimport json\nimport xml.etree.ElementTree as ET\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    accuracy_score,\n    precision_recall_fscore_support,\n    precision_recall_curve,\n    roc_curve,\n    auc\n)\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nimport warnings\nimport gc\nimport time\nwarnings.filterwarnings('ignore')\n\n# DeepSeek-VL imports\nfrom deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\nfrom deepseek_vl.utils.io import load_pil_images\n\n# Configuration\nDATASET_FOLDER = '/kaggle/input/pedestrian-behavior-on-sidewalk'\nIMAGES_FOLDER = '/kaggle/input/pedestrian-behavior-on-sidewalk' \nANNOTATIONS_FOLDER = '/kaggle/input/pedestrian-behavior-on-sidewalk'  \nOUTPUT_FOLDER = '/kaggle/working/pipeline_results'\nCONFIDENCE_THRESHOLD = 0.5\nIOU_THRESHOLD = 0.1\nCONTAINMENT_THRESHOLD = 0.3\nMIN_PERSON_AREA_PERCENTAGE = 0.5\nIOU_MAPPING_THRESHOLD = 0.8\n\n# Detection classes\nDETECTION_CLASSES = [\"person\", \"bike\", \"skateboard\", \"baby stroller\", \"wheelchair\"]\n\ndef setup_environment():\n    \"\"\"Install required packages\"\"\"\n    print(\"🚀 Setting up Pedestrian Behavior Dynamics Pipeline...\")\n    packages = [\n        \"torch torchvision transformers>=4.45.0\", \n        \"opencv-python-headless\", \n        \"tqdm\", \n        \"matplotlib\", \n        \"seaborn\",\n        \"scikit-learn\",\n        \"pandas\",\n        \"accelerate\"\n    ]\n    \n    for package in packages:\n        print(f\"📦 Installing {package}...\")\n        result = subprocess.run(f\"pip install {package}\", shell=True, capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"✅ {package} installed successfully\")\n\ndef calculate_iou(box1, box2):\n    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n    \n    # Calculate intersection\n    x1_i = max(x1_1, x1_2)\n    y1_i = max(y1_1, y1_2)\n    x2_i = min(x2_1, x2_2)\n    y2_i = min(y2_1, y2_2)\n    \n    if x2_i <= x1_i or y2_i <= y1_i:\n        return 0.0\n    \n    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n    union = area1 + area2 - intersection\n    \n    return intersection / union if union > 0 else 0.0\n\ndef calculate_containment(box1, box2):\n    \"\"\"Calculate what percentage of box1 is contained within box2\"\"\"\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n    \n    # Calculate intersection\n    x1_i = max(x1_1, x1_2)\n    y1_i = max(y1_1, y1_2)\n    x2_i = min(x2_1, x2_2)\n    y2_i = min(y2_1, y2_2)\n    \n    if x2_i <= x1_i or y2_i <= y1_i:\n        return 0.0\n    \n    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    \n    return intersection / area1 if area1 > 0 else 0.0\n\ndef should_merge_person_nonperson(det1, det2, iou_threshold=0.1, containment_threshold=0.7):\n    \"\"\"Modified merging logic: Only merge person with non-person labels\"\"\"\n    box1, box2 = det1['box'], det2['box']\n    \n    # Calculate IoU\n    iou = calculate_iou(box1, box2)\n    if iou > iou_threshold:\n        return True, \"iou\"\n    \n    # Calculate areas\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    \n    # Check containment for smaller objects (like skateboards in person boxes)\n    if area1 < area2:  # box1 is smaller\n        containment = calculate_containment(box1, box2)\n        if containment > containment_threshold:\n            return True, f\"containment_{containment:.2f}\"\n    else:  # box2 is smaller\n        containment = calculate_containment(box2, box1)\n        if containment > containment_threshold:\n            return True, f\"containment_{containment:.2f}\"\n    \n    return False, \"no_merge\"\n\ndef merge_person_nonperson_boxes(detections, iou_threshold=0.1, containment_threshold=0.7):\n    \"\"\"Merge overlapping bounding boxes but ONLY merge person with non-person\"\"\"\n    if len(detections) <= 1:\n        return detections\n    \n    merged = []\n    used = set()\n    \n    for i, det1 in enumerate(detections):\n        if i in used:\n            continue\n            \n        overlapping = [det1]\n        used.add(i)\n        merge_reasons = []\n        \n        for j, det2 in enumerate(detections[i+1:], i+1):\n            if j in used:\n                continue\n                \n            should_merge, reason = should_merge_person_nonperson(det1, det2, iou_threshold, containment_threshold)\n            \n            if should_merge:\n                overlapping.append(det2)\n                merge_reasons.append(f\"box_{j}_{reason}\")\n                used.add(j)\n        \n        if len(overlapping) == 1:\n            merged.append(overlapping[0])\n        else:\n            # Merge overlapping detections - take the one with highest confidence\n            best_det = max(overlapping, key=lambda x: x['confidence'])\n            \n            # Expand bounding box to encompass all overlapping boxes\n            all_boxes = [det['box'] for det in overlapping]\n            x1_min = min(box[0] for box in all_boxes)\n            y1_min = min(box[1] for box in all_boxes)\n            x2_max = max(box[2] for box in all_boxes)\n            y2_max = max(box[3] for box in all_boxes)\n            \n            merged_det = {\n                'box': [x1_min, y1_min, x2_max, y2_max],\n                'confidence': best_det['confidence'],\n                'merged_count': len(overlapping),\n                'merge_reasons': merge_reasons\n            }\n            merged.append(merged_det)\n    \n    return merged\n\ndef parse_xml_annotation(xml_path):\n    \"\"\"Parse XML annotation file to extract ground truth bounding boxes and labels\"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        \n        annotations = []\n        image_info = {}\n        \n        # Get image dimensions\n        size_elem = root.find('size')\n        if size_elem is not None:\n            try:\n                image_info['width'] = int(size_elem.find('width').text)\n                image_info['height'] = int(size_elem.find('height').text)\n                image_info['depth'] = int(size_elem.find('depth').text)\n            except:\n                pass\n        \n        for obj in root.findall('object'):\n            name_elem = obj.find('name')\n            if name_elem is None or not name_elem.text:\n                continue\n                \n            name = name_elem.text.strip()\n            if not name:\n                continue\n                \n            bbox = obj.find('bndbox')\n            if bbox is None:\n                continue\n                \n            try:\n                x1 = int(bbox.find('xmin').text)\n                y1 = int(bbox.find('ymin').text)\n                x2 = int(bbox.find('xmax').text)\n                y2 = int(bbox.find('ymax').text)\n                \n                annotations.append({\n                    'label': name,\n                    'bbox': [x1, y1, x2, y2],\n                    'area': (x2 - x1) * (y2 - y1)\n                })\n            except (ValueError, AttributeError):\n                continue\n        \n        return annotations, image_info\n    except Exception as e:\n        print(f\"❌ Error parsing {xml_path}: {e}\")\n        return [], {}\n\ndef analyze_dataset_statistics(image_files, images_folder, annotations_folder):\n    \"\"\"Analyze and display comprehensive dataset statistics\"\"\"\n    print(f\"\\n📊 DATASET STATISTICS ANALYSIS\")\n    print(\"=\" * 80)\n    \n    dataset_stats = {\n        'images': [],\n        'annotations': [],\n        'labels': [],\n        'bbox_areas': [],\n        'bbox_aspect_ratios': [],\n        'image_sizes': [],\n        'objects_per_image': []\n    }\n    \n    print(f\"🔍 Analyzing {len(image_files)} images...\")\n    \n    for image_file in tqdm(image_files, desc=\"Analyzing dataset\"):\n        image_path = os.path.join(images_folder, image_file)\n        image_id = os.path.splitext(image_file)[0]\n        xml_path = os.path.join(annotations_folder, f\"{image_id}.xml\")\n        \n        # Load image to get actual dimensions\n        image = cv2.imread(image_path)\n        if image is not None:\n            h, w, c = image.shape\n            dataset_stats['image_sizes'].append((w, h))\n            dataset_stats['images'].append(image_file)\n        \n        # Parse annotations\n        if os.path.exists(xml_path):\n            annotations, image_info = parse_xml_annotation(xml_path)\n            dataset_stats['objects_per_image'].append(len(annotations))\n            \n            for ann in annotations:\n                dataset_stats['annotations'].append(ann)\n                dataset_stats['labels'].append(ann['label'])\n                \n                # Calculate bbox statistics\n                x1, y1, x2, y2 = ann['bbox']\n                width = x2 - x1\n                height = y2 - y1\n                area = width * height\n                aspect_ratio = width / height if height > 0 else 0\n                \n                dataset_stats['bbox_areas'].append(area)\n                dataset_stats['bbox_aspect_ratios'].append(aspect_ratio)\n    \n    # Display comprehensive statistics\n    print(f\"\\n📈 DATASET OVERVIEW:\")\n    print(f\"   📷 Total Images: {len(dataset_stats['images'])}\")\n    print(f\"   📦 Total Annotations: {len(dataset_stats['annotations'])}\")\n    print(f\"   🏷️  Unique Labels: {len(set(dataset_stats['labels']))}\")\n    \n    # Label distribution\n    label_counts = Counter(dataset_stats['labels'])\n    print(f\"\\n🏷️  LABEL DISTRIBUTION:\")\n    for label, count in label_counts.most_common():\n        percentage = (count / len(dataset_stats['labels'])) * 100\n        print(f\"   {label}: {count} ({percentage:.1f}%)\")\n    \n    return dataset_stats\n\nclass GroundingDINODetector:\n    \"\"\"Grounding DINO detector for pedestrian behavior detection\"\"\"\n    \n    def __init__(self):\n        # Setup device\n        if torch.cuda.is_available():\n            self.device = \"cuda\"\n            print(f\"✅ GPU Available: {torch.cuda.get_device_name(0)}\")\n            torch.cuda.empty_cache()\n        else:\n            self.device = \"cpu\"\n            print(\"⚠️ Using CPU\")\n        \n        # Load Grounding DINO\n        print(\"🔄 Loading Grounding DINO model...\")\n        model_id = \"IDEA-Research/grounding-dino-tiny\"\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n        self.model = self.model.to(self.device)\n        self.model.eval()\n        print(\"✅ Grounding DINO loaded successfully!\")\n    \n    def detect_objects(self, image, classes=DETECTION_CLASSES, confidence_threshold=CONFIDENCE_THRESHOLD):\n        \"\"\"Detect multiple object classes in image using same logic as original pipeline\"\"\"\n        # Convert to PIL if needed\n        if isinstance(image, np.ndarray):\n            if len(image.shape) == 3 and image.shape[2] == 3:\n                pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n            else:\n                pil_image = Image.fromarray(image)\n        else:\n            pil_image = image\n        \n        width, height = pil_image.size\n        total_area = width * height\n        min_person_area = total_area * (MIN_PERSON_AREA_PERCENTAGE / 100)\n        \n        # Create text prompt for all classes\n        text_prompt = [[class_name for class_name in classes]]\n        \n        # Prepare inputs\n        inputs = self.processor(images=pil_image, text=text_prompt, return_tensors=\"pt\")\n        device_inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n                        for k, v in inputs.items()}\n        \n        # Run detection\n        with torch.no_grad():\n            outputs = self.model(**device_inputs)\n        \n        # Process results\n        target_sizes = torch.tensor([[height, width]]).to(self.device)\n        results = self.processor.post_process_grounded_object_detection(\n            outputs,\n            input_ids=device_inputs.get('input_ids'),\n            box_threshold=confidence_threshold,\n            text_threshold=0,\n            target_sizes=target_sizes\n        )[0]\n        \n        # Extract detections\n        raw_detections = []\n        if len(results['boxes']) > 0:\n            boxes = results['boxes'].cpu().numpy()\n            scores = results['scores'].cpu().numpy()\n            \n            for box, score in zip(boxes, scores):\n                x1, y1, x2, y2 = box.astype(int)\n                box_area = (x2 - x1) * (y2 - y1)\n                \n                detection = {\n                    'box': [x1, y1, x2, y2],\n                    'confidence': float(score),\n                    'area': box_area,\n                    'area_percentage': (box_area / total_area) * 100\n                }\n                raw_detections.append(detection)\n        \n        # Merge overlapping detections using modified logic (person with non-person only)\n        merged_detections = merge_person_nonperson_boxes(\n            raw_detections, IOU_THRESHOLD, CONTAINMENT_THRESHOLD\n        )\n        \n        # Filter by area\n        valid_detections = []\n        for det in merged_detections:\n            box_area = det['area'] if 'area' in det else (det['box'][2] - det['box'][0]) * (det['box'][3] - det['box'][1])\n            area_percentage = (box_area / total_area) * 100\n            \n            # Update detection with area info\n            det['area'] = box_area\n            det['area_percentage'] = area_percentage\n            \n            # Only keep detections with sufficient area\n            if area_percentage >= MIN_PERSON_AREA_PERCENTAGE:\n                valid_detections.append(det)\n        \n        return valid_detections, pil_image\n\nclass DeepSeekBARTClassifier:\n    \"\"\"DeepSeek-VL for caption generation + BART for classification\"\"\"\n    \n    def __init__(self):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"🔄 Loading DeepSeek-VL + BART on {self.device}...\")\n        \n        # Clear GPU memory first\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Load DeepSeek-VL\n        model_path = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n        self.vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n        self.tokenizer = self.vl_chat_processor.tokenizer\n        \n        self.vl_gpt = AutoModelForCausalLM.from_pretrained(\n            model_path, \n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32\n        )\n        \n        if self.device == \"cuda\":\n            self.vl_gpt = self.vl_gpt.cuda().eval()\n        else:\n            self.vl_gpt = self.vl_gpt.eval()\n            \n        print(\"✅ DeepSeek-VL loaded successfully!\")\n        \n        # Force GPU memory cleanup before loading BART\n        if self.device == \"cuda\":\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Load BART classifier\n        print(\"🔄 Loading BART classifier...\")\n        try:\n            if self.device == \"cuda\":\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\",\n                    torch_dtype=torch.bfloat16\n                )\n            else:\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\",\n                    device=-1\n                )\n        except Exception as e:\n            print(f\"⚠️ Accelerate loading failed: {e}\")\n            if self.device == \"cuda\":\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\"\n                )\n            else:\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\",\n                    device=-1\n                )\n        \n        print(\"✅ BART loaded successfully!\")\n        print(\"✅ DeepSeek-VL + BART classifier ready!\")\n    \n    def generate_caption(self, image_path):\n        \"\"\"Generate caption using DeepSeek-VL\"\"\"\n        try:\n            conversation = [\n                {\n                    \"role\": \"User\",\n                    \"content\": \"<image_placeholder>Classify the activity of the pedestrian on the sidewalk based on posture, movement. Focus on functional cues like body position, motion, use of hands or feet, and any relevant mobility devices (e.g., bike, scooter, stroller, wheelchair). Answer in a concise sentence by picking one class: walking slowly, running fast, jogging, using a bicycle, standing up on the sidewalk (still), skateboarding, running, using some sort of a scooter, pushing a baby stroller (or baby chair), using a wheelchair.\",\n                    \"images\": [image_path]\n                },\n                {\n                    \"role\": \"Assistant\",\n                    \"content\": \"\"\n                }\n            ]\n\n            # Load images and prepare for inputs\n            pil_images = load_pil_images(conversation)\n            prepare_inputs = self.vl_chat_processor(\n                conversations=conversation,\n                images=pil_images,\n                force_batchify=True\n            ).to(self.vl_gpt.device)\n\n            # Run image encoder to get the image embeddings\n            inputs_embeds = self.vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n\n            outputs = self.vl_gpt.language_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=prepare_inputs.attention_mask,\n                pad_token_id=self.tokenizer.eos_token_id,\n                bos_token_id=self.tokenizer.bos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                max_new_tokens=512,\n                early_stopping=False,\n                do_sample=False,\n                num_beams=3,\n            )\n\n            answer = self.tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n            \n            # Extract just the assistant's response\n            conversation_text = prepare_inputs['sft_format'][0]\n            if conversation_text in answer:\n                caption = answer.replace(conversation_text, \"\").strip()\n            else:\n                caption = answer.strip()\n            \n            return caption\n            \n        except Exception as e:\n            print(f\"❌ Error generating caption: {e}\")\n            return \"A person in an image.\"\n    \n    def classify_behavior(self, image):\n        \"\"\"Classify behavior using DeepSeek-VL + BART pipeline\"\"\"\n        behavior_classes = [\n            \"walking\", \"running\", \"pushing a stroller\", \n            \"biking\", \"standing\", \"skateboarding\", \"scootering\", \n            \"person on wheelchair\"\n        ]\n        \n        try:\n            # Convert to PIL and save temporarily\n            if isinstance(image, np.ndarray):\n                if len(image.shape) == 3 and image.shape[2] == 3:\n                    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n                else:\n                    pil_image = Image.fromarray(image)\n            else:\n                pil_image = image\n            \n            # Save temp image for DeepSeek-VL\n            temp_path = \"/tmp/temp_crop.jpg\"\n            pil_image.save(temp_path)\n            \n            # Generate caption with DeepSeek-VL\n            caption = self.generate_caption(temp_path)\n            print(f\"   📝 Caption: {caption}\")\n            \n            # Classify with BART\n            result = self.bart_classifier(caption, behavior_classes)\n            predicted_behavior = result['labels'][0]\n            confidence = result['scores'][0]\n            \n            print(f\"   🎯 Classification: {predicted_behavior} (confidence: {confidence:.3f})\")\n            \n            # Clean up temp file\n            if os.path.exists(temp_path):\n                os.remove(temp_path)\n            \n            # Clean up GPU memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return predicted_behavior, confidence, result\n            \n        except Exception as e:\n            print(f\"❌ Error classifying behavior: {e}\")\n            return \"unknown\", 0.0, {\"labels\": [\"unknown\"], \"scores\": [0.0]}\n\ndef map_detections_to_annotations(detections, annotations, iou_threshold=IOU_MAPPING_THRESHOLD):\n    \"\"\"Map detected bboxes to ground truth annotations using IoU\"\"\"\n    mapped_pairs = []\n    used_detections = set()\n    used_annotations = set()\n    \n    for det_idx, detection in enumerate(detections):\n        best_iou = 0\n        best_ann_idx = -1\n        \n        for ann_idx, annotation in enumerate(annotations):\n            if ann_idx in used_annotations:\n                continue\n                \n            iou = calculate_iou(detection['box'], annotation['bbox'])\n            if iou >= iou_threshold and iou > best_iou:\n                best_iou = iou\n                best_ann_idx = ann_idx\n        \n        if best_ann_idx != -1:\n            mapped_pairs.append({\n                'detection_idx': det_idx,\n                'annotation_idx': best_ann_idx,\n                'detection': detection,\n                'annotation': annotations[best_ann_idx],\n                'iou': best_iou\n            })\n            used_detections.add(det_idx)\n            used_annotations.add(best_ann_idx)\n    \n    # Find unmatched detections and annotations\n    unmatched_detections = [i for i in range(len(detections)) if i not in used_detections]\n    unmatched_annotations = [i for i in range(len(annotations)) if i not in used_annotations]\n    \n    return mapped_pairs, unmatched_detections, unmatched_annotations\n\ndef compute_detection_metrics(mapped_pairs, total_detections, total_annotations):\n    \"\"\"Compute detection performance metrics\"\"\"\n    true_positives = len(mapped_pairs)\n    false_positives = total_detections - true_positives\n    false_negatives = total_annotations - true_positives\n    \n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return {\n        'true_positives': true_positives,\n        'false_positives': false_positives,\n        'false_negatives': false_negatives,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score\n    }\n\ndef display_comprehensive_metrics(all_results):\n    \"\"\"Display comprehensive classification metrics with enhanced analysis\"\"\"\n    if not all_results:\n        print(\"❌ No results to display\")\n        return None, None\n    \n    df = pd.DataFrame(all_results)\n    \n    # Overall accuracy\n    overall_accuracy = accuracy_score(df['true_label'], df['predicted_label'])\n    print(f\"\\n🎯 OVERALL CLASSIFICATION ACCURACY: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)\")\n    \n    # Get unique labels\n    all_labels = sorted(set(df['true_label'].tolist() + df['predicted_label'].tolist()))\n    \n    # Classification report\n    print(f\"\\n📊 DETAILED CLASSIFICATION REPORT:\")\n    print(\"=\"*80)\n    report = classification_report(df['true_label'], df['predicted_label'], \n                                 output_dict=False, zero_division=0)\n    print(report)\n    \n    # Get classification report as dict for further analysis\n    report_dict = classification_report(df['true_label'], df['predicted_label'], \n                                      output_dict=True, zero_division=0)\n    \n    # Per-class metrics with confidence intervals\n    precision, recall, f1, support = precision_recall_fscore_support(\n        df['true_label'], df['predicted_label'], average=None, zero_division=0, labels=all_labels\n    )\n    \n    metrics_df = pd.DataFrame({\n        'Class': all_labels,\n        'Precision': precision,\n        'Recall': recall,\n        'F1-Score': f1,\n        'Support': support\n    })\n    \n    print(f\"\\n📈 ENHANCED PER-CLASS METRICS:\")\n    print(\"=\"*80)\n    print(metrics_df.round(3).to_string(index=False))\n    \n    # Confusion matrix visualization\n    cm = confusion_matrix(df['true_label'], df['predicted_label'], labels=all_labels)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=all_labels, yticklabels=all_labels)\n    plt.title('Confusion Matrix: DeepSeek-VL + BART', fontsize=14, fontweight='bold')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n    \n    return df, metrics_df\n\ndef run_pedestrian_behavior_pipeline():\n    \"\"\"Main pipeline for pedestrian behavior dynamics analysis with DeepSeek-VL + BART\"\"\"\n    print(\"🚀 PEDESTRIAN BEHAVIOR DYNAMICS PIPELINE - GROUNDING DINO + DEEPSEEK-VL + BART\")\n    print(\"=\" * 80)\n    print(f\"🎯 Detection Classes: {', '.join(DETECTION_CLASSES)}\")\n    print(f\"🎯 Confidence Threshold: {CONFIDENCE_THRESHOLD}\")\n    print(f\"🔄 IoU Merge Threshold: {IOU_THRESHOLD}\")\n    print(f\"📐 Containment Threshold: {CONTAINMENT_THRESHOLD}\")\n    print(f\"📊 Min Area Percentage: {MIN_PERSON_AREA_PERCENTAGE}%\")\n    print(f\"🔗 IoU Mapping Threshold: {IOU_MAPPING_THRESHOLD}\")\n    print(\"=\" * 80)\n    \n    # Setup environment\n    setup_environment()\n    \n    # Create output directory\n    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n    \n    # Find image files\n    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n    image_files = []\n    \n    # Check if folders exist\n    if not os.path.exists(IMAGES_FOLDER):\n        print(f\"❌ Images folder not found: {IMAGES_FOLDER}\")\n        return None\n    \n    if not os.path.exists(ANNOTATIONS_FOLDER):\n        print(f\"❌ Annotations folder not found: {ANNOTATIONS_FOLDER}\")\n        return None\n    \n    for file in os.listdir(IMAGES_FOLDER):\n        if any(file.lower().endswith(ext) for ext in image_extensions):\n            image_files.append(file)\n    \n    if not image_files:\n        print(f\"❌ No image files found in {IMAGES_FOLDER}\")\n        return None\n    \n    print(f\"🖼️ Found {len(image_files)} images\")\n    \n    # Analyze dataset statistics\n    dataset_stats = analyze_dataset_statistics(image_files, IMAGES_FOLDER, ANNOTATIONS_FOLDER)\n    \n    # Initialize models\n    detector = GroundingDINODetector()\n    classifier = DeepSeekBARTClassifier()\n    \n    # Pipeline results storage\n    all_results = []\n    detection_metrics = []\n    all_classifications = []\n    \n    print(f\"\\n🔄 Processing {len(image_files)} images...\")\n    start_time = time.time()\n    \n    for i, image_file in enumerate(tqdm(image_files, desc=\"Processing images\")):\n        image_path = os.path.join(IMAGES_FOLDER, image_file)\n        image_id = os.path.splitext(image_file)[0]\n        xml_path = os.path.join(ANNOTATIONS_FOLDER, f\"{image_id}.xml\")\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"📷 IMAGE {i+1}/{len(image_files)}: {image_file}\")\n        \n        if not os.path.exists(xml_path):\n            print(f\"❌ No annotation found for {image_file}\")\n            continue\n        \n        # Load image and annotation\n        image = cv2.imread(image_path)\n        if image is None:\n            print(f\"❌ Could not load image: {image_file}\")\n            continue\n        \n        annotations, image_info = parse_xml_annotation(xml_path)\n        if not annotations:\n            print(f\"❌ No valid annotations found in {xml_path}\")\n            continue\n        \n        print(f\"📋 Ground truth annotations: {len(annotations)}\")\n        for j, ann in enumerate(annotations):\n            print(f\"   GT {j+1}: {ann['label']} at {ann['bbox']}\")\n        \n        # Run Grounding DINO detection\n        detections, pil_image = detector.detect_objects(image)\n        print(f\"🔍 Grounding DINO detections: {len(detections)}\")\n        \n        # Map detections to annotations\n        mapped_pairs, unmatched_detections, unmatched_annotations = map_detections_to_annotations(\n            detections, annotations, IOU_MAPPING_THRESHOLD\n        )\n        \n        print(f\"🔗 Mapping results:\")\n        print(f\"   ✅ Mapped pairs: {len(mapped_pairs)}\")\n        print(f\"   📦 Unmatched detections: {len(unmatched_detections)} (discarded)\")\n        print(f\"   📋 Unmatched annotations: {len(unmatched_annotations)}\")\n        \n        # Compute detection metrics\n        det_metrics = compute_detection_metrics(mapped_pairs, len(detections), len(annotations))\n        det_metrics['image_file'] = image_file\n        detection_metrics.append(det_metrics)\n        \n        print(f\"📊 Detection Metrics - P: {det_metrics['precision']:.3f}, R: {det_metrics['recall']:.3f}, F1: {det_metrics['f1_score']:.3f}\")\n        \n        # Process only mapped pairs for classification\n        print(f\"🧠 Running DeepSeek-VL + BART classification on {len(mapped_pairs)} mapped detections...\")\n        \n        for pair_idx, pair in enumerate(mapped_pairs):\n            detection = pair['detection']\n            annotation = pair['annotation']\n            x1, y1, x2, y2 = detection['box']\n            \n            # Crop detection\n            img_array = np.array(pil_image)\n            crop = img_array[y1:y2, x1:x2]\n            \n            if crop.size > 0:\n                # Classify behavior with DeepSeek-VL + BART\n                pred_label, pred_conf, all_scores = classifier.classify_behavior(crop)\n                all_classifications.append(pred_label)\n                \n                # Store comprehensive results\n                result = {\n                    'image_file': image_file,\n                    'detection_idx': pair['detection_idx'],\n                    'annotation_idx': pair['annotation_idx'],\n                    'true_label': annotation['label'],\n                    'predicted_label': pred_label,\n                    'confidence': pred_conf,\n                    'deepseek_classification': pred_label,\n                    'iou': pair['iou'],\n                    'detection_bbox': detection['box'],\n                    'annotation_bbox': annotation['bbox'],\n                    'detection_confidence': detection['confidence'],\n                    'detection_area': detection['area'],\n                    'area_percentage': detection['area_percentage'],\n                    'correct': pred_label == annotation['label']\n                }\n                all_results.append(result)\n                \n                # Print classification result\n                print(f\"   🎯 Crop {pair_idx+1}:\")\n                print(f\"      🏷️  Classification: {annotation['label']} -> {pred_label} | IoU: {pair['iou']:.3f}\")\n        \n        # Clean GPU memory periodically\n        if i % 5 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    # Save detailed results\n    if all_results:\n        results_df = pd.DataFrame(all_results)\n        results_df.to_csv(os.path.join(OUTPUT_FOLDER, 'pedestrian_behavior_deepseek_results.csv'), index=False)\n        \n        detection_df = pd.DataFrame(detection_metrics)\n        detection_df.to_csv(os.path.join(OUTPUT_FOLDER, 'detection_metrics.csv'), index=False)\n        \n        # Save classifications\n        classifications_df = pd.DataFrame({'classifications': all_classifications})\n        classifications_df.to_csv(os.path.join(OUTPUT_FOLDER, 'deepseek_classifications.csv'), index=False)\n        \n        # Display comprehensive metrics\n        print(f\"\\n🎉 PIPELINE COMPLETE!\")\n        print(f\"💾 Results saved to: {OUTPUT_FOLDER}\")\n        print(f\"📊 Total processed detections: {len(all_results)}\")\n        print(f\"📝 Total DeepSeek classifications: {len(all_classifications)}\")\n        print(f\"⏱️ Total time: {(time.time() - start_time)/60:.1f} minutes\")\n        \n        # Detection Performance Summary\n        if detection_metrics:\n            avg_precision = np.mean([m['precision'] for m in detection_metrics])\n            avg_recall = np.mean([m['recall'] for m in detection_metrics])\n            avg_f1 = np.mean([m['f1_score'] for m in detection_metrics])\n            \n            print(f\"\\n🔍 DETECTION PERFORMANCE SUMMARY:\")\n            print(f\"   📈 Average Precision: {avg_precision:.3f}\")\n            print(f\"   📈 Average Recall: {avg_recall:.3f}\")\n            print(f\"   📈 Average F1-Score: {avg_f1:.3f}\")\n        \n        # Classification Performance with enhanced metrics\n        final_df, metrics_df = display_comprehensive_metrics(all_results)\n        \n        # Display sample classifications\n        print(f\"\\n📝 SAMPLE DEEPSEEK CLASSIFICATIONS:\")\n        print(\"=\"*60)\n        for i, classification in enumerate(all_classifications[:10]):\n            print(f\"   {i+1}. {classification}\")\n        if len(all_classifications) > 10:\n            print(f\"   ... and {len(all_classifications)-10} more classifications\")\n        \n        return final_df, detection_df, metrics_df, dataset_stats\n    \n    else:\n        print(\"❌ No results to display\")\n        return None, None, None, None\n\n# Run the complete pipeline\nprint(\"🚀 Starting Pedestrian Behavior Dynamics Pipeline with DeepSeek-VL + BART...\")\nresults_df, detection_df, class_metrics_df, dataset_stats = run_pedestrian_behavior_pipeline()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T12:14:59.052509Z","iopub.execute_input":"2025-08-05T12:14:59.052863Z","iopub.status.idle":"2025-08-05T12:32:05.264779Z","shell.execute_reply.started":"2025-08-05T12:14:59.052837Z","shell.execute_reply":"2025-08-05T12:32:05.263961Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Pedestrian Behavior Dynamics Pipeline with DeepSeek-VL + BART...\n🚀 PEDESTRIAN BEHAVIOR DYNAMICS PIPELINE - GROUNDING DINO + DEEPSEEK-VL + BART\n================================================================================\n🎯 Detection Classes: person, bike, skateboard, baby stroller, wheelchair\n🎯 Confidence Threshold: 0.5\n🔄 IoU Merge Threshold: 0.1\n📐 Containment Threshold: 0.3\n📊 Min Area Percentage: 0.5%\n🔗 IoU Mapping Threshold: 0.8\n================================================================================\n🚀 Setting up Pedestrian Behavior Dynamics Pipeline...\n📦 Installing torch torchvision transformers>=4.45.0...\n✅ torch torchvision transformers>=4.45.0 installed successfully\n📦 Installing opencv-python-headless...\n✅ opencv-python-headless installed successfully\n📦 Installing tqdm...\n✅ tqdm installed successfully\n📦 Installing matplotlib...\n✅ matplotlib installed successfully\n📦 Installing seaborn...\n✅ seaborn installed successfully\n📦 Installing scikit-learn...\n✅ scikit-learn installed successfully\n📦 Installing pandas...\n✅ pandas installed successfully\n📦 Installing accelerate...\n✅ accelerate installed successfully\n🖼️ Found 109 images\n\n📊 DATASET STATISTICS ANALYSIS\n================================================================================\n🔍 Analyzing 109 images...\n","output_type":"stream"},{"name":"stderr","text":"Analyzing dataset: 100%|██████████| 109/109 [00:02<00:00, 37.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 DATASET OVERVIEW:\n   📷 Total Images: 109\n   📦 Total Annotations: 239\n   🏷️  Unique Labels: 8\n\n🏷️  LABEL DISTRIBUTION:\n   walking: 93 (38.9%)\n   running: 40 (16.7%)\n   biking: 35 (14.6%)\n   standing: 20 (8.4%)\n   scootering: 17 (7.1%)\n   skateboarding: 15 (6.3%)\n   pushing a stroller: 10 (4.2%)\n   person on wheelchair: 9 (3.8%)\n✅ GPU Available: Tesla T4\n🔄 Loading Grounding DINO model...\n✅ Grounding DINO loaded successfully!\n🔄 Loading DeepSeek-VL + BART on cuda...\n✅ DeepSeek-VL loaded successfully!\n🔄 Loading BART classifier...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"✅ BART loaded successfully!\n✅ DeepSeek-VL + BART classifier ready!\n\n🔄 Processing 109 images...\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   0%|          | 0/109 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 1/109: 000043_1_r.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [3, 2, 97, 308]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   1%|          | 1/109 [00:04<07:41,  4.27s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 2/109: 226d22626b6c.jpg\n📋 Ground truth annotations: 2\n   GT 1: scootering at [128, 167, 213, 360]\n   GT 2: biking at [425, 210, 543, 351]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> biking | IoU: 1.000\n   📝 Caption: Using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   2%|▏         | 2/109 [00:11<11:02,  6.19s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.779)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 3/109: 1262dfa03a48.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [3952, 1322, 4851, 4096]\n   GT 2: walking at [1036, 1627, 1374, 2468]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   3%|▎         | 3/109 [00:20<12:35,  7.13s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 4/109: 4bc257223de9.jpg\n📋 Ground truth annotations: 1\n   GT 1: skateboarding at [441, 121, 662, 613]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Skateboarding\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   4%|▎         | 4/109 [00:24<10:23,  5.94s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n\n============================================================\n📷 IMAGE 5/109: 103191381.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [1, 122, 858, 2599]\n   GT 2: running at [999, 243, 1708, 2666]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 0.999\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   5%|▍         | 5/109 [00:31<11:20,  6.55s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 6/109: 696b3130e5c3.jpg\n📋 Ground truth annotations: 1\n   GT 1: skateboarding at [2983, 312, 4054, 2327]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Skateboarding\n   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   6%|▌         | 6/109 [00:36<10:12,  5.94s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 7/109: 046662307299.jpg\n📋 Ground truth annotations: 1\n   GT 1: scootering at [1513, 2466, 1993, 3815]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   6%|▋         | 7/109 [00:41<09:23,  5.53s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\n📷 IMAGE 8/109: 3bee3475326a.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [518, 200, 713, 805]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   7%|▋         | 8/109 [00:45<08:30,  5.06s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 9/109: 1a54fc462484.jpg\n📋 Ground truth annotations: 6\n   GT 1: walking at [209, 29, 286, 290]\n   GT 2: standing at [163, 72, 206, 175]\n   GT 3: walking at [81, 54, 151, 182]\n   GT 4: walking at [41, 45, 97, 193]\n   GT 5: walking at [17, 72, 32, 118]\n   GT 6: walking at [2, 74, 17, 118]\n🔍 Grounding DINO detections: 6\n🔗 Mapping results:\n   ✅ Mapped pairs: 6\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 6 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 5:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   8%|▊         | 9/109 [01:07<17:31, 10.52s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 6:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 10/109: 16069c3052d5.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [2675, 1622, 3370, 3595]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   9%|▉         | 10/109 [01:12<14:22,  8.71s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 11/109: b832489c8abe.jpg\n📋 Ground truth annotations: 3\n   GT 1: biking at [459, 117, 520, 270]\n   GT 2: biking at [541, 64, 613, 288]\n   GT 3: standing at [248, 94, 272, 170]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 1\n📊 Detection Metrics - P: 1.000, R: 0.667, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n   📝 Caption: Using a bicycle\n   🎯 Classification: biking (confidence: 0.779)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  10%|█         | 11/109 [01:20<13:59,  8.57s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 12/109: 73e86e3ce617.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [754, 125, 1553, 2192]\n   GT 2: walking at [2305, 341, 2625, 1148]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  11%|█         | 12/109 [01:28<13:34,  8.40s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 13/109: 203e0efd886a.jpg\n📋 Ground truth annotations: 2\n   GT 1: biking at [4182, 1100, 4427, 1837]\n   GT 2: biking at [2140, 821, 3064, 3069]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  12%|█▏        | 13/109 [01:37<13:23,  8.37s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 14/109: 0df21f941ce5.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [203, 510, 1331, 1867]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  13%|█▎        | 14/109 [01:41<11:15,  7.11s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 15/109: 1053d4b43d25.jpg\n📋 Ground truth annotations: 5\n   GT 1: walking at [121, 120, 440, 1053]\n   GT 2: walking at [553, 155, 976, 1041]\n   GT 3: walking at [1284, 1, 1658, 1059]\n   GT 4: scootering at [1945, 15, 2235, 1068]\n   GT 5: scootering at [1008, 1, 1347, 1219]\n🔍 Grounding DINO detections: 7\n🔗 Mapping results:\n   ✅ Mapped pairs: 5\n   📦 Unmatched detections: 2 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.714, R: 1.000, F1: 0.833\n🧠 Running DeepSeek-VL + BART classification on 5 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 0.999\n   📝 Caption: Using a scooter\n   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 4:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n   📝 Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  14%|█▍        | 15/109 [02:00<16:51, 10.76s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 5:\n      🏷️  Classification: scootering -> scootering | IoU: 0.999\n\n============================================================\n📷 IMAGE 16/109: 8f8fd0be069e.jpg\n📋 Ground truth annotations: 2\n   GT 1: standing at [194, 98, 272, 398]\n   GT 2: standing at [260, 138, 317, 349]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  15%|█▍        | 16/109 [02:08<15:34, 10.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 17/109: b900827db997.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [456, 90, 590, 473]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  16%|█▌        | 17/109 [02:13<12:42,  8.29s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.953)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 18/109: 0e730b239ccc.jpg\n📋 Ground truth annotations: 2\n   GT 1: running at [354, 102, 529, 505]\n   GT 2: running at [549, 147, 731, 509]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  17%|█▋        | 18/109 [02:20<12:19,  8.13s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 19/109: 000033_2_x.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [6, 12, 144, 151]\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  17%|█▋        | 19/109 [02:21<08:45,  5.84s/it]","output_type":"stream"},{"name":"stdout","text":"🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 0\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 1\n📊 Detection Metrics - P: 0.000, R: 0.000, F1: 0.000\n🧠 Running DeepSeek-VL + BART classification on 0 mapped detections...\n\n============================================================\n📷 IMAGE 20/109: 73e8d0cb84e9.jpg\n📋 Ground truth annotations: 3\n   GT 1: biking at [236, 200, 269, 298]\n   GT 2: biking at [540, 132, 734, 534]\n   GT 3: biking at [280, 178, 357, 339]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.750, R: 1.000, F1: 0.857\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  18%|█▊        | 20/109 [02:33<11:22,  7.67s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 3:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 21/109: 397e6ca337e8.jpg\n📋 Ground truth annotations: 1\n   GT 1: standing at [1053, 791, 1562, 2488]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  19%|█▉        | 21/109 [02:38<09:58,  6.80s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 22/109: 0d483a79a08a.jpg\n📋 Ground truth annotations: 3\n   GT 1: walking at [1053, 2236, 1715, 4098]\n   GT 2: walking at [2186, 2053, 2821, 4091]\n   GT 3: walking at [1826, 2504, 2263, 4029]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  20%|██        | 22/109 [02:50<12:19,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 23/109: 39c9544615cc.jpg\n📋 Ground truth annotations: 4\n   GT 1: walking at [499, 96, 599, 334]\n   GT 2: standing at [596, 85, 685, 285]\n   GT 3: walking at [333, 71, 403, 276]\n   GT 4: walking at [157, 94, 261, 301]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 4\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 4 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  21%|██        | 23/109 [03:06<15:15, 10.64s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 24/109: 860f5d9ad655.jpg\n📋 Ground truth annotations: 2\n   GT 1: pushing a stroller at [174, 71, 668, 941]\n   GT 2: walking at [688, 138, 739, 264]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: pushing a stroller -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  22%|██▏       | 24/109 [03:14<14:04,  9.93s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 25/109: d17b1e5f2d49.jpg\n📋 Ground truth annotations: 3\n   GT 1: running at [70, 177, 276, 769]\n   GT 2: running at [535, 189, 742, 783]\n   GT 3: running at [311, 219, 479, 766]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  23%|██▎       | 25/109 [03:26<14:42, 10.51s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 3:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 26/109: 43fddef1053c.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [772, 103, 942, 594]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  24%|██▍       | 26/109 [03:31<12:12,  8.82s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 27/109: 2137dc2147c2.jpg\n📋 Ground truth annotations: 2\n   GT 1: skateboarding at [255, 144, 356, 459]\n   GT 2: skateboarding at [102, 178, 221, 345]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 2 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Skateboarding\n   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n   📝 Caption: Skateboarding\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  25%|██▍       | 27/109 [03:40<12:11,  8.92s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 2:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n\n============================================================\n📷 IMAGE 28/109: 000029_1_x.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [36, 0, 85, 144]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  26%|██▌       | 28/109 [03:44<10:11,  7.55s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 29/109: 59ee13e5524e.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [137, 90, 289, 375]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  27%|██▋       | 29/109 [03:48<08:47,  6.59s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 30/109: 397638464.jpg\n📋 Ground truth annotations: 1\n   GT 1: scootering at [493, 178, 726, 750]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  28%|██▊       | 30/109 [03:53<07:47,  5.92s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\n📷 IMAGE 31/109: 000040_3_r.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [2, 0, 82, 143]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  28%|██▊       | 31/109 [03:58<07:18,  5.62s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 32/109: 52dc81b8995f.jpg\n📋 Ground truth annotations: 3\n   GT 1: biking at [210, 424, 329, 745]\n   GT 2: walking at [489, 467, 561, 622]\n   GT 3: standing at [1, 474, 132, 607]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 1\n📊 Detection Metrics - P: 1.000, R: 0.667, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  29%|██▉       | 32/109 [04:06<08:14,  6.42s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 33/109: 314567015.jpg\n📋 Ground truth annotations: 3\n   GT 1: skateboarding at [301, 25, 463, 587]\n   GT 2: walking at [584, 61, 717, 376]\n   GT 3: walking at [715, 71, 893, 394]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Skateboarding\n   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  30%|███       | 33/109 [04:18<10:20,  8.16s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 34/109: 376dd7ef6a69.jpg\n📋 Ground truth annotations: 2\n   GT 1: skateboarding at [2844, 1753, 3252, 3214]\n   GT 2: walking at [1973, 1991, 2408, 3218]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Skateboarding\n   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  31%|███       | 34/109 [04:28<10:37,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 35/109: 5d6136ae2318.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [397, 119, 751, 501]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 2 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.333, R: 1.000, F1: 0.500\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  32%|███▏      | 35/109 [04:32<08:59,  7.28s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 36/109: a59a5a1900bb.jpg\n📋 Ground truth annotations: 2\n   GT 1: running at [194, 133, 399, 698]\n   GT 2: running at [468, 158, 693, 706]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  33%|███▎      | 36/109 [04:41<09:20,  7.67s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 37/109: 1879bdd3bf3b.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [1287, 548, 1512, 993]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  34%|███▍      | 37/109 [04:45<08:07,  6.76s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 38/109: 000081_1_x.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [48, 2, 135, 241]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  35%|███▍      | 38/109 [04:50<07:12,  6.08s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: running -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 39/109: 7334f2e5a13e.jpg\n📋 Ground truth annotations: 2\n   GT 1: skateboarding at [1135, 245, 1961, 2829]\n   GT 2: skateboarding at [694, 681, 1228, 2503]\n🔍 Grounding DINO detections: 5\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 3 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.400, R: 1.000, F1: 0.571\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  36%|███▌      | 39/109 [04:58<08:02,  6.89s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: skateboarding -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 40/109: 5b7bb02451d4.jpg\n📋 Ground truth annotations: 3\n   GT 1: walking at [906, 517, 1471, 2311]\n   GT 2: walking at [1628, 699, 1839, 1226]\n   GT 3: walking at [1838, 670, 2040, 1124]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  37%|███▋      | 40/109 [05:11<09:50,  8.56s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 41/109: 846806160.jpg\n📋 Ground truth annotations: 2\n   GT 1: person on wheelchair at [155, 229, 444, 547]\n   GT 2: person on wheelchair at [557, 1, 849, 548]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: using a wheelchair\n   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 1:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n   📝 Caption: Using a wheelchair\n   🎯 Classification: person on wheelchair (confidence: 0.783)\n   🎯 Crop 2:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  38%|███▊      | 41/109 [05:20<09:52,  8.71s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 42/109: 1df319712ff5.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [461, 520, 1149, 1577]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  39%|███▊      | 42/109 [05:24<08:18,  7.44s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 43/109: 27551a5c9032.jpg\n📋 Ground truth annotations: 5\n   GT 1: biking at [830, 104, 1016, 622]\n   GT 2: biking at [1033, 146, 1183, 646]\n   GT 3: walking at [678, 152, 848, 584]\n   GT 4: biking at [355, 199, 596, 769]\n   GT 5: standing at [79, 59, 295, 613]\n🔍 Grounding DINO detections: 5\n🔗 Mapping results:\n   ✅ Mapped pairs: 5\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 5 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Using a bicycle\n   🎯 Classification: biking (confidence: 0.779)\n   🎯 Crop 4:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  39%|███▉      | 43/109 [05:45<12:30, 11.37s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 5:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 44/109: 000069_1_x.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [29, 1, 76, 150]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  40%|████      | 44/109 [05:49<10:01,  9.25s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 45/109: 561178142eca.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [1928, 1677, 3010, 4322]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  41%|████▏     | 45/109 [05:54<08:24,  7.89s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 46/109: 277377119.jpg\n📋 Ground truth annotations: 7\n   GT 1: skateboarding at [172, 76, 245, 290]\n   GT 2: standing at [446, 112, 472, 206]\n   GT 3: standing at [349, 110, 377, 198]\n   GT 4: skateboarding at [253, 113, 274, 178]\n   GT 5: standing at [392, 111, 437, 197]\n   GT 6: walking at [300, 116, 318, 159]\n   GT 7: standing at [220, 118, 246, 200]\n🔍 Grounding DINO detections: 5\n🔗 Mapping results:\n   ✅ Mapped pairs: 5\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 2\n📊 Detection Metrics - P: 1.000, R: 0.714, F1: 0.833\n🧠 Running DeepSeek-VL + BART classification on 5 mapped detections...\n   📝 Caption: Skateboarding\n   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: skateboarding -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 5:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  42%|████▏     | 46/109 [06:15<12:27, 11.86s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 47/109: 32381712b502.jpg\n📋 Ground truth annotations: 3\n   GT 1: standing at [267, 326, 571, 1394]\n   GT 2: standing at [1941, 292, 2169, 1217]\n   GT 3: biking at [727, 356, 1550, 1826]\n🔍 Grounding DINO detections: 5\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 2 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.600, R: 1.000, F1: 0.750\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  43%|████▎     | 47/109 [06:28<12:30, 12.10s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 48/109: 246266068.jpg\n📋 Ground truth annotations: 2\n   GT 1: biking at [1907, 31, 2531, 1672]\n   GT 2: walking at [573, 2, 1236, 1939]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  44%|████▍     | 48/109 [06:37<11:23, 11.20s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 49/109: 689722882.jpg\n📋 Ground truth annotations: 3\n   GT 1: skateboarding at [255, 3, 341, 268]\n   GT 2: walking at [155, 32, 244, 276]\n   GT 3: walking at [55, 30, 129, 279]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.750, R: 1.000, F1: 0.857\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  45%|████▍     | 49/109 [06:50<11:36, 11.61s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 50/109: 83b49c4cf207.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [170, 20, 605, 421]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  46%|████▌     | 50/109 [06:54<09:21,  9.52s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 51/109: 123159472.jpg\n📋 Ground truth annotations: 1\n   GT 1: person on wheelchair at [175, 19, 292, 259]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a wheelchair\n   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 1:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  47%|████▋     | 51/109 [06:59<07:54,  8.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 52/109: e3373b9d3481.jpg\n📋 Ground truth annotations: 3\n   GT 1: walking at [862, 838, 2126, 4335]\n   GT 2: walking at [278, 2036, 865, 4206]\n   GT 3: pushing a stroller at [2173, 1271, 3231, 4409]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 2 (discarded)\n   📋 Unmatched annotations: 2\n📊 Detection Metrics - P: 0.333, R: 0.333, F1: 0.333\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  48%|████▊     | 52/109 [07:04<06:51,  7.21s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 53/109: 191525525.jpg\n📋 Ground truth annotations: 3\n   GT 1: running at [2603, 535, 3204, 2147]\n   GT 2: running at [1765, 561, 2360, 2086]\n   GT 3: running at [1264, 387, 1754, 2044]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  49%|████▊     | 53/109 [07:17<08:13,  8.82s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.953)\n   🎯 Crop 3:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 54/109: 7fefc62978cd.jpg\n📋 Ground truth annotations: 5\n   GT 1: walking at [407, 185, 430, 244]\n   GT 2: walking at [387, 188, 410, 244]\n   GT 3: walking at [428, 184, 449, 245]\n   GT 4: running at [140, 140, 231, 380]\n   GT 5: standing at [7, 161, 56, 302]\n🔍 Grounding DINO detections: 5\n🔗 Mapping results:\n   ✅ Mapped pairs: 5\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 5 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 4:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  50%|████▉     | 54/109 [07:37<11:21, 12.38s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 5:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 55/109: 6d37c7afea46.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [619, 160, 1067, 881]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  50%|█████     | 55/109 [07:42<09:01, 10.03s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 56/109: 3b58d5073737.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [1179, 1577, 2345, 4770]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  51%|█████▏    | 56/109 [07:47<07:40,  8.68s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 57/109: 2e4a2a8cff84.jpg\n📋 Ground truth annotations: 1\n   GT 1: person on wheelchair at [139, 134, 245, 343]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a wheelchair\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  52%|█████▏    | 57/109 [07:52<06:27,  7.44s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 1:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n\n============================================================\n📷 IMAGE 58/109: 7581b6070f80.jpg\n📋 Ground truth annotations: 3\n   GT 1: skateboarding at [442, 78, 737, 846]\n   GT 2: standing at [268, 102, 472, 637]\n   GT 3: walking at [730, 134, 979, 677]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Skateboarding\n   🎯 Classification: skateboarding (confidence: 0.616)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> skateboarding | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  53%|█████▎    | 58/109 [08:05<07:36,  8.96s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 59/109: 217928635.jpg\n📋 Ground truth annotations: 1\n   GT 1: person on wheelchair at [114, 1, 235, 319]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a wheelchair\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  54%|█████▍    | 59/109 [08:09<06:21,  7.62s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 1:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 0.997\n\n============================================================\n📷 IMAGE 60/109: 9d199f1d4f54.jpg\n📋 Ground truth annotations: 6\n   GT 1: walking at [51, 523, 318, 1249]\n   GT 2: standing at [1697, 521, 1940, 1120]\n   GT 3: scootering at [656, 458, 910, 1202]\n   GT 4: scootering at [1020, 370, 1436, 1312]\n   GT 5: walking at [899, 579, 1046, 994]\n   GT 6: walking at [347, 514, 593, 1017]\n🔍 Grounding DINO detections: 7\n🔗 Mapping results:\n   ✅ Mapped pairs: 6\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.857, R: 1.000, F1: 0.923\n🧠 Running DeepSeek-VL + BART classification on 6 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Standing up on the sidewalk\n   🎯 Classification: standing (confidence: 0.924)\n   🎯 Crop 2:\n      🏷️  Classification: standing -> standing | IoU: 1.000\n   📝 Caption: Using a scooter\n   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 3:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n   📝 Caption: using a scooter\n   🎯 Classification: scootering (confidence: 0.928)\n   🎯 Crop 4:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 5:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  55%|█████▌    | 60/109 [08:34<10:24, 12.74s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 6:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 61/109: 2d2de75e913b.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [501, 199, 635, 440]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Using a bicycle\n   🎯 Classification: biking (confidence: 0.779)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  56%|█████▌    | 61/109 [08:39<08:19, 10.40s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 62/109: 645196724.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [210, 196, 304, 378]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  57%|█████▋    | 62/109 [08:43<06:45,  8.62s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 63/109: 39408bf8a260.jpg\n📋 Ground truth annotations: 3\n   GT 1: walking at [268, 60, 504, 592]\n   GT 2: scootering at [593, 19, 778, 648]\n   GT 3: walking at [68, 118, 295, 588]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a scooter\n   🎯 Classification: scootering (confidence: 0.928)\n   🎯 Crop 2:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  58%|█████▊    | 63/109 [08:56<07:29,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 64/109: 186629468.jpg\n📋 Ground truth annotations: 2\n   GT 1: scootering at [1769, 62, 2813, 1779]\n   GT 2: scootering at [1254, 138, 1738, 1409]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Using a scooter\n   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n   📝 Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  59%|█████▊    | 64/109 [09:04<07:07,  9.50s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 2:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\n📷 IMAGE 65/109: 772677099.jpg\n📋 Ground truth annotations: 3\n   GT 1: running at [436, 62, 639, 544]\n   GT 2: running at [166, 65, 394, 555]\n   GT 3: running at [375, 134, 441, 336]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  60%|█████▉    | 65/109 [09:17<07:35, 10.35s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.953)\n   🎯 Crop 3:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 66/109: 0d0f4099138e.jpg\n📋 Ground truth annotations: 3\n   GT 1: biking at [504, 191, 663, 655]\n   GT 2: pushing a stroller at [421, 271, 527, 557]\n   GT 3: walking at [647, 229, 768, 573]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: using a stroller\n   🎯 Classification: pushing a stroller (confidence: 0.713)\n   🎯 Crop 2:\n      🏷️  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  61%|██████    | 66/109 [09:30<07:59, 11.16s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 67/109: 5c121785961f.jpg\n📋 Ground truth annotations: 2\n   GT 1: pushing a stroller at [365, 336, 668, 647]\n   GT 2: walking at [316, 331, 450, 617]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: pushing a stroller -> walking | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  61%|██████▏   | 67/109 [09:39<07:17, 10.42s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 68/109: 000043_6_x.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [92, 1, 199, 350]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  62%|██████▏   | 68/109 [09:43<05:53,  8.61s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> walking | IoU: 0.997\n\n============================================================\n📷 IMAGE 69/109: 8839d0f5f8e4.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [1228, 925, 1714, 2194]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  63%|██████▎   | 69/109 [09:48<04:57,  7.45s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 70/109: 00ed6d77a262.jpg\n📋 Ground truth annotations: 6\n   GT 1: person on wheelchair at [1592, 1171, 2417, 2710]\n   GT 2: biking at [4216, 981, 4563, 1766]\n   GT 3: walking at [3837, 889, 4210, 1733]\n   GT 4: walking at [3318, 898, 3800, 2004]\n   GT 5: pushing a stroller at [2921, 796, 3335, 1974]\n   GT 6: walking at [1, 853, 483, 2016]\n🔍 Grounding DINO detections: 6\n🔗 Mapping results:\n   ✅ Mapped pairs: 6\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 6 mapped detections...\n   📝 Caption: using a wheelchair\n   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 1:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a baby stroller\n   🎯 Classification: pushing a stroller (confidence: 0.825)\n   🎯 Crop 5:\n      🏷️  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  64%|██████▍   | 70/109 [10:13<08:14, 12.69s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 6:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 71/109: 815398412.jpg\n📋 Ground truth annotations: 2\n   GT 1: standing at [915, 1, 1120, 569]\n   GT 2: person on wheelchair at [207, 6, 824, 778]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: standing -> walking | IoU: 0.998\n   📝 Caption: using a wheelchair\n   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 2:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  65%|██████▌   | 71/109 [10:22<07:20, 11.60s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 72/109: 274925954b9a.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [1925, 463, 3171, 3941]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  66%|██████▌   | 72/109 [10:26<05:52,  9.53s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: running -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 73/109: bb1898bb6407.jpg\n📋 Ground truth annotations: 1\n   GT 1: skateboarding at [326, 78, 502, 635]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  67%|██████▋   | 73/109 [10:31<04:50,  8.06s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> walking | IoU: 0.923\n\n============================================================\n📷 IMAGE 74/109: a2e829af9975.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [828, 196, 1613, 2091]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  68%|██████▊   | 74/109 [10:36<04:05,  7.01s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 75/109: 396602767.jpg\n📋 Ground truth annotations: 3\n   GT 1: running at [961, 623, 1119, 1052]\n   GT 2: running at [712, 626, 858, 992]\n   GT 3: running at [562, 598, 722, 1012]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: running -> walking | IoU: 1.000\n   📝 Caption: running\n   🎯 Classification: running (confidence: 0.953)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  69%|██████▉   | 75/109 [10:48<04:50,  8.55s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: running -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 76/109: 640903271.jpg\n📋 Ground truth annotations: 1\n   GT 1: person on wheelchair at [695, 144, 1198, 785]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a wheelchair\n   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 1:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  70%|██████▉   | 76/109 [10:53<04:07,  7.50s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 77/109: e3334775aaf7.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [713, 146, 1252, 1770]\n   GT 2: walking at [81, 244, 725, 1786]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  71%|███████   | 77/109 [11:01<04:08,  7.76s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 78/109: 0b458fb3cc18.jpg\n📋 Ground truth annotations: 3\n   GT 1: running at [182, 148, 241, 285]\n   GT 2: running at [357, 159, 414, 323]\n   GT 3: running at [234, 150, 333, 306]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: running -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: running -> walking | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  72%|███████▏  | 78/109 [11:13<04:42,  9.11s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 3:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 79/109: 141443957.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [427, 101, 592, 512]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  72%|███████▏  | 79/109 [11:18<03:53,  7.77s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 80/109: 3b6627b1909a.jpg\n📋 Ground truth annotations: 3\n   GT 1: walking at [932, 90, 1495, 1528]\n   GT 2: walking at [1428, 567, 1880, 1497]\n   GT 3: walking at [1797, 563, 2267, 1475]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 3\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 3 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  73%|███████▎  | 80/109 [11:31<04:27,  9.21s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 81/109: 19b3e4f09d46.jpg\n📋 Ground truth annotations: 4\n   GT 1: running at [1069, 58, 1384, 776]\n   GT 2: running at [188, 87, 634, 1396]\n   GT 3: walking at [1028, 32, 1172, 398]\n   GT 4: scootering at [786, 160, 951, 698]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 4\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 4 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: scootering -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  74%|███████▍  | 81/109 [11:47<05:21, 11.50s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 82/109: 25bfa28305d9.jpg\n📋 Ground truth annotations: 6\n   GT 1: pushing a stroller at [425, 89, 730, 584]\n   GT 2: walking at [738, 75, 944, 570]\n   GT 3: walking at [1079, 67, 1235, 511]\n   GT 4: walking at [1, 40, 253, 719]\n   GT 5: walking at [379, 90, 478, 326]\n   GT 6: walking at [910, 252, 1063, 570]\n🔍 Grounding DINO detections: 6\n🔗 Mapping results:\n   ✅ Mapped pairs: 6\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 6 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: pushing a stroller -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 0.996\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 5:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  75%|███████▌  | 82/109 [12:12<06:54, 15.36s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 6:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 83/109: 4caf932ae018.jpg\n📋 Ground truth annotations: 7\n   GT 1: biking at [2862, 264, 3359, 1553]\n   GT 2: walking at [2626, 251, 2808, 797]\n   GT 3: biking at [81, 130, 1026, 1766]\n   GT 4: walking at [1461, 184, 1667, 776]\n   GT 5: walking at [1020, 196, 1246, 661]\n   GT 6: walking at [1732, 192, 2319, 1584]\n   GT 7: standing at [2357, 202, 2502, 675]\n🔍 Grounding DINO detections: 7\n🔗 Mapping results:\n   ✅ Mapped pairs: 7\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 7 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 3:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 5:\n      🏷️  Classification: walking -> biking | IoU: 1.000\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 6:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  76%|███████▌  | 83/109 [12:41<08:24, 19.39s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 7:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 84/109: 1b413332a7fb.jpg\n📋 Ground truth annotations: 2\n   GT 1: skateboarding at [265, 360, 630, 1462]\n   GT 2: skateboarding at [232, 416, 343, 686]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: skateboarding -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  77%|███████▋  | 84/109 [12:49<06:42, 16.10s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: skateboarding -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 85/109: 9a17c119d342.jpg\n📋 Ground truth annotations: 5\n   GT 1: walking at [1048, 539, 1195, 947]\n   GT 2: walking at [782, 511, 923, 1020]\n   GT 3: walking at [1737, 456, 2016, 1082]\n   GT 4: biking at [34, 36, 789, 2249]\n   GT 5: walking at [1516, 490, 1736, 1084]\n🔍 Grounding DINO detections: 5\n🔗 Mapping results:\n   ✅ Mapped pairs: 5\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 5 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: biking -> walking | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  78%|███████▊  | 85/109 [13:09<06:56, 17.34s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 5:\n      🏷️  Classification: walking -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 86/109: 16630a9f1da8.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [164, 95, 253, 240]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  79%|███████▉  | 86/109 [13:14<05:13, 13.63s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 87/109: 288845502.jpg\n📋 Ground truth annotations: 1\n   GT 1: scootering at [107, 70, 393, 465]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  80%|███████▉  | 87/109 [13:19<03:59, 10.90s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\n📷 IMAGE 88/109: 7ed68dbcbb77.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [865, 490, 1853, 1990]\n   GT 2: walking at [2661, 347, 3445, 1809]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  81%|████████  | 88/109 [13:27<03:34, 10.20s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 89/109: 000043_5_r.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [1, 1, 78, 186]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  82%|████████▏ | 89/109 [13:32<02:49,  8.48s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 90/109: 5bedd0713b73.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [1103, 958, 1961, 2382]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  83%|████████▎ | 90/109 [13:36<02:18,  7.30s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 91/109: 1f9999b8ea6c.jpg\n📋 Ground truth annotations: 2\n   GT 1: running at [727, 251, 962, 967]\n   GT 2: running at [440, 232, 681, 935]\n🔍 Grounding DINO detections: 3\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 1 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  83%|████████▎ | 91/109 [13:45<02:19,  7.76s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 92/109: 05186f7b853c.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [547, 157, 698, 656]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  84%|████████▍ | 92/109 [13:50<01:55,  6.78s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 93/109: 430b80c19b62.jpg\n📋 Ground truth annotations: 2\n   GT 1: scootering at [306, 108, 381, 293]\n   GT 2: scootering at [205, 130, 306, 328]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> walking | IoU: 1.000\n   📝 Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  85%|████████▌ | 93/109 [13:58<01:56,  7.30s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 2:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\n📷 IMAGE 94/109: 522b1c9ce018.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [2748, 484, 3462, 2733]\n   GT 2: walking at [1369, 249, 2331, 2766]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  86%|████████▌ | 94/109 [14:07<01:56,  7.78s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 95/109: 123d2b2b2f2e.jpg\n📋 Ground truth annotations: 4\n   GT 1: walking at [500, 1240, 2171, 4797]\n   GT 2: pushing a stroller at [1519, 1543, 1978, 2625]\n   GT 3: standing at [66, 1633, 393, 2656]\n   GT 4: walking at [337, 1596, 577, 2500]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 4\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 4 mapped detections...\n   📝 Caption: pushing a baby stroller (or baby chair)\n   🎯 Classification: pushing a stroller (confidence: 0.958)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> pushing a stroller | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: pushing a stroller -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: standing -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  87%|████████▋ | 95/109 [14:25<02:31, 10.85s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 96/109: 584562230.jpg\n📋 Ground truth annotations: 1\n   GT 1: scootering at [1300, 45, 2578, 2157]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Using a scooter\n   🎯 Classification: scootering (confidence: 0.923)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  88%|████████▊ | 96/109 [14:30<01:58,  9.15s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 97/109: 20fc87fc5a4d.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [381, 291, 1323, 1718]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  89%|████████▉ | 97/109 [14:35<01:33,  7.78s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 98/109: 5d05cb649f05.jpg\n📋 Ground truth annotations: 2\n   GT 1: walking at [2735, 55, 3222, 1483]\n   GT 2: walking at [410, 2, 1358, 2748]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  90%|████████▉ | 98/109 [14:43<01:28,  8.00s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 99/109: 13a3448686f6.jpg\n📋 Ground truth annotations: 2\n   GT 1: running at [1521, 1, 2173, 1846]\n   GT 2: running at [777, 39, 1300, 1835]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: running -> walking | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  91%|█████████ | 99/109 [14:52<01:21,  8.15s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 2:\n      🏷️  Classification: running -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 100/109: 600247964.jpg\n📋 Ground truth annotations: 1\n   GT 1: biking at [228, 110, 354, 440]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  92%|█████████▏| 100/109 [14:56<01:03,  7.03s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 1:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n\n============================================================\n📷 IMAGE 101/109: 000109_1_x.jpg\n📋 Ground truth annotations: 1\n   GT 1: pushing a stroller at [1, 2, 116, 157]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: pushing a baby stroller\n   🎯 Classification: pushing a stroller (confidence: 0.970)\n   🎯 Crop 1:\n      🏷️  Classification: pushing a stroller -> pushing a stroller | IoU: 0.991\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  93%|█████████▎| 101/109 [15:02<00:52,  6.51s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 102/109: 000033_1_r.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [2, 1, 43, 132]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  94%|█████████▎| 102/109 [15:06<00:40,  5.84s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 103/109: 1ff401446354.jpg\n📋 Ground truth annotations: 1\n   GT 1: scootering at [182, 100, 336, 349]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  94%|█████████▍| 103/109 [15:10<00:32,  5.45s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: scootering (confidence: 0.928)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\n📷 IMAGE 104/109: 168c57aa4497.jpg\n📋 Ground truth annotations: 4\n   GT 1: walking at [1543, 594, 1793, 1330]\n   GT 2: biking at [1812, 734, 1940, 1114]\n   GT 3: biking at [2027, 651, 2248, 1212]\n   GT 4: person on wheelchair at [1116, 504, 1460, 1335]\n🔍 Grounding DINO detections: 4\n🔗 Mapping results:\n   ✅ Mapped pairs: 4\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 4 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 3:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: using a wheelchair\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  95%|█████████▌| 104/109 [15:27<00:44,  8.89s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: person on wheelchair (confidence: 0.771)\n   🎯 Crop 4:\n      🏷️  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n\n============================================================\n📷 IMAGE 105/109: 432651820.jpg\n📋 Ground truth annotations: 6\n   GT 1: walking at [574, 135, 624, 268]\n   GT 2: biking at [411, 137, 526, 437]\n   GT 3: walking at [742, 146, 794, 267]\n   GT 4: walking at [697, 149, 735, 266]\n   GT 5: pushing a stroller at [884, 144, 989, 383]\n   GT 6: walking at [627, 144, 669, 266]\n🔍 Grounding DINO detections: 6\n🔗 Mapping results:\n   ✅ Mapped pairs: 6\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 6 mapped detections...\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a bicycle\n   🎯 Classification: biking (confidence: 0.679)\n   🎯 Crop 2:\n      🏷️  Classification: biking -> biking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 3:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a baby stroller\n   🎯 Classification: pushing a stroller (confidence: 0.825)\n   🎯 Crop 5:\n      🏷️  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  96%|█████████▋| 105/109 [15:52<00:54, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 6:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 106/109: 24bad69b2781.jpg\n📋 Ground truth annotations: 1\n   GT 1: running at [817, 662, 2643, 2867]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Running\n   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 1:\n      🏷️  Classification: running -> running | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  97%|█████████▋| 106/109 [15:57<00:33, 11.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\n📷 IMAGE 107/109: 301480833.jpg\n📋 Ground truth annotations: 2\n   GT 1: scootering at [212, 86, 328, 378]\n   GT 2: walking at [70, 211, 115, 305]\n🔍 Grounding DINO detections: 2\n🔗 Mapping results:\n   ✅ Mapped pairs: 2\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 2 mapped detections...\n   📝 Caption: using a scooter\n   🎯 Classification: scootering (confidence: 0.928)\n   🎯 Crop 1:\n      🏷️  Classification: scootering -> scootering | IoU: 1.000\n   📝 Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  98%|█████████▊| 107/109 [16:05<00:20, 10.25s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n============================================================\n📷 IMAGE 108/109: 1e4b66a943e6.jpg\n📋 Ground truth annotations: 4\n   GT 1: walking at [3790, 209, 4823, 2315]\n   GT 2: walking at [1, 82, 1535, 3538]\n   GT 3: pushing a stroller at [1145, 54, 2998, 5440]\n   GT 4: walking at [3286, 187, 3993, 1841]\n🔍 Grounding DINO detections: 6\n🔗 Mapping results:\n   ✅ Mapped pairs: 4\n   📦 Unmatched detections: 2 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\n🧠 Running DeepSeek-VL + BART classification on 4 mapped detections...\n   📝 Caption: Walking slowly\n   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: walking slowly\n   🎯 Classification: walking (confidence: 0.964)\n   🎯 Crop 2:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n   📝 Caption: using a baby stroller\n   🎯 Classification: pushing a stroller (confidence: 0.825)\n   🎯 Crop 3:\n      🏷️  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   📝 Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  99%|█████████▉| 108/109 [16:23<00:12, 12.52s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: running (confidence: 0.971)\n   🎯 Crop 4:\n      🏷️  Classification: walking -> running | IoU: 1.000\n\n============================================================\n📷 IMAGE 109/109: 9b7357b415ef.jpg\n📋 Ground truth annotations: 1\n   GT 1: walking at [494, 289, 747, 738]\n🔍 Grounding DINO detections: 1\n🔗 Mapping results:\n   ✅ Mapped pairs: 1\n   📦 Unmatched detections: 0 (discarded)\n   📋 Unmatched annotations: 0\n📊 Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\n🧠 Running DeepSeek-VL + BART classification on 1 mapped detections...\n   📝 Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 109/109 [16:28<00:00,  9.07s/it]","output_type":"stream"},{"name":"stdout","text":"   🎯 Classification: walking (confidence: 0.965)\n   🎯 Crop 1:\n      🏷️  Classification: walking -> walking | IoU: 1.000\n\n🎉 PIPELINE COMPLETE!\n💾 Results saved to: /kaggle/working/pipeline_results\n📊 Total processed detections: 232\n📝 Total DeepSeek classifications: 232\n⏱️ Total time: 16.5 minutes\n\n🔍 DETECTION PERFORMANCE SUMMARY:\n   📈 Average Precision: 0.924\n   📈 Average Recall: 0.976\n   📈 Average F1-Score: 0.941\n\n🎯 OVERALL CLASSIFICATION ACCURACY: 0.776 (77.6%)\n\n📊 DETAILED CLASSIFICATION REPORT:\n================================================================================\n                      precision    recall  f1-score   support\n\n              biking       0.92      0.71      0.80        34\nperson on wheelchair       1.00      1.00      1.00         9\n  pushing a stroller       0.83      0.56      0.67         9\n             running       0.92      0.82      0.87        40\n          scootering       1.00      0.82      0.90        17\n       skateboarding       1.00      0.53      0.70        15\n            standing       1.00      0.06      0.11        17\n             walking       0.65      0.95      0.77        91\n\n            accuracy                           0.78       232\n           macro avg       0.92      0.68      0.73       232\n        weighted avg       0.83      0.78      0.75       232\n\n\n📈 ENHANCED PER-CLASS METRICS:\n================================================================================\n               Class  Precision  Recall  F1-Score  Support\n              biking      0.923   0.706     0.800       34\nperson on wheelchair      1.000   1.000     1.000        9\n  pushing a stroller      0.833   0.556     0.667        9\n             running      0.917   0.825     0.868       40\n          scootering      1.000   0.824     0.903       17\n       skateboarding      1.000   0.533     0.696       15\n            standing      1.000   0.059     0.111       17\n             walking      0.652   0.945     0.771       91\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n📝 SAMPLE DEEPSEEK CLASSIFICATIONS:\n============================================================\n   1. walking\n   2. biking\n   3. biking\n   4. walking\n   5. walking\n   6. skateboarding\n   7. walking\n   8. running\n   9. skateboarding\n   10. scootering\n   ... and 222 more classifications\n","output_type":"stream"}],"execution_count":5}]}