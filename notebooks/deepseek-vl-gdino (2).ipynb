{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12680189,"sourceType":"datasetVersion","datasetId":8004235}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/deepseek-ai/DeepSeek-VL.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T11:17:58.394255Z","iopub.execute_input":"2025-08-05T11:17:58.394538Z","iopub.status.idle":"2025-08-05T11:19:20.788667Z","shell.execute_reply.started":"2025-08-05T11:17:58.394518Z","shell.execute_reply":"2025-08-05T11:19:20.787749Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/deepseek-ai/DeepSeek-VL.git\n  Cloning https://github.com/deepseek-ai/DeepSeek-VL.git to /tmp/pip-req-build-_gtry3d5\n  Running command git clone --filter=blob:none --quiet https://github.com/deepseek-ai/DeepSeek-VL.git /tmp/pip-req-build-_gtry3d5\n  Resolved https://github.com/deepseek-ai/DeepSeek-VL.git to commit 681bffb4519856ad27cc17531aacde31ddf6f1a7\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (4.52.4)\nRequirement already satisfied: timm>=0.9.16 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (1.0.15)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (1.8.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (0.2.0)\nCollecting attrdict (from deepseek_vl==1.0.0)\n  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (0.8.1)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.1->deepseek_vl==1.0.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.1->deepseek_vl==1.0.0) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (25.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (0.21.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->deepseek_vl==1.0.0) (7.0.0)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from attrdict->deepseek_vl==1.0.0) (1.17.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.9.16->deepseek_vl==1.0.0) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->deepseek_vl==1.0.0) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2025.6.15)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=0.9.16->deepseek_vl==1.0.0) (11.2.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.38.2->deepseek_vl==1.0.0) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: deepseek_vl\n  Building wheel for deepseek_vl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for deepseek_vl: filename=deepseek_vl-1.0.0-py3-none-any.whl size=58999 sha256=666436d7248e4df9ddccfb8772e1d3f86d9f5fddb45843a4a1c2d46ac5a839c0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mtofe3ae/wheels/f6/29/b1/e1b943f55abbed361464f2d1635fa8ee8ada935755f5e315e6\nSuccessfully built deepseek_vl\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, attrdict, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepseek_vl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed attrdict-2.0.1 deepseek_vl-1.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image, ImageEnhance\nfrom transformers import (\n    AutoProcessor, \n    AutoModelForZeroShotObjectDetection,\n    AutoModelForCausalLM, \n    pipeline\n)\nimport json\nimport xml.etree.ElementTree as ET\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    accuracy_score,\n    precision_recall_fscore_support,\n    precision_recall_curve,\n    roc_curve,\n    auc\n)\nimport seaborn as sns\nfrom collections import defaultdict, Counter\nimport warnings\nimport gc\nimport time\nwarnings.filterwarnings('ignore')\n\n# DeepSeek-VL imports\nfrom deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\nfrom deepseek_vl.utils.io import load_pil_images\n\n# Configuration\nDATASET_FOLDER = '/kaggle/input/pedestrian-behavior-on-sidewalk'\nIMAGES_FOLDER = '/kaggle/input/pedestrian-behavior-on-sidewalk' \nANNOTATIONS_FOLDER = '/kaggle/input/pedestrian-behavior-on-sidewalk'  \nOUTPUT_FOLDER = '/kaggle/working/pipeline_results'\nCONFIDENCE_THRESHOLD = 0.5\nIOU_THRESHOLD = 0.1\nCONTAINMENT_THRESHOLD = 0.3\nMIN_PERSON_AREA_PERCENTAGE = 0.5\nIOU_MAPPING_THRESHOLD = 0.8\n\n# Detection classes\nDETECTION_CLASSES = [\"person\", \"bike\", \"skateboard\", \"baby stroller\", \"wheelchair\"]\n\ndef setup_environment():\n    \"\"\"Install required packages\"\"\"\n    print(\"üöÄ Setting up Pedestrian Behavior Dynamics Pipeline...\")\n    packages = [\n        \"torch torchvision transformers>=4.45.0\", \n        \"opencv-python-headless\", \n        \"tqdm\", \n        \"matplotlib\", \n        \"seaborn\",\n        \"scikit-learn\",\n        \"pandas\",\n        \"accelerate\"\n    ]\n    \n    for package in packages:\n        print(f\"üì¶ Installing {package}...\")\n        result = subprocess.run(f\"pip install {package}\", shell=True, capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"‚úÖ {package} installed successfully\")\n\ndef calculate_iou(box1, box2):\n    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n    \n    # Calculate intersection\n    x1_i = max(x1_1, x1_2)\n    y1_i = max(y1_1, y1_2)\n    x2_i = min(x2_1, x2_2)\n    y2_i = min(y2_1, y2_2)\n    \n    if x2_i <= x1_i or y2_i <= y1_i:\n        return 0.0\n    \n    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n    union = area1 + area2 - intersection\n    \n    return intersection / union if union > 0 else 0.0\n\ndef calculate_containment(box1, box2):\n    \"\"\"Calculate what percentage of box1 is contained within box2\"\"\"\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n    \n    # Calculate intersection\n    x1_i = max(x1_1, x1_2)\n    y1_i = max(y1_1, y1_2)\n    x2_i = min(x2_1, x2_2)\n    y2_i = min(y2_1, y2_2)\n    \n    if x2_i <= x1_i or y2_i <= y1_i:\n        return 0.0\n    \n    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    \n    return intersection / area1 if area1 > 0 else 0.0\n\ndef should_merge_person_nonperson(det1, det2, iou_threshold=0.1, containment_threshold=0.7):\n    \"\"\"Modified merging logic: Only merge person with non-person labels\"\"\"\n    box1, box2 = det1['box'], det2['box']\n    \n    # Calculate IoU\n    iou = calculate_iou(box1, box2)\n    if iou > iou_threshold:\n        return True, \"iou\"\n    \n    # Calculate areas\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    \n    # Check containment for smaller objects (like skateboards in person boxes)\n    if area1 < area2:  # box1 is smaller\n        containment = calculate_containment(box1, box2)\n        if containment > containment_threshold:\n            return True, f\"containment_{containment:.2f}\"\n    else:  # box2 is smaller\n        containment = calculate_containment(box2, box1)\n        if containment > containment_threshold:\n            return True, f\"containment_{containment:.2f}\"\n    \n    return False, \"no_merge\"\n\ndef merge_person_nonperson_boxes(detections, iou_threshold=0.1, containment_threshold=0.7):\n    \"\"\"Merge overlapping bounding boxes but ONLY merge person with non-person\"\"\"\n    if len(detections) <= 1:\n        return detections\n    \n    merged = []\n    used = set()\n    \n    for i, det1 in enumerate(detections):\n        if i in used:\n            continue\n            \n        overlapping = [det1]\n        used.add(i)\n        merge_reasons = []\n        \n        for j, det2 in enumerate(detections[i+1:], i+1):\n            if j in used:\n                continue\n                \n            should_merge, reason = should_merge_person_nonperson(det1, det2, iou_threshold, containment_threshold)\n            \n            if should_merge:\n                overlapping.append(det2)\n                merge_reasons.append(f\"box_{j}_{reason}\")\n                used.add(j)\n        \n        if len(overlapping) == 1:\n            merged.append(overlapping[0])\n        else:\n            # Merge overlapping detections - take the one with highest confidence\n            best_det = max(overlapping, key=lambda x: x['confidence'])\n            \n            # Expand bounding box to encompass all overlapping boxes\n            all_boxes = [det['box'] for det in overlapping]\n            x1_min = min(box[0] for box in all_boxes)\n            y1_min = min(box[1] for box in all_boxes)\n            x2_max = max(box[2] for box in all_boxes)\n            y2_max = max(box[3] for box in all_boxes)\n            \n            merged_det = {\n                'box': [x1_min, y1_min, x2_max, y2_max],\n                'confidence': best_det['confidence'],\n                'merged_count': len(overlapping),\n                'merge_reasons': merge_reasons\n            }\n            merged.append(merged_det)\n    \n    return merged\n\ndef parse_xml_annotation(xml_path):\n    \"\"\"Parse XML annotation file to extract ground truth bounding boxes and labels\"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        \n        annotations = []\n        image_info = {}\n        \n        # Get image dimensions\n        size_elem = root.find('size')\n        if size_elem is not None:\n            try:\n                image_info['width'] = int(size_elem.find('width').text)\n                image_info['height'] = int(size_elem.find('height').text)\n                image_info['depth'] = int(size_elem.find('depth').text)\n            except:\n                pass\n        \n        for obj in root.findall('object'):\n            name_elem = obj.find('name')\n            if name_elem is None or not name_elem.text:\n                continue\n                \n            name = name_elem.text.strip()\n            if not name:\n                continue\n                \n            bbox = obj.find('bndbox')\n            if bbox is None:\n                continue\n                \n            try:\n                x1 = int(bbox.find('xmin').text)\n                y1 = int(bbox.find('ymin').text)\n                x2 = int(bbox.find('xmax').text)\n                y2 = int(bbox.find('ymax').text)\n                \n                annotations.append({\n                    'label': name,\n                    'bbox': [x1, y1, x2, y2],\n                    'area': (x2 - x1) * (y2 - y1)\n                })\n            except (ValueError, AttributeError):\n                continue\n        \n        return annotations, image_info\n    except Exception as e:\n        print(f\"‚ùå Error parsing {xml_path}: {e}\")\n        return [], {}\n\ndef analyze_dataset_statistics(image_files, images_folder, annotations_folder):\n    \"\"\"Analyze and display comprehensive dataset statistics\"\"\"\n    print(f\"\\nüìä DATASET STATISTICS ANALYSIS\")\n    print(\"=\" * 80)\n    \n    dataset_stats = {\n        'images': [],\n        'annotations': [],\n        'labels': [],\n        'bbox_areas': [],\n        'bbox_aspect_ratios': [],\n        'image_sizes': [],\n        'objects_per_image': []\n    }\n    \n    print(f\"üîç Analyzing {len(image_files)} images...\")\n    \n    for image_file in tqdm(image_files, desc=\"Analyzing dataset\"):\n        image_path = os.path.join(images_folder, image_file)\n        image_id = os.path.splitext(image_file)[0]\n        xml_path = os.path.join(annotations_folder, f\"{image_id}.xml\")\n        \n        # Load image to get actual dimensions\n        image = cv2.imread(image_path)\n        if image is not None:\n            h, w, c = image.shape\n            dataset_stats['image_sizes'].append((w, h))\n            dataset_stats['images'].append(image_file)\n        \n        # Parse annotations\n        if os.path.exists(xml_path):\n            annotations, image_info = parse_xml_annotation(xml_path)\n            dataset_stats['objects_per_image'].append(len(annotations))\n            \n            for ann in annotations:\n                dataset_stats['annotations'].append(ann)\n                dataset_stats['labels'].append(ann['label'])\n                \n                # Calculate bbox statistics\n                x1, y1, x2, y2 = ann['bbox']\n                width = x2 - x1\n                height = y2 - y1\n                area = width * height\n                aspect_ratio = width / height if height > 0 else 0\n                \n                dataset_stats['bbox_areas'].append(area)\n                dataset_stats['bbox_aspect_ratios'].append(aspect_ratio)\n    \n    # Display comprehensive statistics\n    print(f\"\\nüìà DATASET OVERVIEW:\")\n    print(f\"   üì∑ Total Images: {len(dataset_stats['images'])}\")\n    print(f\"   üì¶ Total Annotations: {len(dataset_stats['annotations'])}\")\n    print(f\"   üè∑Ô∏è  Unique Labels: {len(set(dataset_stats['labels']))}\")\n    \n    # Label distribution\n    label_counts = Counter(dataset_stats['labels'])\n    print(f\"\\nüè∑Ô∏è  LABEL DISTRIBUTION:\")\n    for label, count in label_counts.most_common():\n        percentage = (count / len(dataset_stats['labels'])) * 100\n        print(f\"   {label}: {count} ({percentage:.1f}%)\")\n    \n    return dataset_stats\n\nclass GroundingDINODetector:\n    \"\"\"Grounding DINO detector for pedestrian behavior detection\"\"\"\n    \n    def __init__(self):\n        # Setup device\n        if torch.cuda.is_available():\n            self.device = \"cuda\"\n            print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n            torch.cuda.empty_cache()\n        else:\n            self.device = \"cpu\"\n            print(\"‚ö†Ô∏è Using CPU\")\n        \n        # Load Grounding DINO\n        print(\"üîÑ Loading Grounding DINO model...\")\n        model_id = \"IDEA-Research/grounding-dino-tiny\"\n        self.processor = AutoProcessor.from_pretrained(model_id)\n        self.model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n        self.model = self.model.to(self.device)\n        self.model.eval()\n        print(\"‚úÖ Grounding DINO loaded successfully!\")\n    \n    def detect_objects(self, image, classes=DETECTION_CLASSES, confidence_threshold=CONFIDENCE_THRESHOLD):\n        \"\"\"Detect multiple object classes in image using same logic as original pipeline\"\"\"\n        # Convert to PIL if needed\n        if isinstance(image, np.ndarray):\n            if len(image.shape) == 3 and image.shape[2] == 3:\n                pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n            else:\n                pil_image = Image.fromarray(image)\n        else:\n            pil_image = image\n        \n        width, height = pil_image.size\n        total_area = width * height\n        min_person_area = total_area * (MIN_PERSON_AREA_PERCENTAGE / 100)\n        \n        # Create text prompt for all classes\n        text_prompt = [[class_name for class_name in classes]]\n        \n        # Prepare inputs\n        inputs = self.processor(images=pil_image, text=text_prompt, return_tensors=\"pt\")\n        device_inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v \n                        for k, v in inputs.items()}\n        \n        # Run detection\n        with torch.no_grad():\n            outputs = self.model(**device_inputs)\n        \n        # Process results\n        target_sizes = torch.tensor([[height, width]]).to(self.device)\n        results = self.processor.post_process_grounded_object_detection(\n            outputs,\n            input_ids=device_inputs.get('input_ids'),\n            box_threshold=confidence_threshold,\n            text_threshold=0,\n            target_sizes=target_sizes\n        )[0]\n        \n        # Extract detections\n        raw_detections = []\n        if len(results['boxes']) > 0:\n            boxes = results['boxes'].cpu().numpy()\n            scores = results['scores'].cpu().numpy()\n            \n            for box, score in zip(boxes, scores):\n                x1, y1, x2, y2 = box.astype(int)\n                box_area = (x2 - x1) * (y2 - y1)\n                \n                detection = {\n                    'box': [x1, y1, x2, y2],\n                    'confidence': float(score),\n                    'area': box_area,\n                    'area_percentage': (box_area / total_area) * 100\n                }\n                raw_detections.append(detection)\n        \n        # Merge overlapping detections using modified logic (person with non-person only)\n        merged_detections = merge_person_nonperson_boxes(\n            raw_detections, IOU_THRESHOLD, CONTAINMENT_THRESHOLD\n        )\n        \n        # Filter by area\n        valid_detections = []\n        for det in merged_detections:\n            box_area = det['area'] if 'area' in det else (det['box'][2] - det['box'][0]) * (det['box'][3] - det['box'][1])\n            area_percentage = (box_area / total_area) * 100\n            \n            # Update detection with area info\n            det['area'] = box_area\n            det['area_percentage'] = area_percentage\n            \n            # Only keep detections with sufficient area\n            if area_percentage >= MIN_PERSON_AREA_PERCENTAGE:\n                valid_detections.append(det)\n        \n        return valid_detections, pil_image\n\nclass DeepSeekBARTClassifier:\n    \"\"\"DeepSeek-VL for caption generation + BART for classification\"\"\"\n    \n    def __init__(self):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"üîÑ Loading DeepSeek-VL + BART on {self.device}...\")\n        \n        # Clear GPU memory first\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Load DeepSeek-VL\n        model_path = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n        self.vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n        self.tokenizer = self.vl_chat_processor.tokenizer\n        \n        self.vl_gpt = AutoModelForCausalLM.from_pretrained(\n            model_path, \n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32\n        )\n        \n        if self.device == \"cuda\":\n            self.vl_gpt = self.vl_gpt.cuda().eval()\n        else:\n            self.vl_gpt = self.vl_gpt.eval()\n            \n        print(\"‚úÖ DeepSeek-VL loaded successfully!\")\n        \n        # Force GPU memory cleanup before loading BART\n        if self.device == \"cuda\":\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Load BART classifier\n        print(\"üîÑ Loading BART classifier...\")\n        try:\n            if self.device == \"cuda\":\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\",\n                    torch_dtype=torch.bfloat16\n                )\n            else:\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\",\n                    device=-1\n                )\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Accelerate loading failed: {e}\")\n            if self.device == \"cuda\":\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\"\n                )\n            else:\n                self.bart_classifier = pipeline(\n                    \"zero-shot-classification\",\n                    model=\"facebook/bart-large-mnli\",\n                    device=-1\n                )\n        \n        print(\"‚úÖ BART loaded successfully!\")\n        print(\"‚úÖ DeepSeek-VL + BART classifier ready!\")\n    \n    def generate_caption(self, image_path):\n        \"\"\"Generate caption using DeepSeek-VL\"\"\"\n        try:\n            conversation = [\n                {\n                    \"role\": \"User\",\n                    \"content\": \"<image_placeholder>Classify the activity of the pedestrian on the sidewalk based on posture, movement. Focus on functional cues like body position, motion, use of hands or feet, and any relevant mobility devices (e.g., bike, scooter, stroller, wheelchair). Answer in a concise sentence by picking one class: walking slowly, running fast, jogging, using a bicycle, standing up on the sidewalk (still), skateboarding, running, using some sort of a scooter, pushing a baby stroller (or baby chair), using a wheelchair.\",\n                    \"images\": [image_path]\n                },\n                {\n                    \"role\": \"Assistant\",\n                    \"content\": \"\"\n                }\n            ]\n\n            # Load images and prepare for inputs\n            pil_images = load_pil_images(conversation)\n            prepare_inputs = self.vl_chat_processor(\n                conversations=conversation,\n                images=pil_images,\n                force_batchify=True\n            ).to(self.vl_gpt.device)\n\n            # Run image encoder to get the image embeddings\n            inputs_embeds = self.vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n\n            outputs = self.vl_gpt.language_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=prepare_inputs.attention_mask,\n                pad_token_id=self.tokenizer.eos_token_id,\n                bos_token_id=self.tokenizer.bos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                max_new_tokens=512,\n                early_stopping=False,\n                do_sample=False,\n                num_beams=3,\n            )\n\n            answer = self.tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n            \n            # Extract just the assistant's response\n            conversation_text = prepare_inputs['sft_format'][0]\n            if conversation_text in answer:\n                caption = answer.replace(conversation_text, \"\").strip()\n            else:\n                caption = answer.strip()\n            \n            return caption\n            \n        except Exception as e:\n            print(f\"‚ùå Error generating caption: {e}\")\n            return \"A person in an image.\"\n    \n    def classify_behavior(self, image):\n        \"\"\"Classify behavior using DeepSeek-VL + BART pipeline\"\"\"\n        behavior_classes = [\n            \"walking\", \"running\", \"pushing a stroller\", \n            \"biking\", \"standing\", \"skateboarding\", \"scootering\", \n            \"person on wheelchair\"\n        ]\n        \n        try:\n            # Convert to PIL and save temporarily\n            if isinstance(image, np.ndarray):\n                if len(image.shape) == 3 and image.shape[2] == 3:\n                    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n                else:\n                    pil_image = Image.fromarray(image)\n            else:\n                pil_image = image\n            \n            # Save temp image for DeepSeek-VL\n            temp_path = \"/tmp/temp_crop.jpg\"\n            pil_image.save(temp_path)\n            \n            # Generate caption with DeepSeek-VL\n            caption = self.generate_caption(temp_path)\n            print(f\"   üìù Caption: {caption}\")\n            \n            # Classify with BART\n            result = self.bart_classifier(caption, behavior_classes)\n            predicted_behavior = result['labels'][0]\n            confidence = result['scores'][0]\n            \n            print(f\"   üéØ Classification: {predicted_behavior} (confidence: {confidence:.3f})\")\n            \n            # Clean up temp file\n            if os.path.exists(temp_path):\n                os.remove(temp_path)\n            \n            # Clean up GPU memory\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return predicted_behavior, confidence, result\n            \n        except Exception as e:\n            print(f\"‚ùå Error classifying behavior: {e}\")\n            return \"unknown\", 0.0, {\"labels\": [\"unknown\"], \"scores\": [0.0]}\n\ndef map_detections_to_annotations(detections, annotations, iou_threshold=IOU_MAPPING_THRESHOLD):\n    \"\"\"Map detected bboxes to ground truth annotations using IoU\"\"\"\n    mapped_pairs = []\n    used_detections = set()\n    used_annotations = set()\n    \n    for det_idx, detection in enumerate(detections):\n        best_iou = 0\n        best_ann_idx = -1\n        \n        for ann_idx, annotation in enumerate(annotations):\n            if ann_idx in used_annotations:\n                continue\n                \n            iou = calculate_iou(detection['box'], annotation['bbox'])\n            if iou >= iou_threshold and iou > best_iou:\n                best_iou = iou\n                best_ann_idx = ann_idx\n        \n        if best_ann_idx != -1:\n            mapped_pairs.append({\n                'detection_idx': det_idx,\n                'annotation_idx': best_ann_idx,\n                'detection': detection,\n                'annotation': annotations[best_ann_idx],\n                'iou': best_iou\n            })\n            used_detections.add(det_idx)\n            used_annotations.add(best_ann_idx)\n    \n    # Find unmatched detections and annotations\n    unmatched_detections = [i for i in range(len(detections)) if i not in used_detections]\n    unmatched_annotations = [i for i in range(len(annotations)) if i not in used_annotations]\n    \n    return mapped_pairs, unmatched_detections, unmatched_annotations\n\ndef compute_detection_metrics(mapped_pairs, total_detections, total_annotations):\n    \"\"\"Compute detection performance metrics\"\"\"\n    true_positives = len(mapped_pairs)\n    false_positives = total_detections - true_positives\n    false_negatives = total_annotations - true_positives\n    \n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return {\n        'true_positives': true_positives,\n        'false_positives': false_positives,\n        'false_negatives': false_negatives,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score\n    }\n\ndef display_comprehensive_metrics(all_results):\n    \"\"\"Display comprehensive classification metrics with enhanced analysis\"\"\"\n    if not all_results:\n        print(\"‚ùå No results to display\")\n        return None, None\n    \n    df = pd.DataFrame(all_results)\n    \n    # Overall accuracy\n    overall_accuracy = accuracy_score(df['true_label'], df['predicted_label'])\n    print(f\"\\nüéØ OVERALL CLASSIFICATION ACCURACY: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)\")\n    \n    # Get unique labels\n    all_labels = sorted(set(df['true_label'].tolist() + df['predicted_label'].tolist()))\n    \n    # Classification report\n    print(f\"\\nüìä DETAILED CLASSIFICATION REPORT:\")\n    print(\"=\"*80)\n    report = classification_report(df['true_label'], df['predicted_label'], \n                                 output_dict=False, zero_division=0)\n    print(report)\n    \n    # Get classification report as dict for further analysis\n    report_dict = classification_report(df['true_label'], df['predicted_label'], \n                                      output_dict=True, zero_division=0)\n    \n    # Per-class metrics with confidence intervals\n    precision, recall, f1, support = precision_recall_fscore_support(\n        df['true_label'], df['predicted_label'], average=None, zero_division=0, labels=all_labels\n    )\n    \n    metrics_df = pd.DataFrame({\n        'Class': all_labels,\n        'Precision': precision,\n        'Recall': recall,\n        'F1-Score': f1,\n        'Support': support\n    })\n    \n    print(f\"\\nüìà ENHANCED PER-CLASS METRICS:\")\n    print(\"=\"*80)\n    print(metrics_df.round(3).to_string(index=False))\n    \n    # Confusion matrix visualization\n    cm = confusion_matrix(df['true_label'], df['predicted_label'], labels=all_labels)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=all_labels, yticklabels=all_labels)\n    plt.title('Confusion Matrix: DeepSeek-VL + BART', fontsize=14, fontweight='bold')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n    \n    return df, metrics_df\n\ndef run_pedestrian_behavior_pipeline():\n    \"\"\"Main pipeline for pedestrian behavior dynamics analysis with DeepSeek-VL + BART\"\"\"\n    print(\"üöÄ PEDESTRIAN BEHAVIOR DYNAMICS PIPELINE - GROUNDING DINO + DEEPSEEK-VL + BART\")\n    print(\"=\" * 80)\n    print(f\"üéØ Detection Classes: {', '.join(DETECTION_CLASSES)}\")\n    print(f\"üéØ Confidence Threshold: {CONFIDENCE_THRESHOLD}\")\n    print(f\"üîÑ IoU Merge Threshold: {IOU_THRESHOLD}\")\n    print(f\"üìê Containment Threshold: {CONTAINMENT_THRESHOLD}\")\n    print(f\"üìä Min Area Percentage: {MIN_PERSON_AREA_PERCENTAGE}%\")\n    print(f\"üîó IoU Mapping Threshold: {IOU_MAPPING_THRESHOLD}\")\n    print(\"=\" * 80)\n    \n    # Setup environment\n    setup_environment()\n    \n    # Create output directory\n    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n    \n    # Find image files\n    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n    image_files = []\n    \n    # Check if folders exist\n    if not os.path.exists(IMAGES_FOLDER):\n        print(f\"‚ùå Images folder not found: {IMAGES_FOLDER}\")\n        return None\n    \n    if not os.path.exists(ANNOTATIONS_FOLDER):\n        print(f\"‚ùå Annotations folder not found: {ANNOTATIONS_FOLDER}\")\n        return None\n    \n    for file in os.listdir(IMAGES_FOLDER):\n        if any(file.lower().endswith(ext) for ext in image_extensions):\n            image_files.append(file)\n    \n    if not image_files:\n        print(f\"‚ùå No image files found in {IMAGES_FOLDER}\")\n        return None\n    \n    print(f\"üñºÔ∏è Found {len(image_files)} images\")\n    \n    # Analyze dataset statistics\n    dataset_stats = analyze_dataset_statistics(image_files, IMAGES_FOLDER, ANNOTATIONS_FOLDER)\n    \n    # Initialize models\n    detector = GroundingDINODetector()\n    classifier = DeepSeekBARTClassifier()\n    \n    # Pipeline results storage\n    all_results = []\n    detection_metrics = []\n    all_classifications = []\n    \n    print(f\"\\nüîÑ Processing {len(image_files)} images...\")\n    start_time = time.time()\n    \n    for i, image_file in enumerate(tqdm(image_files, desc=\"Processing images\")):\n        image_path = os.path.join(IMAGES_FOLDER, image_file)\n        image_id = os.path.splitext(image_file)[0]\n        xml_path = os.path.join(ANNOTATIONS_FOLDER, f\"{image_id}.xml\")\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"üì∑ IMAGE {i+1}/{len(image_files)}: {image_file}\")\n        \n        if not os.path.exists(xml_path):\n            print(f\"‚ùå No annotation found for {image_file}\")\n            continue\n        \n        # Load image and annotation\n        image = cv2.imread(image_path)\n        if image is None:\n            print(f\"‚ùå Could not load image: {image_file}\")\n            continue\n        \n        annotations, image_info = parse_xml_annotation(xml_path)\n        if not annotations:\n            print(f\"‚ùå No valid annotations found in {xml_path}\")\n            continue\n        \n        print(f\"üìã Ground truth annotations: {len(annotations)}\")\n        for j, ann in enumerate(annotations):\n            print(f\"   GT {j+1}: {ann['label']} at {ann['bbox']}\")\n        \n        # Run Grounding DINO detection\n        detections, pil_image = detector.detect_objects(image)\n        print(f\"üîç Grounding DINO detections: {len(detections)}\")\n        \n        # Map detections to annotations\n        mapped_pairs, unmatched_detections, unmatched_annotations = map_detections_to_annotations(\n            detections, annotations, IOU_MAPPING_THRESHOLD\n        )\n        \n        print(f\"üîó Mapping results:\")\n        print(f\"   ‚úÖ Mapped pairs: {len(mapped_pairs)}\")\n        print(f\"   üì¶ Unmatched detections: {len(unmatched_detections)} (discarded)\")\n        print(f\"   üìã Unmatched annotations: {len(unmatched_annotations)}\")\n        \n        # Compute detection metrics\n        det_metrics = compute_detection_metrics(mapped_pairs, len(detections), len(annotations))\n        det_metrics['image_file'] = image_file\n        detection_metrics.append(det_metrics)\n        \n        print(f\"üìä Detection Metrics - P: {det_metrics['precision']:.3f}, R: {det_metrics['recall']:.3f}, F1: {det_metrics['f1_score']:.3f}\")\n        \n        # Process only mapped pairs for classification\n        print(f\"üß† Running DeepSeek-VL + BART classification on {len(mapped_pairs)} mapped detections...\")\n        \n        for pair_idx, pair in enumerate(mapped_pairs):\n            detection = pair['detection']\n            annotation = pair['annotation']\n            x1, y1, x2, y2 = detection['box']\n            \n            # Crop detection\n            img_array = np.array(pil_image)\n            crop = img_array[y1:y2, x1:x2]\n            \n            if crop.size > 0:\n                # Classify behavior with DeepSeek-VL + BART\n                pred_label, pred_conf, all_scores = classifier.classify_behavior(crop)\n                all_classifications.append(pred_label)\n                \n                # Store comprehensive results\n                result = {\n                    'image_file': image_file,\n                    'detection_idx': pair['detection_idx'],\n                    'annotation_idx': pair['annotation_idx'],\n                    'true_label': annotation['label'],\n                    'predicted_label': pred_label,\n                    'confidence': pred_conf,\n                    'deepseek_classification': pred_label,\n                    'iou': pair['iou'],\n                    'detection_bbox': detection['box'],\n                    'annotation_bbox': annotation['bbox'],\n                    'detection_confidence': detection['confidence'],\n                    'detection_area': detection['area'],\n                    'area_percentage': detection['area_percentage'],\n                    'correct': pred_label == annotation['label']\n                }\n                all_results.append(result)\n                \n                # Print classification result\n                print(f\"   üéØ Crop {pair_idx+1}:\")\n                print(f\"      üè∑Ô∏è  Classification: {annotation['label']} -> {pred_label} | IoU: {pair['iou']:.3f}\")\n        \n        # Clean GPU memory periodically\n        if i % 5 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    # Save detailed results\n    if all_results:\n        results_df = pd.DataFrame(all_results)\n        results_df.to_csv(os.path.join(OUTPUT_FOLDER, 'pedestrian_behavior_deepseek_results.csv'), index=False)\n        \n        detection_df = pd.DataFrame(detection_metrics)\n        detection_df.to_csv(os.path.join(OUTPUT_FOLDER, 'detection_metrics.csv'), index=False)\n        \n        # Save classifications\n        classifications_df = pd.DataFrame({'classifications': all_classifications})\n        classifications_df.to_csv(os.path.join(OUTPUT_FOLDER, 'deepseek_classifications.csv'), index=False)\n        \n        # Display comprehensive metrics\n        print(f\"\\nüéâ PIPELINE COMPLETE!\")\n        print(f\"üíæ Results saved to: {OUTPUT_FOLDER}\")\n        print(f\"üìä Total processed detections: {len(all_results)}\")\n        print(f\"üìù Total DeepSeek classifications: {len(all_classifications)}\")\n        print(f\"‚è±Ô∏è Total time: {(time.time() - start_time)/60:.1f} minutes\")\n        \n        # Detection Performance Summary\n        if detection_metrics:\n            avg_precision = np.mean([m['precision'] for m in detection_metrics])\n            avg_recall = np.mean([m['recall'] for m in detection_metrics])\n            avg_f1 = np.mean([m['f1_score'] for m in detection_metrics])\n            \n            print(f\"\\nüîç DETECTION PERFORMANCE SUMMARY:\")\n            print(f\"   üìà Average Precision: {avg_precision:.3f}\")\n            print(f\"   üìà Average Recall: {avg_recall:.3f}\")\n            print(f\"   üìà Average F1-Score: {avg_f1:.3f}\")\n        \n        # Classification Performance with enhanced metrics\n        final_df, metrics_df = display_comprehensive_metrics(all_results)\n        \n        # Display sample classifications\n        print(f\"\\nüìù SAMPLE DEEPSEEK CLASSIFICATIONS:\")\n        print(\"=\"*60)\n        for i, classification in enumerate(all_classifications[:10]):\n            print(f\"   {i+1}. {classification}\")\n        if len(all_classifications) > 10:\n            print(f\"   ... and {len(all_classifications)-10} more classifications\")\n        \n        return final_df, detection_df, metrics_df, dataset_stats\n    \n    else:\n        print(\"‚ùå No results to display\")\n        return None, None, None, None\n\n# Run the complete pipeline\nprint(\"üöÄ Starting Pedestrian Behavior Dynamics Pipeline with DeepSeek-VL + BART...\")\nresults_df, detection_df, class_metrics_df, dataset_stats = run_pedestrian_behavior_pipeline()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T12:14:59.052509Z","iopub.execute_input":"2025-08-05T12:14:59.052863Z","iopub.status.idle":"2025-08-05T12:32:05.264779Z","shell.execute_reply.started":"2025-08-05T12:14:59.052837Z","shell.execute_reply":"2025-08-05T12:32:05.263961Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting Pedestrian Behavior Dynamics Pipeline with DeepSeek-VL + BART...\nüöÄ PEDESTRIAN BEHAVIOR DYNAMICS PIPELINE - GROUNDING DINO + DEEPSEEK-VL + BART\n================================================================================\nüéØ Detection Classes: person, bike, skateboard, baby stroller, wheelchair\nüéØ Confidence Threshold: 0.5\nüîÑ IoU Merge Threshold: 0.1\nüìê Containment Threshold: 0.3\nüìä Min Area Percentage: 0.5%\nüîó IoU Mapping Threshold: 0.8\n================================================================================\nüöÄ Setting up Pedestrian Behavior Dynamics Pipeline...\nüì¶ Installing torch torchvision transformers>=4.45.0...\n‚úÖ torch torchvision transformers>=4.45.0 installed successfully\nüì¶ Installing opencv-python-headless...\n‚úÖ opencv-python-headless installed successfully\nüì¶ Installing tqdm...\n‚úÖ tqdm installed successfully\nüì¶ Installing matplotlib...\n‚úÖ matplotlib installed successfully\nüì¶ Installing seaborn...\n‚úÖ seaborn installed successfully\nüì¶ Installing scikit-learn...\n‚úÖ scikit-learn installed successfully\nüì¶ Installing pandas...\n‚úÖ pandas installed successfully\nüì¶ Installing accelerate...\n‚úÖ accelerate installed successfully\nüñºÔ∏è Found 109 images\n\nüìä DATASET STATISTICS ANALYSIS\n================================================================================\nüîç Analyzing 109 images...\n","output_type":"stream"},{"name":"stderr","text":"Analyzing dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:02<00:00, 37.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüìà DATASET OVERVIEW:\n   üì∑ Total Images: 109\n   üì¶ Total Annotations: 239\n   üè∑Ô∏è  Unique Labels: 8\n\nüè∑Ô∏è  LABEL DISTRIBUTION:\n   walking: 93 (38.9%)\n   running: 40 (16.7%)\n   biking: 35 (14.6%)\n   standing: 20 (8.4%)\n   scootering: 17 (7.1%)\n   skateboarding: 15 (6.3%)\n   pushing a stroller: 10 (4.2%)\n   person on wheelchair: 9 (3.8%)\n‚úÖ GPU Available: Tesla T4\nüîÑ Loading Grounding DINO model...\n‚úÖ Grounding DINO loaded successfully!\nüîÑ Loading DeepSeek-VL + BART on cuda...\n‚úÖ DeepSeek-VL loaded successfully!\nüîÑ Loading BART classifier...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ BART loaded successfully!\n‚úÖ DeepSeek-VL + BART classifier ready!\n\nüîÑ Processing 109 images...\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   0%|          | 0/109 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 1/109: 000043_1_r.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [3, 2, 97, 308]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   1%|          | 1/109 [00:04<07:41,  4.27s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 2/109: 226d22626b6c.jpg\nüìã Ground truth annotations: 2\n   GT 1: scootering at [128, 167, 213, 360]\n   GT 2: biking at [425, 210, 543, 351]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> biking | IoU: 1.000\n   üìù Caption: Using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   2%|‚ñè         | 2/109 [00:11<11:02,  6.19s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.779)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 3/109: 1262dfa03a48.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [3952, 1322, 4851, 4096]\n   GT 2: walking at [1036, 1627, 1374, 2468]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   3%|‚ñé         | 3/109 [00:20<12:35,  7.13s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 4/109: 4bc257223de9.jpg\nüìã Ground truth annotations: 1\n   GT 1: skateboarding at [441, 121, 662, 613]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Skateboarding\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   4%|‚ñé         | 4/109 [00:24<10:23,  5.94s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 5/109: 103191381.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [1, 122, 858, 2599]\n   GT 2: running at [999, 243, 1708, 2666]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 0.999\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   5%|‚ñç         | 5/109 [00:31<11:20,  6.55s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 6/109: 696b3130e5c3.jpg\nüìã Ground truth annotations: 1\n   GT 1: skateboarding at [2983, 312, 4054, 2327]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Skateboarding\n   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   6%|‚ñå         | 6/109 [00:36<10:12,  5.94s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 7/109: 046662307299.jpg\nüìã Ground truth annotations: 1\n   GT 1: scootering at [1513, 2466, 1993, 3815]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   6%|‚ñã         | 7/109 [00:41<09:23,  5.53s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 8/109: 3bee3475326a.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [518, 200, 713, 805]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   7%|‚ñã         | 8/109 [00:45<08:30,  5.06s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 9/109: 1a54fc462484.jpg\nüìã Ground truth annotations: 6\n   GT 1: walking at [209, 29, 286, 290]\n   GT 2: standing at [163, 72, 206, 175]\n   GT 3: walking at [81, 54, 151, 182]\n   GT 4: walking at [41, 45, 97, 193]\n   GT 5: walking at [17, 72, 32, 118]\n   GT 6: walking at [2, 74, 17, 118]\nüîç Grounding DINO detections: 6\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 6\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 6 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   8%|‚ñä         | 9/109 [01:07<17:31, 10.52s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 6:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 10/109: 16069c3052d5.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [2675, 1622, 3370, 3595]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   9%|‚ñâ         | 10/109 [01:12<14:22,  8.71s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 11/109: b832489c8abe.jpg\nüìã Ground truth annotations: 3\n   GT 1: biking at [459, 117, 520, 270]\n   GT 2: biking at [541, 64, 613, 288]\n   GT 3: standing at [248, 94, 272, 170]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 1\nüìä Detection Metrics - P: 1.000, R: 0.667, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n   üìù Caption: Using a bicycle\n   üéØ Classification: biking (confidence: 0.779)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  10%|‚ñà         | 11/109 [01:20<13:59,  8.57s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 12/109: 73e86e3ce617.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [754, 125, 1553, 2192]\n   GT 2: walking at [2305, 341, 2625, 1148]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  11%|‚ñà         | 12/109 [01:28<13:34,  8.40s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 13/109: 203e0efd886a.jpg\nüìã Ground truth annotations: 2\n   GT 1: biking at [4182, 1100, 4427, 1837]\n   GT 2: biking at [2140, 821, 3064, 3069]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  12%|‚ñà‚ñè        | 13/109 [01:37<13:23,  8.37s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 14/109: 0df21f941ce5.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [203, 510, 1331, 1867]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  13%|‚ñà‚ñé        | 14/109 [01:41<11:15,  7.11s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 15/109: 1053d4b43d25.jpg\nüìã Ground truth annotations: 5\n   GT 1: walking at [121, 120, 440, 1053]\n   GT 2: walking at [553, 155, 976, 1041]\n   GT 3: walking at [1284, 1, 1658, 1059]\n   GT 4: scootering at [1945, 15, 2235, 1068]\n   GT 5: scootering at [1008, 1, 1347, 1219]\nüîç Grounding DINO detections: 7\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 5\n   üì¶ Unmatched detections: 2 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.714, R: 1.000, F1: 0.833\nüß† Running DeepSeek-VL + BART classification on 5 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 0.999\n   üìù Caption: Using a scooter\n   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n   üìù Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  14%|‚ñà‚ñç        | 15/109 [02:00<16:51, 10.76s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 0.999\n\n============================================================\nüì∑ IMAGE 16/109: 8f8fd0be069e.jpg\nüìã Ground truth annotations: 2\n   GT 1: standing at [194, 98, 272, 398]\n   GT 2: standing at [260, 138, 317, 349]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  15%|‚ñà‚ñç        | 16/109 [02:08<15:34, 10.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 17/109: b900827db997.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [456, 90, 590, 473]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  16%|‚ñà‚ñå        | 17/109 [02:13<12:42,  8.29s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.953)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 18/109: 0e730b239ccc.jpg\nüìã Ground truth annotations: 2\n   GT 1: running at [354, 102, 529, 505]\n   GT 2: running at [549, 147, 731, 509]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  17%|‚ñà‚ñã        | 18/109 [02:20<12:19,  8.13s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 19/109: 000033_2_x.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [6, 12, 144, 151]\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  17%|‚ñà‚ñã        | 19/109 [02:21<08:45,  5.84s/it]","output_type":"stream"},{"name":"stdout","text":"üîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 0\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 1\nüìä Detection Metrics - P: 0.000, R: 0.000, F1: 0.000\nüß† Running DeepSeek-VL + BART classification on 0 mapped detections...\n\n============================================================\nüì∑ IMAGE 20/109: 73e8d0cb84e9.jpg\nüìã Ground truth annotations: 3\n   GT 1: biking at [236, 200, 269, 298]\n   GT 2: biking at [540, 132, 734, 534]\n   GT 3: biking at [280, 178, 357, 339]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.750, R: 1.000, F1: 0.857\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  18%|‚ñà‚ñä        | 20/109 [02:33<11:22,  7.67s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 21/109: 397e6ca337e8.jpg\nüìã Ground truth annotations: 1\n   GT 1: standing at [1053, 791, 1562, 2488]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  19%|‚ñà‚ñâ        | 21/109 [02:38<09:58,  6.80s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 22/109: 0d483a79a08a.jpg\nüìã Ground truth annotations: 3\n   GT 1: walking at [1053, 2236, 1715, 4098]\n   GT 2: walking at [2186, 2053, 2821, 4091]\n   GT 3: walking at [1826, 2504, 2263, 4029]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  20%|‚ñà‚ñà        | 22/109 [02:50<12:19,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 23/109: 39c9544615cc.jpg\nüìã Ground truth annotations: 4\n   GT 1: walking at [499, 96, 599, 334]\n   GT 2: standing at [596, 85, 685, 285]\n   GT 3: walking at [333, 71, 403, 276]\n   GT 4: walking at [157, 94, 261, 301]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 4\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 4 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  21%|‚ñà‚ñà        | 23/109 [03:06<15:15, 10.64s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 24/109: 860f5d9ad655.jpg\nüìã Ground truth annotations: 2\n   GT 1: pushing a stroller at [174, 71, 668, 941]\n   GT 2: walking at [688, 138, 739, 264]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: pushing a stroller -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  22%|‚ñà‚ñà‚ñè       | 24/109 [03:14<14:04,  9.93s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 25/109: d17b1e5f2d49.jpg\nüìã Ground truth annotations: 3\n   GT 1: running at [70, 177, 276, 769]\n   GT 2: running at [535, 189, 742, 783]\n   GT 3: running at [311, 219, 479, 766]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  23%|‚ñà‚ñà‚ñé       | 25/109 [03:26<14:42, 10.51s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 26/109: 43fddef1053c.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [772, 103, 942, 594]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  24%|‚ñà‚ñà‚ñç       | 26/109 [03:31<12:12,  8.82s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 27/109: 2137dc2147c2.jpg\nüìã Ground truth annotations: 2\n   GT 1: skateboarding at [255, 144, 356, 459]\n   GT 2: skateboarding at [102, 178, 221, 345]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 2 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Skateboarding\n   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n   üìù Caption: Skateboarding\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  25%|‚ñà‚ñà‚ñç       | 27/109 [03:40<12:11,  8.92s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 28/109: 000029_1_x.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [36, 0, 85, 144]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  26%|‚ñà‚ñà‚ñå       | 28/109 [03:44<10:11,  7.55s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 29/109: 59ee13e5524e.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [137, 90, 289, 375]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  27%|‚ñà‚ñà‚ñã       | 29/109 [03:48<08:47,  6.59s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 30/109: 397638464.jpg\nüìã Ground truth annotations: 1\n   GT 1: scootering at [493, 178, 726, 750]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  28%|‚ñà‚ñà‚ñä       | 30/109 [03:53<07:47,  5.92s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 31/109: 000040_3_r.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [2, 0, 82, 143]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  28%|‚ñà‚ñà‚ñä       | 31/109 [03:58<07:18,  5.62s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 32/109: 52dc81b8995f.jpg\nüìã Ground truth annotations: 3\n   GT 1: biking at [210, 424, 329, 745]\n   GT 2: walking at [489, 467, 561, 622]\n   GT 3: standing at [1, 474, 132, 607]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 1\nüìä Detection Metrics - P: 1.000, R: 0.667, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  29%|‚ñà‚ñà‚ñâ       | 32/109 [04:06<08:14,  6.42s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 33/109: 314567015.jpg\nüìã Ground truth annotations: 3\n   GT 1: skateboarding at [301, 25, 463, 587]\n   GT 2: walking at [584, 61, 717, 376]\n   GT 3: walking at [715, 71, 893, 394]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Skateboarding\n   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  30%|‚ñà‚ñà‚ñà       | 33/109 [04:18<10:20,  8.16s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 34/109: 376dd7ef6a69.jpg\nüìã Ground truth annotations: 2\n   GT 1: skateboarding at [2844, 1753, 3252, 3214]\n   GT 2: walking at [1973, 1991, 2408, 3218]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Skateboarding\n   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  31%|‚ñà‚ñà‚ñà       | 34/109 [04:28<10:37,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 35/109: 5d6136ae2318.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [397, 119, 751, 501]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 2 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.333, R: 1.000, F1: 0.500\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 35/109 [04:32<08:59,  7.28s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 36/109: a59a5a1900bb.jpg\nüìã Ground truth annotations: 2\n   GT 1: running at [194, 133, 399, 698]\n   GT 2: running at [468, 158, 693, 706]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 36/109 [04:41<09:20,  7.67s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 37/109: 1879bdd3bf3b.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [1287, 548, 1512, 993]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 37/109 [04:45<08:07,  6.76s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 38/109: 000081_1_x.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [48, 2, 135, 241]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 38/109 [04:50<07:12,  6.08s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 39/109: 7334f2e5a13e.jpg\nüìã Ground truth annotations: 2\n   GT 1: skateboarding at [1135, 245, 1961, 2829]\n   GT 2: skateboarding at [694, 681, 1228, 2503]\nüîç Grounding DINO detections: 5\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 3 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.400, R: 1.000, F1: 0.571\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 39/109 [04:58<08:02,  6.89s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 40/109: 5b7bb02451d4.jpg\nüìã Ground truth annotations: 3\n   GT 1: walking at [906, 517, 1471, 2311]\n   GT 2: walking at [1628, 699, 1839, 1226]\n   GT 3: walking at [1838, 670, 2040, 1124]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/109 [05:11<09:50,  8.56s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 41/109: 846806160.jpg\nüìã Ground truth annotations: 2\n   GT 1: person on wheelchair at [155, 229, 444, 547]\n   GT 2: person on wheelchair at [557, 1, 849, 548]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: using a wheelchair\n   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n   üìù Caption: Using a wheelchair\n   üéØ Classification: person on wheelchair (confidence: 0.783)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/109 [05:20<09:52,  8.71s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 42/109: 1df319712ff5.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [461, 520, 1149, 1577]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  39%|‚ñà‚ñà‚ñà‚ñä      | 42/109 [05:24<08:18,  7.44s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 43/109: 27551a5c9032.jpg\nüìã Ground truth annotations: 5\n   GT 1: biking at [830, 104, 1016, 622]\n   GT 2: biking at [1033, 146, 1183, 646]\n   GT 3: walking at [678, 152, 848, 584]\n   GT 4: biking at [355, 199, 596, 769]\n   GT 5: standing at [79, 59, 295, 613]\nüîç Grounding DINO detections: 5\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 5\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 5 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Using a bicycle\n   üéØ Classification: biking (confidence: 0.779)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 43/109 [05:45<12:30, 11.37s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 44/109: 000069_1_x.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [29, 1, 76, 150]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 44/109 [05:49<10:01,  9.25s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 45/109: 561178142eca.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [1928, 1677, 3010, 4322]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/109 [05:54<08:24,  7.89s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 46/109: 277377119.jpg\nüìã Ground truth annotations: 7\n   GT 1: skateboarding at [172, 76, 245, 290]\n   GT 2: standing at [446, 112, 472, 206]\n   GT 3: standing at [349, 110, 377, 198]\n   GT 4: skateboarding at [253, 113, 274, 178]\n   GT 5: standing at [392, 111, 437, 197]\n   GT 6: walking at [300, 116, 318, 159]\n   GT 7: standing at [220, 118, 246, 200]\nüîç Grounding DINO detections: 5\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 5\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 2\nüìä Detection Metrics - P: 1.000, R: 0.714, F1: 0.833\nüß† Running DeepSeek-VL + BART classification on 5 mapped detections...\n   üìù Caption: Skateboarding\n   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 46/109 [06:15<12:27, 11.86s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 47/109: 32381712b502.jpg\nüìã Ground truth annotations: 3\n   GT 1: standing at [267, 326, 571, 1394]\n   GT 2: standing at [1941, 292, 2169, 1217]\n   GT 3: biking at [727, 356, 1550, 1826]\nüîç Grounding DINO detections: 5\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 2 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.600, R: 1.000, F1: 0.750\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 47/109 [06:28<12:30, 12.10s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 48/109: 246266068.jpg\nüìã Ground truth annotations: 2\n   GT 1: biking at [1907, 31, 2531, 1672]\n   GT 2: walking at [573, 2, 1236, 1939]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/109 [06:37<11:23, 11.20s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 49/109: 689722882.jpg\nüìã Ground truth annotations: 3\n   GT 1: skateboarding at [255, 3, 341, 268]\n   GT 2: walking at [155, 32, 244, 276]\n   GT 3: walking at [55, 30, 129, 279]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.750, R: 1.000, F1: 0.857\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 49/109 [06:50<11:36, 11.61s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 50/109: 83b49c4cf207.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [170, 20, 605, 421]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 50/109 [06:54<09:21,  9.52s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 51/109: 123159472.jpg\nüìã Ground truth annotations: 1\n   GT 1: person on wheelchair at [175, 19, 292, 259]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a wheelchair\n   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 51/109 [06:59<07:54,  8.18s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 52/109: e3373b9d3481.jpg\nüìã Ground truth annotations: 3\n   GT 1: walking at [862, 838, 2126, 4335]\n   GT 2: walking at [278, 2036, 865, 4206]\n   GT 3: pushing a stroller at [2173, 1271, 3231, 4409]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 2 (discarded)\n   üìã Unmatched annotations: 2\nüìä Detection Metrics - P: 0.333, R: 0.333, F1: 0.333\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/109 [07:04<06:51,  7.21s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 53/109: 191525525.jpg\nüìã Ground truth annotations: 3\n   GT 1: running at [2603, 535, 3204, 2147]\n   GT 2: running at [1765, 561, 2360, 2086]\n   GT 3: running at [1264, 387, 1754, 2044]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 53/109 [07:17<08:13,  8.82s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.953)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 54/109: 7fefc62978cd.jpg\nüìã Ground truth annotations: 5\n   GT 1: walking at [407, 185, 430, 244]\n   GT 2: walking at [387, 188, 410, 244]\n   GT 3: walking at [428, 184, 449, 245]\n   GT 4: running at [140, 140, 231, 380]\n   GT 5: standing at [7, 161, 56, 302]\nüîç Grounding DINO detections: 5\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 5\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 5 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 54/109 [07:37<11:21, 12.38s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 55/109: 6d37c7afea46.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [619, 160, 1067, 881]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 55/109 [07:42<09:01, 10.03s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 56/109: 3b58d5073737.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [1179, 1577, 2345, 4770]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/109 [07:47<07:40,  8.68s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 57/109: 2e4a2a8cff84.jpg\nüìã Ground truth annotations: 1\n   GT 1: person on wheelchair at [139, 134, 245, 343]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a wheelchair\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 57/109 [07:52<06:27,  7.44s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 58/109: 7581b6070f80.jpg\nüìã Ground truth annotations: 3\n   GT 1: skateboarding at [442, 78, 737, 846]\n   GT 2: standing at [268, 102, 472, 637]\n   GT 3: walking at [730, 134, 979, 677]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Skateboarding\n   üéØ Classification: skateboarding (confidence: 0.616)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> skateboarding | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 58/109 [08:05<07:36,  8.96s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 59/109: 217928635.jpg\nüìã Ground truth annotations: 1\n   GT 1: person on wheelchair at [114, 1, 235, 319]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a wheelchair\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 59/109 [08:09<06:21,  7.62s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 0.997\n\n============================================================\nüì∑ IMAGE 60/109: 9d199f1d4f54.jpg\nüìã Ground truth annotations: 6\n   GT 1: walking at [51, 523, 318, 1249]\n   GT 2: standing at [1697, 521, 1940, 1120]\n   GT 3: scootering at [656, 458, 910, 1202]\n   GT 4: scootering at [1020, 370, 1436, 1312]\n   GT 5: walking at [899, 579, 1046, 994]\n   GT 6: walking at [347, 514, 593, 1017]\nüîç Grounding DINO detections: 7\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 6\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.857, R: 1.000, F1: 0.923\nüß† Running DeepSeek-VL + BART classification on 6 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Standing up on the sidewalk\n   üéØ Classification: standing (confidence: 0.924)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: standing -> standing | IoU: 1.000\n   üìù Caption: Using a scooter\n   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n   üìù Caption: using a scooter\n   üéØ Classification: scootering (confidence: 0.928)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/109 [08:34<10:24, 12.74s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 6:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 61/109: 2d2de75e913b.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [501, 199, 635, 440]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Using a bicycle\n   üéØ Classification: biking (confidence: 0.779)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 61/109 [08:39<08:19, 10.40s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 62/109: 645196724.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [210, 196, 304, 378]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 62/109 [08:43<06:45,  8.62s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 63/109: 39408bf8a260.jpg\nüìã Ground truth annotations: 3\n   GT 1: walking at [268, 60, 504, 592]\n   GT 2: scootering at [593, 19, 778, 648]\n   GT 3: walking at [68, 118, 295, 588]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a scooter\n   üéØ Classification: scootering (confidence: 0.928)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 63/109 [08:56<07:29,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 64/109: 186629468.jpg\nüìã Ground truth annotations: 2\n   GT 1: scootering at [1769, 62, 2813, 1779]\n   GT 2: scootering at [1254, 138, 1738, 1409]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Using a scooter\n   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n   üìù Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 64/109 [09:04<07:07,  9.50s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 65/109: 772677099.jpg\nüìã Ground truth annotations: 3\n   GT 1: running at [436, 62, 639, 544]\n   GT 2: running at [166, 65, 394, 555]\n   GT 3: running at [375, 134, 441, 336]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 65/109 [09:17<07:35, 10.35s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.953)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 66/109: 0d0f4099138e.jpg\nüìã Ground truth annotations: 3\n   GT 1: biking at [504, 191, 663, 655]\n   GT 2: pushing a stroller at [421, 271, 527, 557]\n   GT 3: walking at [647, 229, 768, 573]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: using a stroller\n   üéØ Classification: pushing a stroller (confidence: 0.713)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 66/109 [09:30<07:59, 11.16s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 67/109: 5c121785961f.jpg\nüìã Ground truth annotations: 2\n   GT 1: pushing a stroller at [365, 336, 668, 647]\n   GT 2: walking at [316, 331, 450, 617]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: pushing a stroller -> walking | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 67/109 [09:39<07:17, 10.42s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 68/109: 000043_6_x.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [92, 1, 199, 350]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 68/109 [09:43<05:53,  8.61s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 0.997\n\n============================================================\nüì∑ IMAGE 69/109: 8839d0f5f8e4.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [1228, 925, 1714, 2194]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 69/109 [09:48<04:57,  7.45s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 70/109: 00ed6d77a262.jpg\nüìã Ground truth annotations: 6\n   GT 1: person on wheelchair at [1592, 1171, 2417, 2710]\n   GT 2: biking at [4216, 981, 4563, 1766]\n   GT 3: walking at [3837, 889, 4210, 1733]\n   GT 4: walking at [3318, 898, 3800, 2004]\n   GT 5: pushing a stroller at [2921, 796, 3335, 1974]\n   GT 6: walking at [1, 853, 483, 2016]\nüîç Grounding DINO detections: 6\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 6\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 6 mapped detections...\n   üìù Caption: using a wheelchair\n   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a baby stroller\n   üéØ Classification: pushing a stroller (confidence: 0.825)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 70/109 [10:13<08:14, 12.69s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 6:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 71/109: 815398412.jpg\nüìã Ground truth annotations: 2\n   GT 1: standing at [915, 1, 1120, 569]\n   GT 2: person on wheelchair at [207, 6, 824, 778]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 0.998\n   üìù Caption: using a wheelchair\n   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 71/109 [10:22<07:20, 11.60s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 72/109: 274925954b9a.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [1925, 463, 3171, 3941]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 72/109 [10:26<05:52,  9.53s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 73/109: bb1898bb6407.jpg\nüìã Ground truth annotations: 1\n   GT 1: skateboarding at [326, 78, 502, 635]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 73/109 [10:31<04:50,  8.06s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 0.923\n\n============================================================\nüì∑ IMAGE 74/109: a2e829af9975.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [828, 196, 1613, 2091]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 74/109 [10:36<04:05,  7.01s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 75/109: 396602767.jpg\nüìã Ground truth annotations: 3\n   GT 1: running at [961, 623, 1119, 1052]\n   GT 2: running at [712, 626, 858, 992]\n   GT 3: running at [562, 598, 722, 1012]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n   üìù Caption: running\n   üéØ Classification: running (confidence: 0.953)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 75/109 [10:48<04:50,  8.55s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 76/109: 640903271.jpg\nüìã Ground truth annotations: 1\n   GT 1: person on wheelchair at [695, 144, 1198, 785]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a wheelchair\n   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 76/109 [10:53<04:07,  7.50s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 77/109: e3334775aaf7.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [713, 146, 1252, 1770]\n   GT 2: walking at [81, 244, 725, 1786]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 77/109 [11:01<04:08,  7.76s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 78/109: 0b458fb3cc18.jpg\nüìã Ground truth annotations: 3\n   GT 1: running at [182, 148, 241, 285]\n   GT 2: running at [357, 159, 414, 323]\n   GT 3: running at [234, 150, 333, 306]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 78/109 [11:13<04:42,  9.11s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 79/109: 141443957.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [427, 101, 592, 512]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.500, R: 1.000, F1: 0.667\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 79/109 [11:18<03:53,  7.77s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 80/109: 3b6627b1909a.jpg\nüìã Ground truth annotations: 3\n   GT 1: walking at [932, 90, 1495, 1528]\n   GT 2: walking at [1428, 567, 1880, 1497]\n   GT 3: walking at [1797, 563, 2267, 1475]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 3\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 3 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 80/109 [11:31<04:27,  9.21s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 81/109: 19b3e4f09d46.jpg\nüìã Ground truth annotations: 4\n   GT 1: running at [1069, 58, 1384, 776]\n   GT 2: running at [188, 87, 634, 1396]\n   GT 3: walking at [1028, 32, 1172, 398]\n   GT 4: scootering at [786, 160, 951, 698]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 4\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 4 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: scootering -> walking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 81/109 [11:47<05:21, 11.50s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 82/109: 25bfa28305d9.jpg\nüìã Ground truth annotations: 6\n   GT 1: pushing a stroller at [425, 89, 730, 584]\n   GT 2: walking at [738, 75, 944, 570]\n   GT 3: walking at [1079, 67, 1235, 511]\n   GT 4: walking at [1, 40, 253, 719]\n   GT 5: walking at [379, 90, 478, 326]\n   GT 6: walking at [910, 252, 1063, 570]\nüîç Grounding DINO detections: 6\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 6\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 6 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: pushing a stroller -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 0.996\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 82/109 [12:12<06:54, 15.36s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 6:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 83/109: 4caf932ae018.jpg\nüìã Ground truth annotations: 7\n   GT 1: biking at [2862, 264, 3359, 1553]\n   GT 2: walking at [2626, 251, 2808, 797]\n   GT 3: biking at [81, 130, 1026, 1766]\n   GT 4: walking at [1461, 184, 1667, 776]\n   GT 5: walking at [1020, 196, 1246, 661]\n   GT 6: walking at [1732, 192, 2319, 1584]\n   GT 7: standing at [2357, 202, 2502, 675]\nüîç Grounding DINO detections: 7\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 7\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 7 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: walking -> biking | IoU: 1.000\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 6:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 83/109 [12:41<08:24, 19.39s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 7:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 84/109: 1b413332a7fb.jpg\nüìã Ground truth annotations: 2\n   GT 1: skateboarding at [265, 360, 630, 1462]\n   GT 2: skateboarding at [232, 416, 343, 686]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 84/109 [12:49<06:42, 16.10s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: skateboarding -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 85/109: 9a17c119d342.jpg\nüìã Ground truth annotations: 5\n   GT 1: walking at [1048, 539, 1195, 947]\n   GT 2: walking at [782, 511, 923, 1020]\n   GT 3: walking at [1737, 456, 2016, 1082]\n   GT 4: biking at [34, 36, 789, 2249]\n   GT 5: walking at [1516, 490, 1736, 1084]\nüîç Grounding DINO detections: 5\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 5\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 5 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: biking -> walking | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 85/109 [13:09<06:56, 17.34s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: walking -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 86/109: 16630a9f1da8.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [164, 95, 253, 240]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 86/109 [13:14<05:13, 13.63s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 87/109: 288845502.jpg\nüìã Ground truth annotations: 1\n   GT 1: scootering at [107, 70, 393, 465]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 87/109 [13:19<03:59, 10.90s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 88/109: 7ed68dbcbb77.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [865, 490, 1853, 1990]\n   GT 2: walking at [2661, 347, 3445, 1809]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 88/109 [13:27<03:34, 10.20s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 89/109: 000043_5_r.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [1, 1, 78, 186]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 89/109 [13:32<02:49,  8.48s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 90/109: 5bedd0713b73.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [1103, 958, 1961, 2382]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 90/109 [13:36<02:18,  7.30s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 91/109: 1f9999b8ea6c.jpg\nüìã Ground truth annotations: 2\n   GT 1: running at [727, 251, 962, 967]\n   GT 2: running at [440, 232, 681, 935]\nüîç Grounding DINO detections: 3\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 1 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 91/109 [13:45<02:19,  7.76s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 92/109: 05186f7b853c.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [547, 157, 698, 656]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 92/109 [13:50<01:55,  6.78s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 93/109: 430b80c19b62.jpg\nüìã Ground truth annotations: 2\n   GT 1: scootering at [306, 108, 381, 293]\n   GT 2: scootering at [205, 130, 306, 328]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> walking | IoU: 1.000\n   üìù Caption: Using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 93/109 [13:58<01:56,  7.30s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 94/109: 522b1c9ce018.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [2748, 484, 3462, 2733]\n   GT 2: walking at [1369, 249, 2331, 2766]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 94/109 [14:07<01:56,  7.78s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 95/109: 123d2b2b2f2e.jpg\nüìã Ground truth annotations: 4\n   GT 1: walking at [500, 1240, 2171, 4797]\n   GT 2: pushing a stroller at [1519, 1543, 1978, 2625]\n   GT 3: standing at [66, 1633, 393, 2656]\n   GT 4: walking at [337, 1596, 577, 2500]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 4\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 4 mapped detections...\n   üìù Caption: pushing a baby stroller (or baby chair)\n   üéØ Classification: pushing a stroller (confidence: 0.958)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> pushing a stroller | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: pushing a stroller -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: standing -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 95/109 [14:25<02:31, 10.85s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 96/109: 584562230.jpg\nüìã Ground truth annotations: 1\n   GT 1: scootering at [1300, 45, 2578, 2157]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Using a scooter\n   üéØ Classification: scootering (confidence: 0.923)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 96/109 [14:30<01:58,  9.15s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 97/109: 20fc87fc5a4d.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [381, 291, 1323, 1718]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 97/109 [14:35<01:33,  7.78s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 98/109: 5d05cb649f05.jpg\nüìã Ground truth annotations: 2\n   GT 1: walking at [2735, 55, 3222, 1483]\n   GT 2: walking at [410, 2, 1358, 2748]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 98/109 [14:43<01:28,  8.00s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 99/109: 13a3448686f6.jpg\nüìã Ground truth annotations: 2\n   GT 1: running at [1521, 1, 2173, 1846]\n   GT 2: running at [777, 39, 1300, 1835]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> walking | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 99/109 [14:52<01:21,  8.15s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 100/109: 600247964.jpg\nüìã Ground truth annotations: 1\n   GT 1: biking at [228, 110, 354, 440]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a bicycle\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 100/109 [14:56<01:03,  7.03s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 101/109: 000109_1_x.jpg\nüìã Ground truth annotations: 1\n   GT 1: pushing a stroller at [1, 2, 116, 157]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: pushing a baby stroller\n   üéØ Classification: pushing a stroller (confidence: 0.970)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: pushing a stroller -> pushing a stroller | IoU: 0.991\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 101/109 [15:02<00:52,  6.51s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 102/109: 000033_1_r.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [2, 1, 43, 132]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 102/109 [15:06<00:40,  5.84s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 103/109: 1ff401446354.jpg\nüìã Ground truth annotations: 1\n   GT 1: scootering at [182, 100, 336, 349]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: using a scooter\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 103/109 [15:10<00:32,  5.45s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: scootering (confidence: 0.928)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 104/109: 168c57aa4497.jpg\nüìã Ground truth annotations: 4\n   GT 1: walking at [1543, 594, 1793, 1330]\n   GT 2: biking at [1812, 734, 1940, 1114]\n   GT 3: biking at [2027, 651, 2248, 1212]\n   GT 4: person on wheelchair at [1116, 504, 1460, 1335]\nüîç Grounding DINO detections: 4\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 4\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 4 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: using a wheelchair\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 104/109 [15:27<00:44,  8.89s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: person on wheelchair (confidence: 0.771)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: person on wheelchair -> person on wheelchair | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 105/109: 432651820.jpg\nüìã Ground truth annotations: 6\n   GT 1: walking at [574, 135, 624, 268]\n   GT 2: biking at [411, 137, 526, 437]\n   GT 3: walking at [742, 146, 794, 267]\n   GT 4: walking at [697, 149, 735, 266]\n   GT 5: pushing a stroller at [884, 144, 989, 383]\n   GT 6: walking at [627, 144, 669, 266]\nüîç Grounding DINO detections: 6\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 6\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 6 mapped detections...\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a bicycle\n   üéØ Classification: biking (confidence: 0.679)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: biking -> biking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a baby stroller\n   üéØ Classification: pushing a stroller (confidence: 0.825)\n   üéØ Crop 5:\n      üè∑Ô∏è  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 105/109 [15:52<00:54, 13.66s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 6:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 106/109: 24bad69b2781.jpg\nüìã Ground truth annotations: 1\n   GT 1: running at [817, 662, 2643, 2867]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Running\n   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: running -> running | IoU: 1.000\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 106/109 [15:57<00:33, 11.05s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüì∑ IMAGE 107/109: 301480833.jpg\nüìã Ground truth annotations: 2\n   GT 1: scootering at [212, 86, 328, 378]\n   GT 2: walking at [70, 211, 115, 305]\nüîç Grounding DINO detections: 2\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 2\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 2 mapped detections...\n   üìù Caption: using a scooter\n   üéØ Classification: scootering (confidence: 0.928)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: scootering -> scootering | IoU: 1.000\n   üìù Caption: walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 107/109 [16:05<00:20, 10.25s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 108/109: 1e4b66a943e6.jpg\nüìã Ground truth annotations: 4\n   GT 1: walking at [3790, 209, 4823, 2315]\n   GT 2: walking at [1, 82, 1535, 3538]\n   GT 3: pushing a stroller at [1145, 54, 2998, 5440]\n   GT 4: walking at [3286, 187, 3993, 1841]\nüîç Grounding DINO detections: 6\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 4\n   üì¶ Unmatched detections: 2 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 0.667, R: 1.000, F1: 0.800\nüß† Running DeepSeek-VL + BART classification on 4 mapped detections...\n   üìù Caption: Walking slowly\n   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: walking slowly\n   üéØ Classification: walking (confidence: 0.964)\n   üéØ Crop 2:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n   üìù Caption: using a baby stroller\n   üéØ Classification: pushing a stroller (confidence: 0.825)\n   üéØ Crop 3:\n      üè∑Ô∏è  Classification: pushing a stroller -> pushing a stroller | IoU: 1.000\n   üìù Caption: Running\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 108/109 [16:23<00:12, 12.52s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: running (confidence: 0.971)\n   üéØ Crop 4:\n      üè∑Ô∏è  Classification: walking -> running | IoU: 1.000\n\n============================================================\nüì∑ IMAGE 109/109: 9b7357b415ef.jpg\nüìã Ground truth annotations: 1\n   GT 1: walking at [494, 289, 747, 738]\nüîç Grounding DINO detections: 1\nüîó Mapping results:\n   ‚úÖ Mapped pairs: 1\n   üì¶ Unmatched detections: 0 (discarded)\n   üìã Unmatched annotations: 0\nüìä Detection Metrics - P: 1.000, R: 1.000, F1: 1.000\nüß† Running DeepSeek-VL + BART classification on 1 mapped detections...\n   üìù Caption: Walking slowly\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [16:28<00:00,  9.07s/it]","output_type":"stream"},{"name":"stdout","text":"   üéØ Classification: walking (confidence: 0.965)\n   üéØ Crop 1:\n      üè∑Ô∏è  Classification: walking -> walking | IoU: 1.000\n\nüéâ PIPELINE COMPLETE!\nüíæ Results saved to: /kaggle/working/pipeline_results\nüìä Total processed detections: 232\nüìù Total DeepSeek classifications: 232\n‚è±Ô∏è Total time: 16.5 minutes\n\nüîç DETECTION PERFORMANCE SUMMARY:\n   üìà Average Precision: 0.924\n   üìà Average Recall: 0.976\n   üìà Average F1-Score: 0.941\n\nüéØ OVERALL CLASSIFICATION ACCURACY: 0.776 (77.6%)\n\nüìä DETAILED CLASSIFICATION REPORT:\n================================================================================\n                      precision    recall  f1-score   support\n\n              biking       0.92      0.71      0.80        34\nperson on wheelchair       1.00      1.00      1.00         9\n  pushing a stroller       0.83      0.56      0.67         9\n             running       0.92      0.82      0.87        40\n          scootering       1.00      0.82      0.90        17\n       skateboarding       1.00      0.53      0.70        15\n            standing       1.00      0.06      0.11        17\n             walking       0.65      0.95      0.77        91\n\n            accuracy                           0.78       232\n           macro avg       0.92      0.68      0.73       232\n        weighted avg       0.83      0.78      0.75       232\n\n\nüìà ENHANCED PER-CLASS METRICS:\n================================================================================\n               Class  Precision  Recall  F1-Score  Support\n              biking      0.923   0.706     0.800       34\nperson on wheelchair      1.000   1.000     1.000        9\n  pushing a stroller      0.833   0.556     0.667        9\n             running      0.917   0.825     0.868       40\n          scootering      1.000   0.824     0.903       17\n       skateboarding      1.000   0.533     0.696       15\n            standing      1.000   0.059     0.111       17\n             walking      0.652   0.945     0.771       91\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nüìù SAMPLE DEEPSEEK CLASSIFICATIONS:\n============================================================\n   1. walking\n   2. biking\n   3. biking\n   4. walking\n   5. walking\n   6. skateboarding\n   7. walking\n   8. running\n   9. skateboarding\n   10. scootering\n   ... and 222 more classifications\n","output_type":"stream"}],"execution_count":5}]}